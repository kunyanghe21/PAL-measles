---
title: |
  | Poisson Approximate Likelihood versus the block particle filter for a spatiotemporal measles model
  | __Preliminary version, not for circulation__
author:
  - "Kunyang He, Yize Hao and Edward L. Ionides"
date: "May 28, 2025"
abstract: |
  Filtering algorithms for high-dimensional nonlinear non-Gaussian partially observed stochastic processes provide access to the likelihood function and hence enable likelihood-based or Bayesian inference for complex dynamic systems.
  A novel Poisson approximate likelihood (PAL) filter was introduced by Whitehouse et al.\ (2023).
  PAL employs a Poisson approximation to conditional densities, offering a fast approximation to the likelihood function for a certain subset of partially observed Markov process models.
  PAL was demonstrated on an epidemiological metapopulation model for measles, specifically, a spatiotemporal model for disease transmission within and between cities. 
  At face value, Table\ 3 of Whitehouse et al.\ (2023) suggests that PAL considerably out-performs previous analysis as well as an ARMA benchmark model.
  We show that PAL does not outperform a block particle filter and that the lookahead component of PAL was implemented in a way that introduces substantial positive bias in the log-likelihood estimates.
  Therefore, the results of Table\ 3 of Whitehouse et al.\ (2023) do not accurately represent the true capabilities of PAL.

format:
  pdf:
    cite-method: natbib
    biblio-style: apalike
    include-in-header:
      - text: |
          \usepackage{graphicx} % Required for inserting images
          \usepackage{fullpage}

          \newcommand\loglik{\lambda}
          \newcommand\fproc{f_{\mathrm{proc}}}
          \newcommand\fmeas{f_{\mathrm{meas}}}
          \newcommand\sproc{s_{\mathrm{proc}}}
          \newcommand\smeas{s_{\mathrm{meas}}}

          \newcommand\fprocDiscrete{\mathrm{discrete}}
          \newcommand\fprocEuler{\mathrm{Euler}}
          \newcommand\fmeasBinomial{\mathrm{binomial}}
          \newcommand\fmeasGaussian{\mathrm{Gaussian}}

          \newcommand\sprocDiscrete{\mathrm{discrete}}
          \newcommand\sprocEuler{\mathrm{Euler}}
          \newcommand\smeasBinomial{\mathrm{binomial}}
          \newcommand\smeasGaussian{\mathrm{Gaussian}}

          \newcommand{\PALL}{\ensuremath{\mathrm{PAL}_L}}
          \newcommand{\PALV}{\ensuremath{\mathrm{PAL}_V}}
          \usepackage{multirow}
          \usepackage[dvipsnames]{xcolor}
          \newcommand\eic[1]{{\color{Orange} #1}}

    documentclass: article
    pdf-engine: pdflatex
    geometry: "margin=1in"
    number-sections: true
bibliography: bib-pal-measles.bib
---

```{r Setup, include=FALSE}
library(reticulate)
# use_python("/usr/local/bin/python3", required = TRUE)
use_python("/Library/Frameworks/Python.framework/Versions/3.12/bin/python3", required = TRUE)

pomp_dir="pomp/"
```

\section{Introduction}
\label{sec:intro}

Investigations of the metapopulation dynamics of measles (i.e., studying how measles infection moves within and between collections of spatially distinct populations) have motivated various methodological innovations for inference on high-dimensional partially observed stochastic processes \citep{xia04,park20,ionides23-jasa}. The analysis by \citet{whitehouse23} (henceforth, WWR) provides a new approach to model-based inference on population dynamics via the Poisson approximate likelihood (PAL) filtering algorithm.
WWR claimed impressive results on both a low-dimensional rotavirus transmission model and a high-dimensional measles model.
On close inspection, the rotavirus results turned out to be overstated \citep{hao24-arxiv} leading to a published correction \citep{whitehouse25-correction}.
However, the spatiotemporal measles results were unaffected by that correction, and our present goal is to revisit this evidence.

In Section\ \ref{sec:numerics} we show by direct numerical experimentation that the WWR implementation of PAL has substantial positive bias in the log-likelihood estimate, and so the use of log-likelihood to support the use of the method is flawed.
While doing this, we show that a widely applicable block particle filter (BPF) is adequate on this problem.
In Section\ \ref{sec:theory} we explain theoretically how the positive bias for PAL arises as a result of the lookahead mechanism included in the implementation of PAL for this model.
The lookahead mechanism was not used by WWR for the rotavirus analysis since its purpose was to address numerical issues involved in high-dimensional filtering. Section\ \ref{sec:conclusion} is a concluding discussion.

For our current purposes, we do not have to delve into the details of the measles data and model, so we simply provide an overview.
The data are measles case counts aggregated over 2-week intervals for forty of the largest towns in England and Wales, from 1949 to 1964.
The data and the model are derived from \citet{park20} which builds on a long tradition of models described therein.
Recently, weekly data for more towns have become publicly available \citep{korevaar20}, but we limit ourselves to the data used by WWR.
The latent process model describes an integer count of infected, susceptible and recovered individuals in each town.
The rate of disease transmission within cities follows widely used epidemiological equations.
Transmission between pairs of cities follows a power law, diminishing with distance between the cities.
This is known as a gravity model.
Overdispersion for the latent dynamics is achieved by placing multiplicative gamma white noise on the transmission rate.
The measurement model is a discretized Gaussian approximation to an overdispersed binomial \citep{park20} or Gaussian noise on a binomial rate (WWR).
\citet{park20} used a particle filter known as a guided intermediate resampling filter (GIRF).
Recently, BPF has been shown to have good performance on this class of models \citep{ionides23-jasa,ionides24-sinica,ning23}. Therefore, we compare PAL with BPF.

\section{Numerical experiments for PAL and BPF}
\label{sec:numerics}

There are many possible numerical experiments comparing filters on spatiotemporal measles models. 
Here, we focus on developing experiments aimed at establishing two specific hypotheses:

\begin{enumerate}
\item[Q1] The lookahead version of the Poisson approximate likelihood estimator of WWR, which we call $\PALL$, can have substantial positive bias on its log-likelihood estimate. This occurs in the spatiotemporal example of WWR.

\item[Q2] The bias in the $\PALL$ log-likelihood estimate scales approximately linearly with the number of spatial units.
\end{enumerate}
We consider probabilistic filtering algorithms that are defined in the context of a model, its model parameters, and additional algorithmic parameters. 
Additionally, we require data, and this can either be the real historical measles data or can be simulated from another model that may or may not be the same model with the same parameters as used for the filter. We also have a choice of how many spatial units to include, these being UK towns in the measles example. The experimental variables, and the list of values we consider for them, are summarized in Table\ \ref{tbl:description} and further described below.

A critical part of our reasoning is that a probabilistic forecasting filter (i.e., one that solves the one-step prediction problem without looking ahead to future data) cannot, on average, obtain higher log-likelihood than the exact prediction distribution, when the data are generated by the exact model.
This is a restatement of the well-known fact that log-likelihood is a proper scoring rule \citep{gneiting07}.
To apply this property, we must work with simulated data so that the true generating model is known.
In high dimensions, it is generally not possible to calculate the exact prediction distribution to within arbitrarily small error.Add commentMore actions
Provably consistent Monte\ Carlo methods have intolerable Monte\ Carlo error.
That is the reason why algorithms such as $\PALL$ are being invented.
There are two special situations where we can establish accurately the true log-likelihood for the spatiotemporal measles models of interest: (i) when the number of spatial units is very small; (ii) when there is no spatial coupling, so the filtering problem can be solved independently for each single unit.
In both these cases, a basic particle filter provides the desired, essentially exact, log-likelihood estimate.
The basic particle filter is consistent and unbiased for the likelihood \citep{delmoral04} and so, when its estimates have low empirical variance, it provides the required ground truth.
In practice, order $10^5$ particles give a highly accurate log-likelihood for one unit, but the strong sensitivity of the particle filter to the curse of dimensionality \citep{bengtsson08} means that quantifiably exact estimates rapidly become unfeasible.
Therefore, we consider two choices of size for the system, $U=1$ and $U=40$, with the latter being the size of the system tested by WWR.
To allow for the study of systems without coupling, we consider two model variations, $C_1$ and $C_2$, where $C_1$ is the original model with gravity coupling used by WWR, and $C_2$ is a modification where direct movement of infection between cities is replaced by a constant background rate of importation of infection.

Evidently, the basic particle filter is not a powerful tool for general spatiotemporal systems, and so alternative strategies are needed.
One approach is to take advantage of the possibility to design the filter's prediction distribution for the $n$th observation, at time $t_n$, to take advantage of data occurring at, or subsequent to, time $t_n$.
Such lookahead filters cannot be implemented for forecasting, but are available for likelihood evaluation.
This is the approach adopted by WWR's lookahead filter, $\PALL$.
There is no mathematical theorem prohibiting a lookahead filter estimating a higher likelihood than the truth, and in an extreme case the lookahead filter could just assert a one-step prediction distribution with all its mass on the actual data.
Lookahead filters therefore need careful theoretical guarantees if we want to use a high likelihood estimate as evidence for both the success of the filter and (when doing data analysis) evidence supporting the model used to construct the filter.
We will investigate the theory behind $\PALL$ later, in Section\ \ref{sec:theory}, but for now we just identify the potential hazard.

\begin{table}
\begin{tabular}{lllll}
Variable & Description & Value 1 & Value 2 & Value 3
\\
\hline
$F$ & filter algorithm & \PALL & \PALV & BPF
\\
$J$ & number of particles & $J_1=5\times 10^3$ &  $J_2=10^5$ &
\\
$U$ & number of spatial units & $U_1=1$ &  $U_2=40$ &
\\
$f_C$ & spatiotemporal coupling for filter & $C_1 =(g\neq 0, \iota = 0)$ & $C_2=(g=0, \iota\neq 0)$ &
\\
$\fproc$ & process model for filter & $\fprocEuler$ & $\fprocDiscrete$ &
\\
$\fmeas$ & measurement model for filter & $\fmeasBinomial$ & $\fmeasGaussian$ &
\\
$f_{\theta}$ & parameter for filter & $\hat\theta^*_{BPF}$ & $\hat\theta_{PAL}$ &
\\
$s_C$ & spatiotemporal coupling for simulation & $C_1 =(g\neq 0, \iota = 0)$ & $C_2=(g=0, \iota\neq 0)$ &
\\
$\sproc$ & process model for simulation & $\sprocEuler$ & $\sprocDiscrete$ &
\\
$\smeas$ & measurement model for simulation & $\smeasBinomial$ & $\smeasGaussian$ &
\\
$s_{\theta}$ & parameter for simulation & $\hat\theta^*_{BPF}$ & $\hat\theta_{PAL}$ &
\\
\hline
\end{tabular}
\caption{Variables for the numerical experiments and their set of values.}
\label{tbl:description}
\end{table}

The experimental variables listed in Table\ \ref{tbl:description} are now described in more detail:

\begin{itemize}
\item[F] {\bf The filtering algorithm}. 
{\PALL} is the lookahead filter of WWR, and {\PALV} is the plain, so-called vanilla, implementation. 
BPF is the block particle filter of \citet{rebeschini15} implemented as bpfilter in spatPomp \citep{asfaw24}.
Likelihood optimization for {\PALL} and {\PALV} is conducted using stochastic gradient descent and automatic differentiation, using the implementation by WWR. 
Likelihood optimization for BPF is conducted using the iterated block particle filter algorithm \citep{ning23,ionides24-sinica} implemented as ibpf in spatPomp.
For $U=1$, and for $U=40$ with $g=0$, BPF is identical to a basic particle filter. 
For simplicity, we use spatPomp::bpfilter even when pomp::pfilter is equivalent.

\item[$f_C$]  {\bf Spatiotemporal coupling for the filter model}.
The choice $C=C_1$ corresponds to the coupling used by WWR, with spatial movement of infection ($g\neq 0$) and no background immigration of infection from outside the study system ($\iota=0$). 
In order to test the methods on a high-dimensional system for which the true likelihood is known to a good degree of accuracy, we also consider setting $C_2$, without coupling ($g=0$) and with compensating immigration to prevent permanent extinction of measles in small towns ($\iota\neq 0$).
When $U=U_1=1$, we use the largest city, London, for which stochastic extinctions are very unlikely. Add commentMore actions
Note that, when $U=1$, the value of $g$ becomes irrelevant.

\item[$\fproc$]  {\bf Latent process transition model for the filter}.
For $\fproc=\fprocDiscrete$,  a single gamma-distributed dynamic noise variable is chosen for each observation interval.Add commentMore actions
This is to the choice made by WWR.
For $\fproc=\fprocEuler$, independent gamma noise variables are included in each Euler time step, so that the limit of the process model (as the Euler time step decreases) corresponds to a continuous-time over-dispersed Markov chain. 
Both $\fproc=\fprocDiscrete$ and $\fproc=\fprocEuler$ are implemented with a step of $1/2$ week for the multinomial transitions conditional on the gamma noise. 

\item[$\fmeas$]  {\bf Measurement model for the filter}. The choice of WWR is $\fmeas=\fmeasBinomial$, corresponding to binomial measurements with truncated multiplicative Gaussian noise on the reporting rate, i.e., the expected fraction of infections that are reported. 
The basic PAL algorithm requires a binomial measurement model, but the SMC-PAL extension permits noise on the measurement probability.
We also consider $\fmeas=\fmeasGaussian$, corresponding to a discretized Gaussian measurement model. Add commentMore actions
This choice leads to a measurement model that can directly be evaluated, without costly Monte Carlo calculation, assisting with efficient Monte Carlo inference.

\item[$f_{\theta}$]  {\bf Model parameter vector for the filter}. 
{\PALL} and {\PALV} are evaluated at optimized parameter vectors for each data set. 
For PAL, there is generally no true POMP model for which PAL is an exact filter.
However, we give PAL a reasonable chance to show its capabilities by optimizing it using the code provided by WWR. 

\item[$\sproc$]  {\bf Process model for the simulation}. 
Always set to $\sproc=\sprocEuler$ since simulations were carried out using an implementation of the model in spatPomp. 

\item[$\smeas$]  {\bf Measurement model for the simulation}. 
Always set to $\smeas=\smeasGaussian$ since simulations were carried out using an implementation of the model in spatPomp. 



\item[$s_{\theta}$]  {\bf Model parameter vector for the simulated data}.
To have an essentially exact likelihood evaluation using BPF, we have $s_\theta=f_\theta$ for all BPF situations. 

\end{itemize}

We consider two software platforms for the experiments.Add commentMore actions
All $\PALL$ and $\PALV$ calculations were carried out using the Python code provided by WWR, and all BPF calculations were carried out using the spatPomp R package.
Simulations were carried out using spatPomp, since our reasoning depends critically on the particle filter calculations being carried out with the data drawn from the model assumed by the filter.
We present results for a single simulation for the experimental treatments with simulated data.
That decision simplifies the experimental design and permits the computational effort to focus on a few direct comparisons between the methods on a small number of simulated datasets.

\begin{table}\label{tbl:treatments}
\centering
\begin{tabular}{llllllllllll}
E & F & J & U & 
  $f_C$ & $\fproc$ &  $\fmeas$ & $f_{\theta}$ &
  $s_C$ & $\sproc$ &  $\smeas$ & $s_{\theta}$ 
\\
\hline
%1 &  \PALV & $J_1$ & $U_1$ & 
%  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
%  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
%\\
$E_1$ &  BPF & $J_2$ & $U_1$ & 
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_2$ &  \PALV & $J_1$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_3$ &  \PALV & $J_2$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_4$ &  \PALL & $J_1$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_5$ &  \PALL & $J_2$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_6$ &  \PALL & $J_1$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_7$ &  BPF & $J_1$ & $U_1$ & 
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_8$ &  BPF & $J_2$ & $U_2$ & 
  $C_2$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$ &
  $C_2$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_9$ &  \PALV & $J_1$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_2$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_{10}$ &  \PALL & $J_1$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_2$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_{11}$ &  BPF & $J_2$ & $U_2$ & 
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_{12}$ &  \PALV & $J_1$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_{13}$ &  \PALV & $J_2$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_{14}$ &  \PALL & $J_1$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_{15}$ &  \PALL & $J_2$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
%4 &  \PALV & $J_2$ & $U_1$ & 
%  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
%  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
%\\
%5 &  \PALV & $J_2$ & $U_1$ & 
%  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
%  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
%\\
\hline
 \end{tabular}
\caption{Combinations of variable values used for each experiment, $E_k$, $k=1\dots 15$. The simulation settings, $s_C$, $\sproc$, $\smeas$ and $s_\theta$, are applicable only when we filter simulated data rather than real data.}
\end{table}

Each experiment, $E_k$, has two primary outcomes, a log-likelihood estimate, $\loglik_k$, and its standard error, $\sigma_k$. Add commentMore actions
These results are tabulated in Table\ \ref{tbl:method_comparison}, together with benchmark log-likelihoods for a log-ARMA model and a negative binomial autoregressive model.


```{r E1,include=FALSE}
## ----packages,incluxde=F,echo=F,cache=F----------------------------------------
library("spatPomp")
library("ggplot2")
library("tidyverse")
library("knitr")
library("doRNG")
library("doParallel")


library(pomp)

measles_cases <- read.csv(paste0(pomp_dir,"case1.csv"))
measles_covar <- read.csv(paste0(pomp_dir,"covar2.csv"))

measles_cases<- measles_cases[measles_cases$city == "LONDON", ]
measles_covar <- measles_covar[measles_covar$city == "LONDON", ]


measles_cases <-  measles_cases[,-1]
measles_covar <-  measles_covar[,-1]



colnames(measles_cases) <- c("time","cases1")
colnames(measles_covar) <- c("time",
                             "lag_birthrate1","pop1")


basic_params <- c(
  alpha       = 1,
  iota        = 0,
  betabar     = 6.32,
  c           = 0.219,
  a           = 0.1476,
  rho         = 0.142,
  gamma       = 0.0473,
  delta       = 0.02/(26*4),  # timescale transform
  sigma_xi    = 0.318,
  gaussianrho = 0.7,
  psi         = 0.306,
  g           = 0,
  S_0         = 0.02545,
  E_0         = 0.00422,
  I_0         = 0.000061
)


rproc <- Csnippet("
  double t_mod = fmod(t, 364.0);
  double br1;
  double beta1, seas1;
  double foi1;         
  double xi1;           
  double betafinal1;

  int trans_S1[2], trans_E1[2], trans_I1[2];
  double prob_S1[2], prob_E1[2], prob_I1[2];

  if ((t_mod >= 6 && t_mod < 99) ||
      (t_mod >= 115 && t_mod < 198) ||
      (t_mod >= 252 && t_mod < 299) ||
      (t_mod >= 308 && t_mod < 355)) {
    seas1 = 1.0 + a * 2 * (1 - 0.759);
  } else {
    seas1 = 1.0 - 2 * a * 0.759;
  }

  beta1 = betabar * seas1;

  if (fabs(t_mod - 248.5) < 0.5) {
    br1 = c * lag_birthrate1;
  } else {
    br1 = (1.0 - c) * lag_birthrate1 / 103.0;
  }

  double I_ratio1 = I1 / pop1;

  foi1 = pow((I1 + iota) / pop1, alpha);
 
  xi1 = rgamma(sigma_xi, 1 / sigma_xi);;
  betafinal1 = beta1 * I_ratio1 * xi1;

  int SD1 = rbinom(S1, delta);
  int ED1 = rbinom(E1, delta);
  int ID1 = rbinom(I1, delta);
  int RD1 = rbinom(R1, delta);

  S1 -= SD1;  E1 -= ED1;  I1 -= ID1;  R1 -= RD1;
  
  prob_S1[0] = exp(-dt * betafinal1);
  prob_S1[1] = 1 - exp(-dt * betafinal1);

  prob_E1[0] = exp(-dt * rho);
  prob_E1[1] = 1 - exp(-dt * rho);

  prob_I1[0] = exp(-dt * gamma);
  prob_I1[1] = 1 - exp(-dt * gamma);

  rmultinom(S1, prob_S1, 2, trans_S1);
  rmultinom(E1, prob_E1, 2, trans_E1);
  rmultinom(I1, prob_I1, 2, trans_I1);

  S1 = trans_S1[0] + rpois(br1);
  E1 = trans_E1[0] + trans_S1[1];
  I1 = trans_I1[0] + trans_E1[1];
  R1 += trans_I1[1];
  C1 += trans_I1[1];
");




## ----dmeasure-------------------------------------------------
dmeas <- Csnippet("
  double m = gaussianrho*C1;
  double v = m*(1.0-gaussianrho+psi*psi*m);
  double tol = 0.0;
  if (cases1 > 0.0) {
    lik = pnorm(cases1+0.5,m,sqrt(v)+tol,1,0)
           - pnorm(cases1-0.5,m,sqrt(v)+tol,1,0) + tol;
  } else {
    lik = pnorm(cases1+0.5,m,sqrt(v)+tol,1,0) + tol;
  }
  if (give_log) lik = log(lik);
")

## ----rmeasure-------------------------------------------------
rmeas <- Csnippet("
  double m = gaussianrho*C1;
  double v = m*(1.0-gaussianrho+psi*psi*m);
  double tol = 0.0;
  cases1 = rnorm(m,sqrt(v)+tol);
  if (cases1 > 0.0) {
    cases1 = nearbyint(cases1);
  } else {
    cases1 = 0.0;
  }
")

rinit <- Csnippet("
  double probs1[4];
  probs1[0] = S_0;
  probs1[1] = E_0;
  probs1[2] = I_0;
  probs1[3] = 1.0 - probs1[0] - probs1[1] - probs1[2];

  int counts1[4];
  rmultinom(pop1, probs1, 4, counts1);

  S1 = counts1[0];
  E1 = counts1[1];
  I1 = counts1[2];
  R1 = counts1[3];
  C1 = 0;
");

basic_log_names   <- c("rho", "gamma", "sigma_xi", "betabar", "g", "iota", "delta")
basic_logit_names <- c("a", "alpha", "c", "gaussianrho", "S_0", "E_0", "I_0", "psi")
log_names   <- basic_log_names
logit_names <- basic_logit_names
measles_partrans <- parameter_trans(
  log   = log_names,
  logit = logit_names
)

one_city_pomp <- pomp(
  data       = measles_cases,
  times      = "time",
  t0         = 0,
  rprocess   = euler(rproc, delta.t = 3.5), 
  rinit      = rinit,
  dmeasure   = dmeas,
  rmeasure   = rmeas,
  statenames = c("S1","E1","I1","R1","C1"),
  paramnames = c("alpha","iota","betabar","c","a","rho","gamma",
                 "delta","sigma_xi","g","gaussianrho","psi",
                 "S_0","E_0","I_0"),
  covar      = covariate_table(measles_covar,times = "time"),
  covarnames = c("lag_birthrate1","pop1"),
  accumvars  = c("C1")
)

coef(one_city_pomp) <- basic_params

sim <- simulate(one_city_pomp, params =  basic_params,  nsim   = 1,
                seed   = 154234)

Pomp_dir <- paste0(pomp_dir,"Pomp_E",1,"/")
if(!dir.exists(Pomp_dir)) dir.create(Pomp_dir)

stew(file=paste0(Pomp_dir,"E1_non_optimize.rda"),seed=456,{
  
  cat(capture.output(sessionInfo()),
      file=paste0(Pomp_dir,"sessionInfo.txt"),sep="\n")
  
  pf_logLik <- replicate(20,
                         logLik(pfilter(sim,Np = 100000))
  )
  
  
})
E1_result <- logmeanexp(pf_logLik,se = T)

E1_result[1]

tmp_benchmark <- arma_benchmark(sim)

tmp_benchmark$total

E1_sim <- sim@data

E1_sim <- t(E1_sim)

negloglik <- function(x) optim(par=c(0.5,0.5,1),function(theta)-sum(dnbinom(x,mu=theta[1]+theta[2]*c(0,head(x,-1)),size=theta[3],log=T)))$value

tmp_negbinom <- -sum(apply(E1_sim,2,negloglik))

sim.data <- as.data.frame(sim)

londonsim <- sim.data$cases1

df <- as.data.frame(t(londonsim)) 

colnames(df) <- 0:(length(londonsim) - 1)

write.csv(df, "londonsim.csv", row.names = FALSE)
```


```{r E7,include=FALSE}
## ----packages,incluxde=F,echo=F,cache=F----------------------------------------
library("spatPomp")
library("ggplot2")
library("tidyverse")
library("knitr")
library("doRNG")
library("doParallel")
library("doFuture")


pomp_dir="pomp/"


library(pomp)

measles_cases <- read.csv(paste0(pomp_dir,"case1.csv"))
measles_covar <- read.csv(paste0(pomp_dir,"covar2.csv"))

measles_cases<- measles_cases[measles_cases$city == "LONDON", ]
measles_covar <- measles_covar[measles_covar$city == "LONDON", ]


measles_cases <-  measles_cases[,-1]
measles_covar <-  measles_covar[,-1]



colnames(measles_cases) <- c("time","cases1")
colnames(measles_covar) <- c("time",
                             "lag_birthrate1","pop1")


basic_params <- c(
  alpha       = 1,
  iota        = 0,
  betabar     = 6.32,
  c           = 0.219,
  a           = 0.1476,
  rho         = 0.142,
  gamma       = 0.0473,
  delta       = 0.02/(26*4),  # timescale transform
  sigma_xi    = 0.318,
  gaussianrho = 0.55,
  psi         = 0.306,
  g           = 0,
  S_0         = 0.02545,
  E_0         = 0.00422,
  I_0         = 0.000061
)


rproc <- Csnippet("
  double t_mod = fmod(t, 364.0);
  double br1;
  double beta1, seas1;
  double foi1;         
  double xi1;           
  double betafinal1;

  int trans_S1[2], trans_E1[2], trans_I1[2];
  double prob_S1[2], prob_E1[2], prob_I1[2];

  if ((t_mod >= 6 && t_mod < 99) ||
      (t_mod >= 115 && t_mod < 198) ||
      (t_mod >= 252 && t_mod < 299) ||
      (t_mod >= 308 && t_mod < 355)) {
    seas1 = 1.0 + a * 2 * (1 - 0.759);
  } else {
    seas1 = 1.0 - 2 * a * 0.759;
  }

  beta1 = betabar * seas1;

  if (fabs(t_mod - 248.5) < 0.5) {
    br1 = c * lag_birthrate1;
  } else {
    br1 = (1.0 - c) * lag_birthrate1 / 103.0;
  }

  double I_ratio1 = I1 / pop1;

  foi1 = pow((I1 + iota) / pop1, alpha);
 
  xi1 = rgamma(sigma_xi, 1 / sigma_xi);;
  betafinal1 = beta1 * I_ratio1 * xi1;

  int SD1 = rbinom(S1, delta);
  int ED1 = rbinom(E1, delta);
  int ID1 = rbinom(I1, delta);
  int RD1 = rbinom(R1, delta);

  S1 -= SD1;  E1 -= ED1;  I1 -= ID1;  R1 -= RD1;
  
  prob_S1[0] = exp(-dt * betafinal1);
  prob_S1[1] = 1 - exp(-dt * betafinal1);

  prob_E1[0] = exp(-dt * rho);
  prob_E1[1] = 1 - exp(-dt * rho);

  prob_I1[0] = exp(-dt * gamma);
  prob_I1[1] = 1 - exp(-dt * gamma);

  rmultinom(S1, prob_S1, 2, trans_S1);
  rmultinom(E1, prob_E1, 2, trans_E1);
  rmultinom(I1, prob_I1, 2, trans_I1);

  S1 = trans_S1[0] + rpois(br1);
  E1 = trans_E1[0] + trans_S1[1];
  I1 = trans_I1[0] + trans_E1[1];
  R1 += trans_I1[1];
  C1 += trans_I1[1];
");




## ----dmeasure-------------------------------------------------
dmeas <- Csnippet("
  double m = gaussianrho*C1;
  double v = m*(1.0-gaussianrho+psi*psi*m);
  double tol = 0.0;
  if (cases1 > 0.0) {
    lik = pnorm(cases1+0.5,m,sqrt(v)+tol,1,0)
           - pnorm(cases1-0.5,m,sqrt(v)+tol,1,0) + tol;
  } else {
    lik = pnorm(cases1+0.5,m,sqrt(v)+tol,1,0) + tol;
  }
  if (give_log) lik = log(lik);
")

## ----rmeasure-------------------------------------------------
rmeas <- Csnippet("
  double m = gaussianrho*C1;
  double v = m*(1.0-gaussianrho+psi*psi*m);
  double tol = 0.0;
  cases1 = rnorm(m,sqrt(v)+tol);
  if (cases1 > 0.0) {
    cases1 = nearbyint(cases1);
  } else {
    cases1 = 0.0;
  }
")

rinit <- Csnippet("
  double probs1[4];
  probs1[0] = S_0;
  probs1[1] = E_0;
  probs1[2] = I_0;
  probs1[3] = 1.0 - probs1[0] - probs1[1] - probs1[2];

  int counts1[4];
  rmultinom(pop1, probs1, 4, counts1);

  S1 = counts1[0];
  E1 = counts1[1];
  I1 = counts1[2];
  R1 = counts1[3];
  C1 = 0;
");

basic_log_names   <- c("rho", "gamma", "sigma_xi", "betabar", "g", "iota", "delta")
basic_logit_names <- c("a", "alpha", "c", "gaussianrho", "S_0", "E_0", "I_0", "psi")
log_names   <- basic_log_names
logit_names <- basic_logit_names
measles_partrans <- parameter_trans(
  log   = log_names,
  logit = logit_names
)

one_city_pomp <- pomp(
  data       = measles_cases,
  times      = "time",
  t0         = 0,
  rprocess   = euler(rproc, delta.t = 3.5), 
  rinit      = rinit,
  dmeasure   = dmeas,
  rmeasure   = rmeas,
  statenames = c("S1","E1","I1","R1","C1"),
  paramnames = c("alpha","iota","betabar","c","a","rho","gamma",
                 "delta","sigma_xi","g","gaussianrho","psi",
                 "S_0","E_0","I_0"),
  covar      = covariate_table(measles_covar,times = "time"),
  covarnames = c("lag_birthrate1","pop1"),
  accumvars  = c("C1")
)

coef(one_city_pomp) <- basic_params


negloglik <- function(x) optim(par=c(0.5,0.5,1),function(theta)-sum(dnbinom(x,mu=theta[1]+theta[2]*c(0,head(x,-1)),size=theta[3],log=T)))$value


Pomp_dir <- paste0(pomp_dir,"Pomp_E",7,"/")
if(!dir.exists(Pomp_dir)) dir.create(Pomp_dir)

bake(file=paste0(Pomp_dir,"E7_search.rds"),{
  foreach(i=1:10,.combine=c,
          .options.future=list(seed=482947940)
  ) %dofuture% {
    one_city_pomp |>
      mif2(
        Np=20000, Nmif=50,
        cooling.fraction.50=0.5,
        rw.sd=rw_sd(c        = 0.02,
                    betabar  = 0.02,
                    a        = 0.02,
                    rho      = 0.02,
                    gamma    = 0.02,
                    sigma_xi = 0.02,
                    psi     = 0.02,
                    gaussianrho = 0.02,
                    S_0      = ivp(0.02),
                    E_0      = ivp(0.02),
                    I_0      = ivp(0.02)),
        partrans=parameter_trans(log=c("rho", "gamma", "sigma_xi", "betabar", "g", "iota", "delta"),logit = c("a", "alpha", "c", "gaussianrho", "S_0", "E_0", "I_0", "psi")),
        paramnames=c("alpha","iota","betabar","c","a","rho","gamma",
                     "delta","sigma_xi","g","gaussianrho","psi",
                     "S_0","E_0","I_0")
      )
  } -> mifs_local
  attr(mifs_local,"ncpu") <- nbrOfWorkers()
  mifs_local
}) -> mifs_local
t_loc <- attr(mifs_local,"system.time")
ncpu_loc <- attr(mifs_local,"ncpu")


mifs_local |>
  traces() |>
  melt() |>
  ggplot(aes(x = iteration, y = value, group = .L1, color = factor(.L1))) +
  geom_line() +
  guides(color = "none") +
  facet_wrap(~ name, scales = "free_y")


bake(file = paste0(Pomp_dir, "E7_local_search.rds"), {
  foreach(mf = mifs_local, .combine = rbind,
          .options.future = list(seed = 482947940)
  ) %dofuture% {
    evals <- replicate(20, logLik(pfilter(mf, Np = 5000)))
    ll <- logmeanexp(evals, se = TRUE)
    mf %>% coef() %>% bind_rows() %>% 
      bind_cols(loglik = ll[1], loglik.se = ll[2])
  } -> local_search
  attr(local_search, "ncpu") <- nbrOfWorkers()
  local_search
}) -> local_search


bind_rows(local_search) %>%
  filter(is.finite(loglik)) %>%
  filter(loglik.se < 2) %>%
  arrange(-loglik) -> best_searches    

head(best_searches)

E7_result <- best_searches[1,16:17]

real_one_benchmark <- arma_benchmark(one_city_pomp)
real_one_benchmark$total

E1_real <- one_city_pomp@data

E1_real <- t(E1_real)

real_one_negbinom <- -sum(apply(E1_real,2,negloglik))
real_one_negbinom
```


```{r E8,include=FALSE}
## ----packages,incluxde=F,echo=F,cache=F----------------------------------------
library("spatPomp")
library("ggplot2")
library("tidyverse")
library("knitr")
library("doRNG")
library("doParallel")
cores <-  as.numeric(Sys.getenv('SLURM_NTASKS_PER_NODE', unset=NA))
if(is.na(cores)) cores <- detectCores()
registerDoParallel(cores)
ggplot2::theme_set(ggplot2::theme_bw())

stopifnot(packageVersion("pomp")>="5.0")

pomp_dir="pomp/"

set.seed(42)

basic_params <- c(
  alpha     = 1,
  iota      = 0.1,
  betabar   = 6.32,
  c         = 0.219,
  a         = 0.1467,
  rho       = 0.142,
  gamma     = 0.0473,
  delta     = 0.02/(26*4),
  sigma_xi  = 0.318,
  gaussianrho     = 0.7,
  psi      = 0.306,
  g         = 0,
  S_0       = 0.02545,
  E_0       = 0.00422,
  I_0       = 0.000061
)
expandedParNames <- NULL

dt <- 3.5
U  <- 40

measles_cases  <- read.csv(paste0(pomp_dir,"case1.csv"))

measles_covar  <- read.csv(paste0(pomp_dir,"covar2.csv"))

measles_covarnames <- paste0(rep(c("pop", "lag_birthrate"), each = U), 1:U)
measles_unit_covarnames <- c("pop", "lag_birthrate")

data_measles_distance <- read.csv(paste0(pomp_dir,'data_measles_distance.csv'))




data_measles_distance <- data_measles_distance

v_by_g <- as.matrix(data_measles_distance)

to_C_array <- function(v) paste0("{", paste0(v, collapse = ","), "}")
v_by_g_C_rows  <- apply(v_by_g, 1, to_C_array)
v_by_g_C_array <- to_C_array(v_by_g_C_rows)
v_by_g_C <- Csnippet(paste0("const double v_by_g[", U, "][", U, "] = ", v_by_g_C_array, "; "))

parNames      <- names(basic_params)
fixedParNames <- setdiff(parNames, expandedParNames)

set_expanded <- Csnippet(
  paste0("const int ", expandedParNames, "_unit = 1;\n", collapse = " ")
)
set_fixed <- Csnippet(
  paste0("const int ", fixedParNames, "_unit = 0;\n", collapse = " ")
)
measles_globals <- Csnippet(
  paste(v_by_g_C, set_expanded, set_fixed, sep = "\n")
)

measles_paramnames <- c(
  if (length(fixedParNames) > 0) {
    paste0(fixedParNames, "1")
  },
  if (length(expandedParNames) > 0) {
    paste0(rep(expandedParNames, each = U), 1:U)
  }
)

unit_statenames <- c("S", "E", "I", "R", "C")


measles_rprocess <- spatPomp_Csnippet(
  unit_statenames  = c("S", "E", "I", "R", "C"),
  unit_covarnames  = c("pop", "lag_birthrate"),
  unit_paramnames  = c("alpha", "iota", "betabar", "c", "a",
                       "rho", "gamma", "delta", "sigma_xi", "g"),
  code ="
    // Variables
    double br, beta, seas, foi, births, xi, betafinal;
    int trans_S[2], trans_E[2], trans_I[2];
    double prob_S[2], prob_E[2], prob_I[2];
    int SD[U], ED[U], ID[U], RD[U];
    double powVec[U];
    int u, v;

    // Calculate the day of the year without any offset
    // Pre-computing this saves substantial time
    // powVec[u] = pow(I[u]/pop[u], alpha);
    for (u = 0; u < U; u++) {
        powVec[u] = I[u] / pop[u];
        // IS THIS INTENDED TO BE FIXED TO ALPHA=1?
    }

    for (u = 0; u < U; u++) {
        double t_mod = fmod(t, 364.0);

        // Transmission rate
        if ((t_mod >= 6 && t_mod < 99) || (t_mod >= 115 && t_mod < 198) ||
            (t_mod >= 252 && t_mod < 299) || (t_mod >= 308 && t_mod < 355))
            seas = 1.0 + a[u * a_unit] * 2 * (1 - 0.759);
        else
            seas = 1.0 - 2 * a[u * a_unit] * 0.759;

        beta = betabar[u * betabar_unit] * seas;

        // Birth rate calculation
        if (fabs(t_mod - 248.5) < 0.5) {
            br = c[u * c_unit] * lag_birthrate[u];
        } else {
            br = (1.0 - c[u * c_unit]) * lag_birthrate[u] / 103;
        }

        // Expected force of infection
        if (alpha[u * alpha_unit] == 1.0 && iota[u * iota_unit] == 0.0) {
            foi = I[u] / pop[u];
        } else {
            foi = pow((I[u] + iota[u * iota_unit]) / pop[u], alpha[u * alpha_unit]);
        }

        for (v = 0; v < U; v++) {
            if (v != u) {
                foi += g[u * g_unit] * v_by_g[u][v] * (powVec[v] - powVec[u]) / pop[u];
            }
        }

        xi = rgamma(sigma_xi[u * sigma_xi_unit], 1 / sigma_xi[u * sigma_xi_unit]);
        betafinal = beta * foi * xi;  // Stochastic force of infection

        // Poisson births
        births = rpois(br);

        SD[u] = rbinom(S[u], delta[u * delta_unit]);
        ED[u] = rbinom(E[u], delta[u * delta_unit]);
        ID[u] = rbinom(I[u], delta[u * delta_unit]);
        RD[u] = rbinom(R[u], delta[u * delta_unit]);

        S[u] = S[u] - SD[u];
        E[u] = E[u] - ED[u];
        I[u] = I[u] - ID[u];
        R[u] = R[u] - RD[u];

        // Probabilities for state transitions
        prob_S[0] = exp(-dt * betafinal);
        prob_S[1] = 1 - exp(-dt * betafinal);

        prob_E[0] = exp(-dt * rho[u * rho_unit]);
        prob_E[1] = 1 - exp(-dt * rho[u * rho_unit]);

        prob_I[0] = exp(-dt * gamma[u * gamma_unit]);
        prob_I[1] = 1 - exp(-dt * gamma[u * gamma_unit]);

        // Multinomial transitions
        rmultinom(S[u], &prob_S[0], 2, &trans_S[0]); // B, (S-F)-B
        rmultinom(E[u], &prob_E[0], 2, &trans_E[0]); // C, (E-F)-C
        rmultinom(I[u], &prob_I[0], 2, &trans_I[0]); // E, (I-F)-D

        // Update compartments
        S[u] = trans_S[0] + births;
        E[u] = trans_E[0] + trans_S[1];
        I[u] = trans_I[0] + trans_E[1];
        R[u] = R[u] + trans_I[1];
        C[u] += trans_I[1];  // True incidence
    }
"
)


measles_dmeasure <-  spatPomp_Csnippet(
  unit_statenames = 'C',
  unit_obsnames = 'cases',
  unit_paramnames = c('gaussianrho','psi'),
  code="
      double m,v;
      double tol = 1e-300;
      double mytol = 1e-5;
      int u;
      lik = 0;
      for (u = 0; u < U; u++) {
        m = gaussianrho[u*gaussianrho_unit]*(C[u]+mytol);
        v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);

        // Deal with NA measurements by omitting them
        if(!(ISNA(cases[u]))){
          // C < 0 can happen in bootstrap methods such as bootgirf
          if (C[u] < 0) {lik += log(tol);} else {
            if (cases[u] > tol) {
              lik += log(pnorm(cases[u]+0.5,m,sqrt(v)+tol,1,0)-
                pnorm(cases[u]-0.5,m,sqrt(v)+tol,1,0)+tol);
            } else {
                lik += log(pnorm(cases[u]+0.5,m,sqrt(v)+tol,1,0)+tol);
            }
          }
        }
      }
      if(!give_log) lik = (lik > log(tol)) ? exp(lik) : tol;
    "
)

measles_rmeasure <- spatPomp_Csnippet(
  method='rmeasure',
  unit_paramnames=c('gaussianrho','psi'),
  unit_statenames='C',
  unit_obsnames='cases',
  code="
      double m,v;
      double tol = 1.0e-300;
      int u;
      for (u = 0; u < U; u++) {
        m = gaussianrho[u*gaussianrho_unit]*(C[u]+tol);
        v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);
        cases[u] = rnorm(m,sqrt(v)+tol);
        if (cases[u] > 0.0) {
          cases[u] = nearbyint(cases[u]);
        } else {
          cases[u] = 0.0;
        }
      }
    "
)

measles_dunit_measure <- spatPomp_Csnippet(
  unit_paramnames=c('gaussianrho','psi'),
  code="
      double mytol = 1e-5;
      double m = gaussianrho[u*gaussianrho_unit]*(C+mytol);
      double v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);
      double tol = 1e-300;
      // C < 0 can happen in bootstrap methods such as bootgirf
      if(ISNA(cases)) {lik=1;} else { 
        if (C < 0) {lik = 0;} else {
          if (cases > tol) {
            lik = pnorm(cases+0.5,m,sqrt(v)+tol,1,0)-
              pnorm(cases-0.5,m,sqrt(v)+tol,1,0)+tol;
          } else {
            lik = pnorm(cases+0.5,m,sqrt(v)+tol,1,0)+tol;
          }
        }
      }
      if(give_log) lik = log(lik);
    "
)

measles_rinit <- spatPomp_Csnippet(
  unit_paramnames = c("S_0", "E_0", "I_0"),
  unit_statenames = c("S", "E", "I", "R", "C"),
  unit_covarnames = "pop",
  code = "
    int u;
    for (u = 0; u < U; u++) {
        double probs[4];
        probs[0] = S_0[u * S_0_unit];
        probs[1] = E_0[u * E_0_unit];
        probs[2] = I_0[u * I_0_unit];
        probs[3] = 1.0 - probs[0] - probs[1] - probs[2];
        int counts[4];
        rmultinom(pop[u], &probs[0], 4, &counts[0]);
        S[u] = counts[0];
        E[u] = counts[1];
        I[u] = counts[2];
        R[u] = counts[3];
        C[u] = 0;
    }
")




### === Parameter Transformation Settings ===

basic_log_names   <- c("rho", "gamma", "sigma_xi", "betabar", "g", "iota", "delta")
basic_log_names   <- setdiff(basic_log_names, fixedParNames)

basic_logit_names <- c("a", "alpha", "c", "gaussianrho", "S_0", "E_0", "I_0",'psi')
basic_logit_names <- setdiff(basic_logit_names, fixedParNames)
log_names   <- unlist(lapply(basic_log_names, function(x, U) paste0(x, 1:U), U))
logit_names <- unlist(lapply(basic_logit_names, function(x, U) paste0(x, 1:U), U))
measles_partrans <- parameter_trans(log = log_names, logit = logit_names)

m9 <- spatPomp(
  measles_cases,
  units           = "city",
  times           = "days",
  t0              = min(measles_cases$days) - 14,
  unit_statenames = unit_statenames,
  covar           = measles_covar,
  rprocess        = euler(measles_rprocess, delta.t = 3.5),
  unit_accumvars  = c("C"),
  paramnames      = measles_paramnames,
  globals         = measles_globals,
  rinit           = measles_rinit,
  dmeasure        = measles_dmeasure,
  rmeasure        = measles_rmeasure,
  dunit_measure   = measles_dunit_measure,
  partrans = measles_partrans
)

measles_params <- rep(0, length = length(measles_paramnames))

names(measles_params) <- measles_paramnames

for (p in fixedParNames)
  measles_params[paste0(p, 1)] <- basic_params[p]
for (p in expandedParNames)
  measles_params[paste0(p, 1:U)] <- basic_params[p]
coef(m9) <- measles_params

sim <- simulate(m9, params =  measles_params,  nsim   = 1,
                seed   = 154234)
##


spatPomp_dir <- paste0(pomp_dir,"E_",8,"/")
if(!dir.exists(spatPomp_dir)) dir.create(spatPomp_dir)

stew(file=paste0(spatPomp_dir,"E8.rda"),seed=124,{
  cat(capture.output(sessionInfo()),
      file=paste0(spatPomp_dir,"sessionInfo.txt"),sep="\n")
  
  bpf_logLik_40 <- foreach(i = 1:20, .combine = c) %dopar% {
    logLik(bpfilter(sim, Np = 100000, block_size = 1))
  }
})


E8_result <- logmeanexp(bpf_logLik_40,se = T,ess = T)

tmp_benchmark_spat <- arma_benchmark(sim)

tmp_benchmark_spat$total

E8_sim <- sim@data

E8_sim <- t(E8_sim)

negloglik <- function(x) optim(par=c(0.5,0.5,1),function(theta)-sum(dnbinom(x,mu=theta[1]+theta[2]*c(0,head(x,-1)),size=theta[3],log=T)))$value

tmp_negbinom_spat <- -sum(apply(E8_sim,2,negloglik))

realdata_benchmark_spat <- arma_benchmark(m9)

realdata_benchmark_spat$total

E8_real <- m9@data

E8_real <- t(E8_real)

realdata_negbinom_spat <- -sum(apply(E8_real,2,negloglik))

realdata_negbinom_spat
## Prepare the simulated data for python.
simdata <- as.data.frame(sim)

simdata <- simdata[order(simdata$city),]

yt <- simdata$cases

M40 <- matrix(yt, nrow = 40, byrow = TRUE)

M40 <- as.data.frame(M40)

colnames(M40) <- as.character(0:415)

write.csv(M40,file = "M40.csv",row.names = F)
```

```{r E11,include=FALSE}
## ----packages,incluxde=F,echo=F,cache=F----------------------------------------
library("spatPomp")
library("ggplot2")
library("tidyverse")
library("knitr")
library("doRNG")
library("doParallel")
cores <-  as.numeric(Sys.getenv('SLURM_NTASKS_PER_NODE', unset=NA))
if(is.na(cores)) cores <- detectCores()
registerDoParallel(cores)
ggplot2::theme_set(ggplot2::theme_bw())

stopifnot(packageVersion("pomp")>="5.0")

pomp_dir="pomp/"

set.seed(42)

basic_params <- c(
  alpha     = 1,
  iota      = 0,
  betabar   = 6.32,
  c         = 0.219,
  a         = 0.1467,
  rho       = 0.142,
  gamma     = 0.0473,
  delta     = 0.02/(26*4),
  sigma_xi  = 0.318,
  gaussianrho     = 0.7,
  psi      = 0.306,
  g         = 716,
  S_0       = 0.02545,
  E_0       = 0.00422,
  I_0       = 0.000061
)

basic_paramsC <- c(
  alpha1 = 1,
  alpha2 = 1,
  alpha3 = 1,
  alpha4 = 1,
  alpha5 = 1,
  alpha6 = 1,
  alpha7 = 1,
  alpha8 = 1,
  alpha9 = 1,
  alpha10 = 1,
  alpha11 = 1,
  alpha12 = 1,
  alpha13 = 1,
  alpha14 = 1,
  alpha15 = 1,
  alpha16 = 1,
  alpha17 = 1,
  alpha18 = 1,
  alpha19 = 1,
  alpha20 = 1,
  alpha21 = 1,
  alpha22 = 1,
  alpha23 = 1,
  alpha24 = 1,
  alpha25 = 1,
  alpha26 = 1,
  alpha27 = 1,
  alpha28 = 1,
  alpha29 = 1,
  alpha30 = 1,
  alpha31 = 1,
  alpha32 = 1,
  alpha33 = 1,
  alpha34 = 1,
  alpha35 = 1,
  alpha36 = 1,
  alpha37 = 1,
  alpha38 = 1,
  alpha39 = 1,
  alpha40 = 1,
  iota1 = 0,
  iota2 = 0,
  iota3 = 0,
  iota4 = 0,
  iota5 = 0,
  iota6 = 0,
  iota7 = 0,
  iota8 = 0,
  iota9 = 0,
  iota10 = 0,
  iota11 = 0,
  iota12 = 0,
  iota13 = 0,
  iota14 = 0,
  iota15 = 0,
  iota16 = 0,
  iota17 = 0,
  iota18 = 0,
  iota19 = 0,
  iota20 = 0,
  iota21 = 0,
  iota22 = 0,
  iota23 = 0,
  iota24 = 0,
  iota25 = 0,
  iota26 = 0,
  iota27 = 0,
  iota28 = 0,
  iota29 = 0,
  iota30 = 0,
  iota31 = 0,
  iota32 = 0,
  iota33 = 0,
  iota34 = 0,
  iota35 = 0,
  iota36 = 0,
  iota37 = 0,
  iota38 = 0,
  iota39 = 0,
  iota40 = 0,
  c1 = 0.032121729,
  c2 = 0.032121729,
  c3 = 0.032121729,
  c4 = 0.032121729,
  c5 = 0.032121729,
  c6 = 0.032121729,
  c7 = 0.032121729,
  c8 = 0.032121729,
  c9 = 0.032121729,
  c10 = 0.032121729,
  c11 = 0.032121729,
  c12 = 0.032121729,
  c13 = 0.032121729,
  c14 = 0.032121729,
  c15 = 0.032121729,
  c16 = 0.032121729,
  c17 = 0.032121729,
  c18 = 0.032121729,
  c19 = 0.032121729,
  c20 = 0.032121729,
  c21 = 0.032121729,
  c22 = 0.032121729,
  c23 = 0.032121729,
  c24 = 0.032121729,
  c25 = 0.032121729,
  c26 = 0.032121729,
  c27 = 0.032121729,
  c28 = 0.032121729,
  c29 = 0.032121729,
  c30 = 0.032121729,
  c31 = 0.032121729,
  c32 = 0.032121729,
  c33 = 0.032121729,
  c34 = 0.032121729,
  c35 = 0.032121729,
  c36 = 0.032121729,
  c37 = 0.032121729,
  c38 = 0.032121729,
  c39 = 0.032121729,
  c40 = 0.032121729,
  a1 = 0.37107605,
  a2 = 0.37107605,
  a3 = 0.37107605,
  a4 = 0.37107605,
  a5 = 0.37107605,
  a6 = 0.37107605,
  a7 = 0.37107605,
  a8 = 0.37107605,
  a9 = 0.37107605,
  a10 = 0.37107605,
  a11 = 0.37107605,
  a12 = 0.37107605,
  a13 = 0.37107605,
  a14 = 0.37107605,
  a15 = 0.37107605,
  a16 = 0.37107605,
  a17 = 0.37107605,
  a18 = 0.37107605,
  a19 = 0.37107605,
  a20 = 0.37107605,
  a21 = 0.37107605,
  a22 = 0.37107605,
  a23 = 0.37107605,
  a24 = 0.37107605,
  a25 = 0.37107605,
  a26 = 0.37107605,
  a27 = 0.37107605,
  a28 = 0.37107605,
  a29 = 0.37107605,
  a30 = 0.37107605,
  a31 = 0.37107605,
  a32 = 0.37107605,
  a33 = 0.37107605,
  a34 = 0.37107605,
  a35 = 0.37107605,
  a36 = 0.37107605,
  a37 = 0.37107605,
  a38 = 0.37107605,
  a39 = 0.37107605,
  a40 = 0.37107605,
  rho1 = 0.11784376,
  rho2 = 0.11784376,
  rho3 = 0.11784376,
  rho4 = 0.11784376,
  rho5 = 0.11784376,
  rho6 = 0.11784376,
  rho7 = 0.11784376,
  rho8 = 0.11784376,
  rho9 = 0.11784376,
  rho10 = 0.11784376,
  rho11 = 0.11784376,
  rho12 = 0.11784376,
  rho13 = 0.11784376,
  rho14 = 0.11784376,
  rho15 = 0.11784376,
  rho16 = 0.11784376,
  rho17 = 0.11784376,
  rho18 = 0.11784376,
  rho19 = 0.11784376,
  rho20 = 0.11784376,
  rho21 = 0.11784376,
  rho22 = 0.11784376,
  rho23 = 0.11784376,
  rho24 = 0.11784376,
  rho25 = 0.11784376,
  rho26 = 0.11784376,
  rho27 = 0.11784376,
  rho28 = 0.11784376,
  rho29 = 0.11784376,
  rho30 = 0.11784376,
  rho31 = 0.11784376,
  rho32 = 0.11784376,
  rho33 = 0.11784376,
  rho34 = 0.11784376,
  rho35 = 0.11784376,
  rho36 = 0.11784376,
  rho37 = 0.11784376,
  rho38 = 0.11784376,
  rho39 = 0.11784376,
  rho40 = 0.11784376,
  gamma1 = 0.10496352,
  gamma2 = 0.10496352,
  gamma3 = 0.10496352,
  gamma4 = 0.10496352,
  gamma5 = 0.10496352,
  gamma6 = 0.10496352,
  gamma7 = 0.10496352,
  gamma8 = 0.10496352,
  gamma9 = 0.10496352,
  gamma10 = 0.10496352,
  gamma11 = 0.10496352,
  gamma12 = 0.10496352,
  gamma13 = 0.10496352,
  gamma14 = 0.10496352,
  gamma15 = 0.10496352,
  gamma16 = 0.10496352,
  gamma17 = 0.10496352,
  gamma18 = 0.10496352,
  gamma19 = 0.10496352,
  gamma20 = 0.10496352,
  gamma21 = 0.10496352,
  gamma22 = 0.10496352,
  gamma23 = 0.10496352,
  gamma24 = 0.10496352,
  gamma25 = 0.10496352,
  gamma26 = 0.10496352,
  gamma27 = 0.10496352,
  gamma28 = 0.10496352,
  gamma29 = 0.10496352,
  gamma30 = 0.10496352,
  gamma31 = 0.10496352,
  gamma32 = 0.10496352,
  gamma33 = 0.10496352,
  gamma34 = 0.10496352,
  gamma35 = 0.10496352,
  gamma36 = 0.10496352,
  gamma37 = 0.10496352,
  gamma38 = 0.10496352,
  gamma39 = 0.10496352,
  gamma40 = 0.10496352,
  delta1 = 0.0001923076923076923,
  delta2 = 0.0001923076923076923,
  delta3 = 0.0001923076923076923,
  delta4 = 0.0001923076923076923,
  delta5 = 0.0001923076923076923,
  delta6 = 0.0001923076923076923,
  delta7 = 0.0001923076923076923,
  delta8 = 0.0001923076923076923,
  delta9 = 0.0001923076923076923,
  delta10 = 0.0001923076923076923,
  delta11 = 0.0001923076923076923,
  delta12 = 0.0001923076923076923,
  delta13 = 0.0001923076923076923,
  delta14 = 0.0001923076923076923,
  delta15 = 0.0001923076923076923,
  delta16 = 0.0001923076923076923,
  delta17 = 0.0001923076923076923,
  delta18 = 0.0001923076923076923,
  delta19 = 0.0001923076923076923,
  delta20 = 0.0001923076923076923,
  delta21 = 0.0001923076923076923,
  delta22 = 0.0001923076923076923,
  delta23 = 0.0001923076923076923,
  delta24 = 0.0001923076923076923,
  delta25 = 0.0001923076923076923,
  delta26 = 0.0001923076923076923,
  delta27 = 0.0001923076923076923,
  delta28 = 0.0001923076923076923,
  delta29 = 0.0001923076923076923,
  delta30 = 0.0001923076923076923,
  delta31 = 0.0001923076923076923,
  delta32 = 0.0001923076923076923,
  delta33 = 0.0001923076923076923,
  delta34 = 0.0001923076923076923,
  delta35 = 0.0001923076923076923,
  delta36 = 0.0001923076923076923,
  delta37 = 0.0001923076923076923,
  delta38 = 0.0001923076923076923,
  delta39 = 0.0001923076923076923,
  delta40 = 0.0001923076923076923,
  sigma_xi1 = 0.1776823,
  sigma_xi2 = 0.1776823,
  sigma_xi3 = 0.1776823,
  sigma_xi4 = 0.1776823,
  sigma_xi5 = 0.1776823,
  sigma_xi6 = 0.1776823,
  sigma_xi7 = 0.1776823,
  sigma_xi8 = 0.1776823,
  sigma_xi9 = 0.1776823,
  sigma_xi10 = 0.1776823,
  sigma_xi11 = 0.1776823,
  sigma_xi12 = 0.1776823,
  sigma_xi13 = 0.1776823,
  sigma_xi14 = 0.1776823,
  sigma_xi15 = 0.1776823,
  sigma_xi16 = 0.1776823,
  sigma_xi17 = 0.1776823,
  sigma_xi18 = 0.1776823,
  sigma_xi19 = 0.1776823,
  sigma_xi20 = 0.1776823,
  sigma_xi21 = 0.1776823,
  sigma_xi22 = 0.1776823,
  sigma_xi23 = 0.1776823,
  sigma_xi24 = 0.1776823,
  sigma_xi25 = 0.1776823,
  sigma_xi26 = 0.1776823,
  sigma_xi27 = 0.1776823,
  sigma_xi28 = 0.1776823,
  sigma_xi29 = 0.1776823,
  sigma_xi30 = 0.1776823,
  sigma_xi31 = 0.1776823,
  sigma_xi32 = 0.1776823,
  sigma_xi33 = 0.1776823,
  sigma_xi34 = 0.1776823,
  sigma_xi35 = 0.1776823,
  sigma_xi36 = 0.1776823,
  sigma_xi37 = 0.1776823,
  sigma_xi38 = 0.1776823,
  sigma_xi39 = 0.1776823,
  sigma_xi40 = 0.1776823,
  psi1 = 0.20831605,
  psi2 = 0.20831605,
  psi3 = 0.20831605,
  psi4 = 0.20831605,
  psi5 = 0.20831605,
  psi6 = 0.20831605,
  psi7 = 0.20831605,
  psi8 = 0.20831605,
  psi9 = 0.20831605,
  psi10 = 0.20831605,
  psi11 = 0.20831605,
  psi12 = 0.20831605,
  psi13 = 0.20831605,
  psi14 = 0.20831605,
  psi15 = 0.20831605,
  psi16 = 0.20831605,
  psi17 = 0.20831605,
  psi18 = 0.20831605,
  psi19 = 0.20831605,
  psi20 = 0.20831605,
  psi21 = 0.20831605,
  psi22 = 0.20831605,
  psi23 = 0.20831605,
  psi24 = 0.20831605,
  psi25 = 0.20831605,
  psi26 = 0.20831605,
  psi27 = 0.20831605,
  psi28 = 0.20831605,
  psi29 = 0.20831605,
  psi30 = 0.20831605,
  psi31 = 0.20831605,
  psi32 = 0.20831605,
  psi33 = 0.20831605,
  psi34 = 0.20831605,
  psi35 = 0.20831605,
  psi36 = 0.20831605,
  psi37 = 0.20831605,
  psi38 = 0.20831605,
  psi39 = 0.20831605,
  psi40 = 0.20831605,
  g1 = 621.00315,
  g2 = 621.00315,
  g3 = 621.00315,
  g4 = 621.00315,
  g5 = 621.00315,
  g6 = 621.00315,
  g7 = 621.00315,
  g8 = 621.00315,
  g9 = 621.00315,
  g10 = 621.00315,
  g11 = 621.00315,
  g12 = 621.00315,
  g13 = 621.00315,
  g14 = 621.00315,
  g15 = 621.00315,
  g16 = 621.00315,
  g17 = 621.00315,
  g18 = 621.00315,
  g19 = 621.00315,
  g20 = 621.00315,
  g21 = 621.00315,
  g22 = 621.00315,
  g23 = 621.00315,
  g24 = 621.00315,
  g25 = 621.00315,
  g26 = 621.00315,
  g27 = 621.00315,
  g28 = 621.00315,
  g29 = 621.00315,
  g30 = 621.00315,
  g31 = 621.00315,
  g32 = 621.00315,
  g33 = 621.00315,
  g34 = 621.00315,
  g35 = 621.00315,
  g36 = 621.00315,
  g37 = 621.00315,
  g38 = 621.00315,
  g39 = 621.00315,
  g40 = 621.00315,
  betabar1  = 0.88894647,
  betabar2  = 0.59131747,
  betabar3  = 1.3571352,
  betabar4  = 0.99088466,
  betabar5  = 1.1142877,
  betabar6  = 1.073081,
  betabar7  = 1.5383922,
  betabar8  = 0.90597922,
  betabar9  = 0.82007521,
  betabar10 = 0.85654908,
  betabar11 = 1.0977113,
  betabar12 = 0.86888778,
  betabar13 = 1.1310883,
  betabar14 = 0.9737336,
  betabar15 = 0.94829279,
  betabar16 = 0.62169701,
  betabar17 = 0.94519132,
  betabar18 = 0.59131724,
  betabar19 = 0.5913173,
  betabar20 = 0.76468575,
  betabar21 = 1.188135,
  betabar22 = 0.96471751,
  betabar23 = 1.3417162,
  betabar24 = 1.2114047,
  betabar25 = 1.2141544,
  betabar26 = 1.4035884,
  betabar27 = 0.98535007,
  betabar28 = 0.78903711,
  betabar29 = 1.3568828,
  betabar30 = 0.92531413,
  betabar31 = 0.85022265,
  betabar32 = 1.1698065,
  betabar33 = 1.747846,
  betabar34 = 1.0385478,
  betabar35 = 0.94807833,
  betabar36 = 0.90443295,
  betabar37 = 1.564422,
  betabar38 = 1.3209716,
  betabar39 = 1.4434845,
  betabar40 = 0.89985961,
  S_01 = 0.075939171,
  S_02 = 0.045746908,
  S_03 = 0.072099209,
  S_04 = 0.093366429,
  S_05 = 0.12166313,
  S_06 = 0.082433015,
  S_07 = 0.076253399,
  S_08 = 0.073552363,
  S_09 = 0.091899686,
  S_010 = 0.11601865,
  S_011 = 0.11060591,
  S_012 = 0.086007304,
  S_013 = 0.090033337,
  S_014 = 0.068562545,
  S_015 = 0.08527638,
  S_016 = 0.099353261,
  S_017 = 0.071033828,
  S_018 = 0.057539228,
  S_019 = 0.0457469,
  S_020 = 0.056584992,
  S_021 = 0.066623405,
  S_022 = 0.071289554,
  S_023 = 0.10958349,
  S_024 = 0.057944749,
  S_025 = 0.098137014,
  S_026 = 0.083878987,
  S_027 = 0.073387422,
  S_028 = 0.065006606,
  S_029 = 0.076856337,
  S_030 = 0.089820623,
  S_031 = 0.078178115,
  S_032 = 0.11018048,
  S_033 = 0.1181622,
  S_034 = 0.098707311,
  S_035 = 0.09720967,
  S_036 = 0.076138817,
  S_037 = 0.066983677,
  S_038 = 0.081948154,
  S_039 = 0.083776928,
  S_040 = 0.063756049,
  E_01 = 6.5060573e-05,
  E_02 = 4.5087443e-05,
  E_03 = 5.2817129e-05,
  E_04 = 6.8182577e-05,
  E_05 = 6.4530977e-05,
  E_06 = 4.4786735e-05,
  E_07 = 7.5147931e-05,
  E_08 = 9.4057032e-05,
  E_09 = 4.7683639e-05,
  E_010 = 4.2368429e-05,
  E_011 = 6.2710962e-05,
  E_012 = 7.0720002e-05,
  E_013 = 6.5743807e-05,
  E_014 = 9.3865732e-05,
  E_015 = 9.4957613e-05,
  E_016 = 5.9195183e-05,
  E_017 = 4.9652379e-05,
  E_018 = 4.4339824e-05,
  E_019 = 5.5780311e-05,
  E_020 = 3.4462537e-05,
  E_021 = 6.6237786e-05,
  E_022 = 4.6263423e-05,
  E_023 = 4.987068e-05,
  E_024 = 4.3129025e-05,
  E_025 = 7.405701e-05,
  E_026 = 6.1690727e-05,
  E_027 = 7.0626847e-05,
  E_028 = 7.152609e-05,
  E_029 = 5.3734158e-05,
  E_030 = 6.6238754e-05,
  E_031 = 5.7956771e-05,
  E_032 = 6.3070715e-05,
  E_033 = 8.4001782e-05,
  E_034 = 8.3680527e-05,
  E_035 = 3.8972536e-05,
  E_036 = 5.995209e-05,
  E_037 = 5.8326987e-05,
  E_038 = 5.703792e-05,
  E_039 = 8.9273613e-05,
  E_040 = 6.1138249e-05,
  I_01 = 1.25675637e-04,
  I_02 = 1.28807835e-04,
  I_03 = 0.0025939241,
  I_04 = 0.0011983996,
  I_05 = 5.3031814e-05,
  I_06 = 0.00043914493,
  I_07 = 0.00035218208,
  I_08 = 0.0020560939,
  I_09 = 0.00058301724,
  I_010 = 0.00017986108,
  I_011 = 7.6727294e-05,
  I_012 = 6.1292536e-05,
  I_013 = 0.00021765193,
  I_014 = 0.00083288341,
  I_015 = 1.3892895e-08,
  I_016 = 0.0016802071,
  I_017 = 0.0013289195,
  I_018 = 0.00024760532,
  I_019 = 0.00020639459,
  I_020 = 0.0014469447,
  I_021 = 0.00066906348,
  I_022 = 0.00023718234,
  I_023 = 1.6463227e-08,
  I_024 = 0.0006781175,
  I_025 = 0.00091942732,
  I_026 = 0.00076855475,
  I_027 = 0.0029490807,
  I_028 = 0.0024151804,
  I_029 = 0.0013736223,
  I_030 = 0.0010850356,
  I_031 = 0.0030775415,
  I_032 = 0.0039083785,
  I_033 = 0.00043130366,
  I_034 = 0.0025644207,
  I_035 = 0.0023132868,
  I_036 = 0.00070547545,
  I_037 = 0.00088096713,
  I_038 = 0.00051698624,
  I_039 = 0.00079949765,
  I_040 = 0.00021292546,
  gaussianrho1  = 0.512295,
  gaussianrho2  = 0.5846346,
  gaussianrho3  = 0.73733903,
  gaussianrho4  = 0.64644552,
  gaussianrho5  = 0.68464784,
  gaussianrho6  = 0.62805587,
  gaussianrho7  = 0.59746622,
  gaussianrho8  = 0.62227199,
  gaussianrho9  = 0.57668756,
  gaussianrho10 = 0.68815189,
  gaussianrho11 = 0.75385802,
  gaussianrho12 = 0.63758144,
  gaussianrho13 = 0.65418166,
  gaussianrho14 = 0.59742807,
  gaussianrho15 = 0.58928316,
  gaussianrho16 = 0.7143576,
  gaussianrho17 = 0.64989739,
  gaussianrho18 = 0.52764322,
  gaussianrho19 = 0.51610781,
  gaussianrho20 = 0.61995668,
  gaussianrho21 = 0.63451302,
  gaussianrho22 = 0.64841047,
  gaussianrho23 = 0.66641999,
  gaussianrho24 = 0.58411126,
  gaussianrho25 = 0.74216196,
  gaussianrho26 = 0.65110744,
  gaussianrho27 = 0.57886699,
  gaussianrho28 = 0.6156806,
  gaussianrho29 = 0.51067983,
  gaussianrho30 = 0.47119888,
  gaussianrho31 = 0.70101722,
  gaussianrho32 = 0.59482185,
  gaussianrho33 = 0.76027942,
  gaussianrho34 = 0.66776662,
  gaussianrho35 = 0.52625039,
  gaussianrho36 = 0.60342687,
  gaussianrho37 = 0.68950329,
  gaussianrho38 = 0.5869341,
  gaussianrho39 = 0.70792855,
  gaussianrho40 = 0.57809921
)

expandedParNames <- names(basic_params)

dt <- 3.5
U  <- 40

measles_cases  <- read.csv(paste0(pomp_dir,"case1.csv"))

measles_covar  <- read.csv(paste0(pomp_dir,"covar2.csv"))

measles_covarnames <- paste0(rep(c("pop", "lag_birthrate"), each = U), 1:U)
measles_unit_covarnames <- c("pop", "lag_birthrate")

data_measles_distance <- read.csv(paste0(pomp_dir,'data_measles_distance.csv'))




data_measles_distance <- data_measles_distance

v_by_g <- as.matrix(data_measles_distance)

to_C_array <- function(v) paste0("{", paste0(v, collapse = ","), "}")
v_by_g_C_rows  <- apply(v_by_g, 1, to_C_array)
v_by_g_C_array <- to_C_array(v_by_g_C_rows)
v_by_g_C <- Csnippet(paste0("const double v_by_g[", U, "][", U, "] = ", v_by_g_C_array, "; "))

parNames      <- names(basic_params)
fixedParNames <- setdiff(parNames, expandedParNames)

set_expanded <- Csnippet(
  paste0("const int ", expandedParNames, "_unit = 1;\n", collapse = " ")
)
set_fixed <- Csnippet(
  paste0("const int ", fixedParNames, "_unit = 0;\n", collapse = " ")
)
measles_globals <- Csnippet(
  paste(v_by_g_C, set_expanded, set_fixed, sep = "\n")
)

measles_paramnames <- c(
  if (length(fixedParNames) > 0) {
    paste0(fixedParNames, "1")
  },
  if (length(expandedParNames) > 0) {
    paste0(rep(expandedParNames, each = U), 1:U)
  }
)

unit_statenames <- c("S", "E", "I", "R", "C")


measles_rprocess <- spatPomp_Csnippet(
  unit_statenames  = c("S", "E", "I", "R", "C"),
  unit_covarnames  = c("pop", "lag_birthrate"),
  unit_paramnames  = c("alpha", "iota", "betabar", "c", "a",
                       "rho", "gamma", "delta", "sigma_xi", "g"),
  code ="
    // Variables
    double br, beta, seas, foi, births, xi, betafinal;
    int trans_S[2], trans_E[2], trans_I[2];
    double prob_S[2], prob_E[2], prob_I[2];
    int SD[U], ED[U], ID[U], RD[U];
    double powVec[U];
    int u, v;

    // Calculate the day of the year without any offset
    // Pre-computing this saves substantial time
    // powVec[u] = pow(I[u]/pop[u], alpha);
    for (u = 0; u < U; u++) {
        powVec[u] = I[u] / pop[u];
        // IS THIS INTENDED TO BE FIXED TO ALPHA=1?
    }

    for (u = 0; u < U; u++) {
        double t_mod = fmod(t, 364.0);

        // Transmission rate
        if ((t_mod >= 6 && t_mod < 99) || (t_mod >= 115 && t_mod < 198) ||
            (t_mod >= 252 && t_mod < 299) || (t_mod >= 308 && t_mod < 355))
            seas = 1.0 + a[u * a_unit] * 2 * (1 - 0.759);
        else
            seas = 1.0 - 2 * a[u * a_unit] * 0.759;

        beta = betabar[u * betabar_unit] * seas;

        // Birth rate calculation
        if (fabs(t_mod - 248.5) < 0.5) {
            br = c[u * c_unit] * lag_birthrate[u];
        } else {
            br = (1.0 - c[u * c_unit]) * lag_birthrate[u] / 103;
        }

        // Expected force of infection
        if (alpha[u * alpha_unit] == 1.0 && iota[u * iota_unit] == 0.0) {
            foi = I[u] / pop[u];
        } else {
            foi = pow((I[u] + iota[u * iota_unit]) / pop[u], alpha[u * alpha_unit]);
        }

        for (v = 0; v < U; v++) {
            if (v != u) {
                foi += g[u * g_unit] * v_by_g[u][v] * (powVec[v] - powVec[u]) / pop[u];
            }
        }

        xi = rgamma(sigma_xi[u * sigma_xi_unit], 1 / sigma_xi[u * sigma_xi_unit]);
        betafinal = beta * foi * xi;  // Stochastic force of infection

        // Poisson births
        births = rpois(br);

        SD[u] = rbinom(S[u], delta[u * delta_unit]);
        ED[u] = rbinom(E[u], delta[u * delta_unit]);
        ID[u] = rbinom(I[u], delta[u * delta_unit]);
        RD[u] = rbinom(R[u], delta[u * delta_unit]);

        S[u] = S[u] - SD[u];
        E[u] = E[u] - ED[u];
        I[u] = I[u] - ID[u];
        R[u] = R[u] - RD[u];

        // Probabilities for state transitions
        prob_S[0] = exp(-dt * betafinal);
        prob_S[1] = 1 - exp(-dt * betafinal);

        prob_E[0] = exp(-dt * rho[u * rho_unit]);
        prob_E[1] = 1 - exp(-dt * rho[u * rho_unit]);

        prob_I[0] = exp(-dt * gamma[u * gamma_unit]);
        prob_I[1] = 1 - exp(-dt * gamma[u * gamma_unit]);

        // Multinomial transitions
        rmultinom(S[u], &prob_S[0], 2, &trans_S[0]); // B, (S-F)-B
        rmultinom(E[u], &prob_E[0], 2, &trans_E[0]); // C, (E-F)-C
        rmultinom(I[u], &prob_I[0], 2, &trans_I[0]); // E, (I-F)-D

        // Update compartments
        S[u] = trans_S[0] + births;
        E[u] = trans_E[0] + trans_S[1];
        I[u] = trans_I[0] + trans_E[1];
        R[u] = R[u] + trans_I[1];
        C[u] += trans_I[1];  // True incidence
    }
"
)


measles_dmeasure <-  spatPomp_Csnippet(
  unit_statenames = 'C',
  unit_obsnames = 'cases',
  unit_paramnames = c('gaussianrho','psi'),
  code="
      double m,v;
      double tol = 1e-300;
      double mytol = 1e-5;
      int u;
      lik = 0;
      for (u = 0; u < U; u++) {
        m = gaussianrho[u*gaussianrho_unit]*(C[u]+mytol);
        v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);

        // Deal with NA measurements by omitting them
        if(!(ISNA(cases[u]))){
          // C < 0 can happen in bootstrap methods such as bootgirf
          if (C[u] < 0) {lik += log(tol);} else {
            if (cases[u] > tol) {
              lik += log(pnorm(cases[u]+0.5,m,sqrt(v)+tol,1,0)-
                pnorm(cases[u]-0.5,m,sqrt(v)+tol,1,0)+tol);
            } else {
                lik += log(pnorm(cases[u]+0.5,m,sqrt(v)+tol,1,0)+tol);
            }
          }
        }
      }
      if(!give_log) lik = (lik > log(tol)) ? exp(lik) : tol;
    "
)

measles_rmeasure <- spatPomp_Csnippet(
  method='rmeasure',
  unit_paramnames=c('gaussianrho','psi'),
  unit_statenames='C',
  unit_obsnames='cases',
  code="
      double m,v;
      double tol = 1.0e-300;
      int u;
      for (u = 0; u < U; u++) {
        m = gaussianrho[u*gaussianrho_unit]*(C[u]+tol);
        v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);
        cases[u] = rnorm(m,sqrt(v)+tol);
        if (cases[u] > 0.0) {
          cases[u] = nearbyint(cases[u]);
        } else {
          cases[u] = 0.0;
        }
      }
    "
)

measles_dunit_measure <- spatPomp_Csnippet(
  unit_paramnames=c('gaussianrho','psi'),
  code="
      double mytol = 1e-5;
      double m = gaussianrho[u*gaussianrho_unit]*(C+mytol);
      double v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);
      double tol = 1e-300;
      // C < 0 can happen in bootstrap methods such as bootgirf
      if(ISNA(cases)) {lik=1;} else { 
        if (C < 0) {lik = 0;} else {
          if (cases > tol) {
            lik = pnorm(cases+0.5,m,sqrt(v)+tol,1,0)-
              pnorm(cases-0.5,m,sqrt(v)+tol,1,0)+tol;
          } else {
            lik = pnorm(cases+0.5,m,sqrt(v)+tol,1,0)+tol;
          }
        }
      }
      if(give_log) lik = log(lik);
    "
)

measles_rinit <- spatPomp_Csnippet(
  unit_paramnames = c("S_0", "E_0", "I_0"),
  unit_statenames = c("S", "E", "I", "R", "C"),
  unit_covarnames = "pop",
  code = "
    int u;
    for (u = 0; u < U; u++) {
        double probs[4];
        probs[0] = S_0[u * S_0_unit];
        probs[1] = E_0[u * E_0_unit];
        probs[2] = I_0[u * I_0_unit];
        probs[3] = 1.0 - probs[0] - probs[1] - probs[2];
        int counts[4];
        rmultinom(pop[u], &probs[0], 4, &counts[0]);
        S[u] = counts[0];
        E[u] = counts[1];
        I[u] = counts[2];
        R[u] = counts[3];
        C[u] = 0;
    }
")




### === Parameter Transformation Settings ===

basic_log_names   <- c("rho", "gamma", "sigma_xi", "betabar", "g", "iota", "delta")
basic_log_names   <- setdiff(basic_log_names, fixedParNames)

basic_logit_names <- c("a", "alpha", "c", "gaussianrho", "S_0", "E_0", "I_0",'psi')
basic_logit_names <- setdiff(basic_logit_names, fixedParNames)
log_names   <- unlist(lapply(basic_log_names, function(x, U) paste0(x, 1:U), U))
logit_names <- unlist(lapply(basic_logit_names, function(x, U) paste0(x, 1:U), U))
measles_partrans <- parameter_trans(log = log_names, logit = logit_names)

m9 <- spatPomp(
  measles_cases,
  units           = "city",
  times           = "days",
  t0              = min(measles_cases$days) - 14,
  unit_statenames = unit_statenames,
  covar           = measles_covar,
  rprocess        = euler(measles_rprocess, delta.t = 3.5),
  unit_accumvars  = c("C"),
  paramnames      = measles_paramnames,
  globals         = measles_globals,
  rinit           = measles_rinit,
  dmeasure        = measles_dmeasure,
  rmeasure        = measles_rmeasure,
  dunit_measure   = measles_dunit_measure,
  partrans = measles_partrans
)

measles_params <- rep(0, length = length(measles_paramnames))

names(measles_params) <- measles_paramnames

for (p in fixedParNames)
  measles_params[paste0(p, 1)] <- basic_params[p]
for (p in expandedParNames)
  measles_params[paste0(p, 1:U)] <- basic_params[p]
coef(m9) <- measles_params

coef(m9) <- basic_paramsC

sim <- simulate(m9)

## ----bm-model,echo=T,eval=T---------------------------------------------------

library(spatPomp)
i <- 2

measles_dir <- paste0(pomp_dir,"measles_more",i,"/")
if(!dir.exists(measles_dir)) dir.create(measles_dir)

## -------- new rw.sd specification ----------------------------------
par_rw <- setdiff(names(basic_params), c("alpha", "iota","detla","gaussianrho","psi")) 

U       <- 40          
ivp_sd  <- 0.001        
rp_sd   <- 0.001       

IVP_names   <- unlist(lapply(c("S_0", "E_0", "I_0"),
                             function(p) paste0(p, 1:U)))

OTHER_names <- unlist(lapply(setdiff(par_rw, c("S_0", "E_0", "I_0")),
                             function(p) paste0(p, 1:U)))


string_rwsd <- paste0(
  "rw_sd(",
  paste0(IVP_names,   "=ivp(", ivp_sd, ")", collapse = ", "),
  if (length(OTHER_names) > 0)
      paste0(", ", paste0(OTHER_names, "=", rp_sd, collapse = ", ")),
  ")"
)

measles_rw.sd <- eval(parse(text = string_rwsd))

## ----ibpf-mle-eval,eval=T,echo=F----------------------------------------------
stew(file=paste0(measles_dir,"ibpf_mle.rda"),seed=999,{
  tic <- Sys.time()
  params_start <- coef(m9)
  ibpf_mle_searches <- foreach(reps=1:switch(i,3,10))%dopar%{
    ibpf(m9,params=params_start,
      Nbpf=switch(i,2,50),Np=switch(i,10,10000),
      rw.sd=measles_rw.sd ,
      unitParNames= c("betabar","S_0","E_0","I_0") ,
      sharedParNames = c("a","c","rho","gamma","sigma_xi","g"),
      block_size=1,
      spat_regression=0.1,
      cooling.fraction.50=0.5
    )
  }
  toc <- Sys.time()
  })

  prof1time <- toc-tic


m1 <- ibpf_mle_searches[[9]]

spatPomp_dir <- paste0(pomp_dir,"E_",11,"/")
if(!dir.exists(spatPomp_dir)) dir.create(spatPomp_dir)

stew(file=paste0(spatPomp_dir,"E11.rda"),seed=124,{
  cat(capture.output(sessionInfo()),
      file=paste0(spatPomp_dir,"sessionInfo.txt"),sep="\n")
  
  bpf_logLik_40 <- foreach(i = 1:20, .combine = c) %dopar% {
    logLik(bpfilter(m1, Np = 100000, block_size = 1))
  }
})

E11_result <- logmeanexp(bpf_logLik_40,se = T,ess = T)
```

```{python E2,include=FALSE}
from __future__ import annotations

import os
import numpy as np

# ------------- cache settings -----------------------------------
CACHE_DIR  = "wwr/E2"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_vanilla.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# ----------------------------------------------------------------
# 1) Try to load cached results
# ----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results  {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# ----------------------------------------------------------------
# 2) No cache  run the original simulation code unmodified
# ----------------------------------------------------------------
else:
    print("[cache] No cache found  running the full simulation ")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>  ORIGINAL CODE  DO NOT EDIT  <<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    #  (Everything between the double lines is your untouched script;
    #   only indentation has changed so it nests inside this block.)
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize

    import matplotlib.pyplot as plt
    from scipy.special import logsumexp
    from sympy.polys.benchmarks.bench_solvers import uk_10

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("wwr/E2"):
        os.makedirs("wwr/E2")

    os.environ['PYTHONHASHSEED'] = '45'
    random.seed(45)
    np.random.seed(45)
    tf.random.set_seed(45)

    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")
    modelA_array = np.load("wwr/Data/Parameter/final_parameters_lookahead_A.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array[18:19, :], dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array[18:19, :], dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array[18:19, :], dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array[18:19, 18:19],
                                                   dtype=tf.float32)

    df = pd.read_csv("wwr/Data/londonsim.csv")

    data_array = df.values
    UKmeasles = tf.convert_to_tensor(data_array, dtype=tf.float32)                                               

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype = tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)
    initial_pop = UKpop[:,0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14/tf.cast(intermediate_steps, dtype = tf.float32), dtype = tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(T, intermediate_steps, term, school)

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype = tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype = tf.float32)
    p      = tf.constant(0.759, dtype = tf.float32)
    delta_year = tf.convert_to_tensor([[1/50]], dtype = tf.float32)*tf.ones((n_cities, 4), dtype = tf.float32)

    # increase the n_experiments for proper variance estimates, use n_experiments to just test the log-likelihood
    n_experiments = 20

    # lookahead
    # shared
    best_parameters = np.load(os.path.join("wwr/E2", "E2_param_exp.npz"))["E2_param_exp"]
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)
    q_mean = tf.constant(np.mean(np.load("wwr/Data/q_mean.npy")), dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)

    # --- parameter block (same format, new values) -----------------------
    pi_0_1 = float(best_parameters[0])
    pi_0_2 = float(best_parameters[1])
    pi_0_3 = float(best_parameters[2])
    pi_0   = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(best_parameters[3] * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([best_parameters[4]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([best_parameters[5]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = tf.convert_to_tensor([[0.0]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(best_parameters[6], dtype=tf.float32)
    c      = tf.constant(best_parameters[7] , dtype=tf.float32)
    xi_var = 10*tf.convert_to_tensor(best_parameters[8] , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(best_parameters[9], dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=float(0.7), scale=q_var, low=0.0, high=1.0)

    n_particles = 5000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = np.max(x)
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_minus_k = np.delete(x, k)
                max_x_mk = np.max(x_minus_k)
                jk_vals[k] = max_x_mk + np.log(np.mean(np.exp(x_minus_k - max_x_mk)))
            xse = (n - 1) * np.std(jk_vals, ddof=1) / np.sqrt(n)
            results["se"] = xse
        if ess:
            w = np.exp(x - max_x)
            xss = np.sum(w) ** 2 / np.sum(w**2)
            results["ess"] = xss
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_res(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    mean_log = np.mean(log_likelihood_shared)

    print("Variance of log likelihoods:", variance_log)
    print("mean of log likelihoods:", mean_log)

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>  ORIGINAL CODE END  <<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # ------ save array to cache so future runs can skip heavy work ---
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached  {CACHE_FILE}")

# --------------------------------------------------------------------
# 3) Quick summary (identical whether loaded or freshly computed)
# --------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    """
    Return (log-mean-exp, jackknife SE) for a 1-D array of log-likelihoods.
    """
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))

    # jackknife
    jk_vals = np.empty(n)
    for k in range(n):
        x_k = np.delete(x, k)
        max_k = x_k.max()
        jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))

    se = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E2_est = float(lme)    
E2_se  = float(se) 
```


```{python E3, include=FALSE}
from __future__ import annotations

import os
import numpy as np

# ------------- cache settings -----------------------------------
CACHE_DIR  = "wwr/E3"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_vanilla.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# ----------------------------------------------------------------
# 1) Try to load cached results
# ----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results  {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# ----------------------------------------------------------------
# 2) No cache  run the original simulation code unmodified
# ----------------------------------------------------------------
else:
    print("[cache] No cache found  running the full simulation ")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>  ORIGINAL CODE  DO NOT EDIT  <<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    #  (Everything between the double lines is your untouched script;
    #   only indentation has changed so it nests inside this block.)
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize

    import matplotlib.pyplot as plt
    from scipy.special import logsumexp
    from sympy.polys.benchmarks.bench_solvers import uk_10

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("wwr/E3"):
        os.makedirs("wwr/E3")

    os.environ['PYTHONHASHSEED'] = '45'
    random.seed(45)
    np.random.seed(45)
    tf.random.set_seed(45)

    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")
    modelA_array = np.load("wwr/Data/Parameter/final_parameters_lookahead_A.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array[18:19, :], dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array[18:19, :], dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array[18:19, :], dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array[18:19, 18:19],
                                                   dtype=tf.float32)

    df = pd.read_csv("wwr/Data/londonsim.csv")

    data_array = df.values
    UKmeasles = tf.convert_to_tensor(data_array, dtype=tf.float32)                                               

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype = tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)
    initial_pop = UKpop[:,0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14/tf.cast(intermediate_steps, dtype = tf.float32), dtype = tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(T, intermediate_steps, term, school)

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype = tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype = tf.float32)
    p      = tf.constant(0.759, dtype = tf.float32)
    delta_year = tf.convert_to_tensor([[1/50]], dtype = tf.float32)*tf.ones((n_cities, 4), dtype = tf.float32)

    # increase the n_experiments for proper variance estimates, use n_experiments to just test the log-likelihood
    n_experiments = 20

    # lookahead
    # shared
    best_parameters = np.load(os.path.join("wwr/E2", "E2_param_exp.npz"))["E2_param_exp"]
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)
    q_mean = tf.constant(np.mean(np.load("wwr/Data/q_mean.npy")), dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)

    # --- parameter block (same format, new values) -----------------------
    pi_0_1 = float(best_parameters[0])
    pi_0_2 = float(best_parameters[1])
    pi_0_3 = float(best_parameters[2])
    pi_0   = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(best_parameters[3] * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([best_parameters[4]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([best_parameters[5]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = tf.convert_to_tensor([[0.0]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(best_parameters[6], dtype=tf.float32)
    c      = tf.constant(best_parameters[7] , dtype=tf.float32)
    xi_var = 10*tf.convert_to_tensor(best_parameters[8] , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(best_parameters[9], dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=float(0.7), scale=q_var, low=0.0, high=1.0)

    n_particles = 100000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = np.max(x)
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_minus_k = np.delete(x, k)
                max_x_mk = np.max(x_minus_k)
                jk_vals[k] = max_x_mk + np.log(np.mean(np.exp(x_minus_k - max_x_mk)))
            xse = (n - 1) * np.std(jk_vals, ddof=1) / np.sqrt(n)
            results["se"] = xse
        if ess:
            w = np.exp(x - max_x)
            xss = np.sum(w) ** 2 / np.sum(w**2)
            results["ess"] = xss
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_res(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    mean_log = np.mean(log_likelihood_shared)

    print("Variance of log likelihoods:", variance_log)
    print("mean of log likelihoods:", mean_log)

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>  ORIGINAL CODE END  <<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # ------ save array to cache so future runs can skip heavy work ---
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached  {CACHE_FILE}")

# --------------------------------------------------------------------
# 3) Quick summary (identical whether loaded or freshly computed)
# --------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    """
    Return (log-mean-exp, jackknife SE) for a 1-D array of log-likelihoods.
    """
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))

    # jackknife
    jk_vals = np.empty(n)
    for k in range(n):
        x_k = np.delete(x, k)
        max_k = x_k.max()
        jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))

    se = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E3_est = float(lme)    
E3_se  = float(se) 
```

```{python E4, include = FALSE}
from __future__ import annotations

import os
import numpy as np

# ------------- cache settings -----------------------------------
CACHE_DIR  = "wwr/E4"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_lookahead.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# ----------------------------------------------------------------
# 1) Try to load cached results
# ----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results  {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# ----------------------------------------------------------------
# 2) No cache  run the original simulation code unmodified
# ----------------------------------------------------------------
else:
    print("[cache] No cache found  running the full simulation ")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>  ORIGINAL CODE  DO NOT EDIT  <<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    #  (Everything between the double lines is your untouched script;
    #   only indentation has changed so it nests inside this block.)
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize

    import matplotlib.pyplot as plt
    from scipy.special import logsumexp
    from sympy.polys.benchmarks.bench_solvers import uk_10

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("wwr/E4"):
        os.makedirs("wwr/E4")

    os.environ['PYTHONHASHSEED'] = '45'
    random.seed(45)
    np.random.seed(45)
    tf.random.set_seed(45)

    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")
    modelA_array = np.load("wwr/Data/Parameter/final_parameters_lookahead_A.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array[18:19, :], dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array[18:19, :], dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array[18:19, :], dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array[18:19, 18:19],
                                                   dtype=tf.float32)

    df = pd.read_csv("wwr/Data/londonsim.csv")

    data_array = df.values
    UKmeasles = tf.convert_to_tensor(data_array, dtype=tf.float32)                                               

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype = tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)
    initial_pop = UKpop[:,0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14/tf.cast(intermediate_steps, dtype = tf.float32), dtype = tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(T, intermediate_steps, term, school)

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype = tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype = tf.float32)
    p      = tf.constant(0.759, dtype = tf.float32)
    delta_year = tf.convert_to_tensor([[1/50]], dtype = tf.float32)*tf.ones((n_cities, 4), dtype = tf.float32)

    # increase the n_experiments for proper variance estimates, use n_experiments to just test the log-likelihood
    n_experiments = 20

    # lookahead
    # shared
    best_parameters = np.load(os.path.join("wwr/E4", "E4_param_exp.npz"))["E4_param_exp"]
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)
    q_mean = tf.constant(np.mean(np.load("wwr/Data/q_mean.npy")), dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)

    # --- parameter block (same format, new values) -----------------------
    pi_0_1 = float(best_parameters[0])
    pi_0_2 = float(best_parameters[1])
    pi_0_3 = float(best_parameters[2])
    pi_0   = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(best_parameters[3] * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([best_parameters[4]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([best_parameters[5]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = tf.convert_to_tensor([[0.0]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(best_parameters[6], dtype=tf.float32)
    c      = tf.constant(best_parameters[7] , dtype=tf.float32)
    xi_var = 10*tf.convert_to_tensor(best_parameters[8] , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(best_parameters[9], dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=float(0.7), scale=q_var, low=0.0, high=1.0)

    n_particles = 5000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = np.max(x)
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_minus_k = np.delete(x, k)
                max_x_mk = np.max(x_minus_k)
                jk_vals[k] = max_x_mk + np.log(np.mean(np.exp(x_minus_k - max_x_mk)))
            xse = (n - 1) * np.std(jk_vals, ddof=1) / np.sqrt(n)
            results["se"] = xse
        if ess:
            w = np.exp(x - max_x)
            xss = np.sum(w) ** 2 / np.sum(w**2)
            results["ess"] = xss
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_lookahead(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    mean_log = np.mean(log_likelihood_shared)

    print("Variance of log likelihoods:", variance_log)
    print("mean of log likelihoods:", mean_log)

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>  ORIGINAL CODE END  <<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # ------ save array to cache so future runs can skip heavy work ---
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached  {CACHE_FILE}")

# --------------------------------------------------------------------
# 3) Quick summary (identical whether loaded or freshly computed)
# --------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    """
    Return (log-mean-exp, jackknife SE) for a 1-D array of log-likelihoods.
    """
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))

    # jackknife
    jk_vals = np.empty(n)
    for k in range(n):
        x_k = np.delete(x, k)
        max_k = x_k.max()
        jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))

    se = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E4_est = float(lme)    
E4_se  = float(se) 
```

```{python E5, include = FALSE}
from __future__ import annotations

import os
import numpy as np

# ------------- cache settings -----------------------------------
CACHE_DIR  = "wwr/E5"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_lookahead.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# ----------------------------------------------------------------
# 1) Try to load cached results
# ----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results  {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# ----------------------------------------------------------------
# 2) No cache  run the original simulation code unmodified
# ----------------------------------------------------------------
else:
    print("[cache] No cache found  running the full simulation ")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>  ORIGINAL CODE  DO NOT EDIT  <<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    #  (Everything between the double lines is your untouched script;
    #   only indentation has changed so it nests inside this block.)
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize

    import matplotlib.pyplot as plt
    from scipy.special import logsumexp
    from sympy.polys.benchmarks.bench_solvers import uk_10

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("wwr/E5"):
        os.makedirs("wwr/E5")

    os.environ['PYTHONHASHSEED'] = '45'
    random.seed(45)
    np.random.seed(45)
    tf.random.set_seed(45)

    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")
    modelA_array = np.load("wwr/Data/Parameter/final_parameters_lookahead_A.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array[18:19, :], dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array[18:19, :], dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array[18:19, :], dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array[18:19, 18:19],
                                                   dtype=tf.float32)

    df = pd.read_csv("wwr/Data/londonsim.csv")

    data_array = df.values
    UKmeasles = tf.convert_to_tensor(data_array, dtype=tf.float32)                                               

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype = tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)
    initial_pop = UKpop[:,0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14/tf.cast(intermediate_steps, dtype = tf.float32), dtype = tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(T, intermediate_steps, term, school)

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype = tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype = tf.float32)
    p      = tf.constant(0.759, dtype = tf.float32)
    delta_year = tf.convert_to_tensor([[1/50]], dtype = tf.float32)*tf.ones((n_cities, 4), dtype = tf.float32)

    # increase the n_experiments for proper variance estimates, use n_experiments to just test the log-likelihood
    n_experiments = 20

    # lookahead
    # shared
    best_parameters = np.load(os.path.join("wwr/E4", "E4_param_exp.npz"))["E4_param_exp"]
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)
    q_mean = tf.constant(np.mean(np.load("wwr/Data/q_mean.npy")), dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)

    # --- parameter block (same format, new values) -----------------------
    pi_0_1 = float(best_parameters[0])
    pi_0_2 = float(best_parameters[1])
    pi_0_3 = float(best_parameters[2])
    pi_0   = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(best_parameters[3] * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([best_parameters[4]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([best_parameters[5]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = tf.convert_to_tensor([[0.0]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(best_parameters[6], dtype=tf.float32)
    c      = tf.constant(best_parameters[7] , dtype=tf.float32)
    xi_var = 10*tf.convert_to_tensor(best_parameters[8] , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(best_parameters[9], dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=float(0.7), scale=q_var, low=0.0, high=1.0)

    n_particles = 100000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = np.max(x)
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_minus_k = np.delete(x, k)
                max_x_mk = np.max(x_minus_k)
                jk_vals[k] = max_x_mk + np.log(np.mean(np.exp(x_minus_k - max_x_mk)))
            xse = (n - 1) * np.std(jk_vals, ddof=1) / np.sqrt(n)
            results["se"] = xse
        if ess:
            w = np.exp(x - max_x)
            xss = np.sum(w) ** 2 / np.sum(w**2)
            results["ess"] = xss
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_lookahead(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    mean_log = np.mean(log_likelihood_shared)

    print("Variance of log likelihoods:", variance_log)
    print("mean of log likelihoods:", mean_log)

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>  ORIGINAL CODE END  <<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # ------ save array to cache so future runs can skip heavy work ---
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached  {CACHE_FILE}")

# --------------------------------------------------------------------
# 3) Quick summary (identical whether loaded or freshly computed)
# --------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    """
    Return (log-mean-exp, jackknife SE) for a 1-D array of log-likelihoods.
    """
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))

    # jackknife
    jk_vals = np.empty(n)
    for k in range(n):
        x_k = np.delete(x, k)
        max_k = x_k.max()
        jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))

    se = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E5_est = float(lme)    
E5_se  = float(se) 
```

```{python E6, include = FALSE}
from __future__ import annotations

import os
import numpy as np

# ------------- cache settings -----------------------------------
CACHE_DIR  = "wwr/E6"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_lookahead.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# ----------------------------------------------------------------
# 1) Try to load cached results
# ----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results  {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# ----------------------------------------------------------------
# 2) No cache  run the original simulation code unmodified
# ----------------------------------------------------------------
else:
    print("[cache] No cache found  running the full simulation ")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>  ORIGINAL CODE  DO NOT EDIT  <<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    #  (Everything between the double lines is your untouched script;
    #   only indentation has changed so it nests inside this block.)
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize

    import matplotlib.pyplot as plt
    from scipy.special import logsumexp
    from sympy.polys.benchmarks.bench_solvers import uk_10

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("wwr/E6"):
        os.makedirs("wwr/E6")

    os.environ['PYTHONHASHSEED'] = '45'
    random.seed(45)
    np.random.seed(45)
    tf.random.set_seed(45)

    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")
    modelA_array = np.load("wwr/Data/Parameter/final_parameters_lookahead_A.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array[18:19, :], dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array[18:19, :], dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array[18:19, :], dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array[18:19, 18:19],
                                                   dtype=tf.float32)

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype = tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)
    initial_pop = UKpop[:,0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14/tf.cast(intermediate_steps, dtype = tf.float32), dtype = tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(T, intermediate_steps, term, school)

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype = tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype = tf.float32)
    p      = tf.constant(0.759, dtype = tf.float32)
    delta_year = tf.convert_to_tensor([[1/50]], dtype = tf.float32)*tf.ones((n_cities, 4), dtype = tf.float32)

    # increase the n_experiments for proper variance estimates, use n_experiments to just test the log-likelihood
    n_experiments = 20

    # lookahead
    # shared
    best_parameters = np.load(os.path.join("wwr/E6", "E6_param_exp.npz"))["E6_param_exp"]
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)
    q_mean = tf.constant(np.mean(np.load("wwr/Data/q_mean.npy")), dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)

    # --- parameter block (same format, new values) -----------------------
    pi_0_1 = float(best_parameters[0])
    pi_0_2 = float(best_parameters[1])
    pi_0_3 = float(best_parameters[2])
    pi_0   = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(best_parameters[3] * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([best_parameters[4]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([best_parameters[5]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = tf.convert_to_tensor([[0.0]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(best_parameters[6], dtype=tf.float32)
    c      = tf.constant(best_parameters[7] , dtype=tf.float32)
    xi_var = 10*tf.convert_to_tensor(best_parameters[8] , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(best_parameters[9], dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=float(best_parameters[10]), scale=q_var, low=0.0, high=1.0)

    n_particles = 5000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = np.max(x)
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_minus_k = np.delete(x, k)
                max_x_mk = np.max(x_minus_k)
                jk_vals[k] = max_x_mk + np.log(np.mean(np.exp(x_minus_k - max_x_mk)))
            xse = (n - 1) * np.std(jk_vals, ddof=1) / np.sqrt(n)
            results["se"] = xse
        if ess:
            w = np.exp(x - max_x)
            xss = np.sum(w) ** 2 / np.sum(w**2)
            results["ess"] = xss
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_lookahead(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    mean_log = np.mean(log_likelihood_shared)

    print("Variance of log likelihoods:", variance_log)
    print("mean of log likelihoods:", mean_log)

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>  ORIGINAL CODE END  <<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # ------ save array to cache so future runs can skip heavy work ---
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached  {CACHE_FILE}")

# --------------------------------------------------------------------
# 3) Quick summary (identical whether loaded or freshly computed)
# --------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    """
    Return (log-mean-exp, jackknife SE) for a 1-D array of log-likelihoods.
    """
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))

    # jackknife
    jk_vals = np.empty(n)
    for k in range(n):
        x_k = np.delete(x, k)
        max_k = x_k.max()
        jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))

    se = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E6_est = float(lme)    
E6_se  = float(se) 
```

```{python E9, include = FALSE}
#!/usr/bin/env python3
# ================================================================
#  Block-particle PAL experiment (40 cities) with on-disk caching
# ================================================================

from __future__ import annotations
import os
import numpy as np

# ----------------------------- cache -----------------------------
CACHE_DIR  = "wwr/E9"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_res_40.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# -----------------------------------------------------------------
# 1) Load cached results if available
# -----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results  {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# -----------------------------------------------------------------
# 2) If no cache, run the (heavy) original code unmodified
# -----------------------------------------------------------------
else:
    print("[cache] No cache found  running the full simulation ")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>> ORIGINAL CODE  DO NOT EDIT <<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize
    import matplotlib.pyplot as plt

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("wwr/E9"):
        os.makedirs("wwr/E9")

    os.environ['PYTHONHASHSEED'] = '42'
    random.seed(42)
    np.random.seed(42)
    tf.random.set_seed(42)

    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array, dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array, dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array, dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array, dtype=tf.float32)

    df = pd.read_csv("wwr/Data/M40.csv")
    data_array = df.values
    UKmeasles = tf.convert_to_tensor(data_array, dtype=tf.float32)

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype=tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=tf.float32)

    n_cities = tf.constant(40, dtype=tf.int64)
    initial_pop = UKpop[:, 0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14 / tf.cast(intermediate_steps, dtype=tf.float32), dtype=tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(
        T, intermediate_steps, term, school
    )

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype=tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype=tf.float32)
    p = tf.constant(0.759, dtype=tf.float32)
    delta_year = tf.convert_to_tensor([[1 / 50]], dtype=tf.float32) * tf.ones((n_cities, 4), dtype=tf.float32)

    n_experiments = 20

    best_parameters = np.load(os.path.join("wwr/E9", "E9_param_exp.npz"))["E9_param_exp"]
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)
    q_mean = tf.constant(np.mean(np.load("wwr/Data/q_mean.npy")), dtype = tf.float32)

    # --- parameter block (same format, new values) -----------------------
    pi_0_1 = float(best_parameters[0])
    pi_0_2 = float(best_parameters[1])
    pi_0_3 = float(best_parameters[2])
    pi_0   = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(best_parameters[3] * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([best_parameters[4]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([best_parameters[5]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = 100 * tf.convert_to_tensor([[6]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(best_parameters[7], dtype=tf.float32)
    c      = tf.constant(best_parameters[8] , dtype=tf.float32)
    xi_var = 10*tf.convert_to_tensor(best_parameters[9] , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(best_parameters[10], dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=0.7, scale=q_var, low=0.0, high=1.0)

    n_particles = 5000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = x.max()
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_k = np.delete(x, k)
                max_k = x_k.max()
                jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))
            results["se"] = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
        if ess:
            w = np.exp(x - max_x)
            results["ess"] = (w.sum() ** 2) / (w ** 2).sum()
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_res(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    print("Variance of log likelihoods:", variance_log)
    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>> ORIGINAL CODE END <<<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # -------------- cache the array for future runs -----------------
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached  {CACHE_FILE}")

# -------------------------------------------------------------------
# 3) Unified summary  always recomputed (includes jackknife SE)
# -------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))
    jk = np.array([
        (np.delete(x, k).max() +
         np.log(np.mean(np.exp(np.delete(x, k) - np.delete(x, k).max()))))
        for k in range(n)
    ])
    se = (n - 1) * jk.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E9_est = float(lme)    
E9_se  = float(se) 

```


```{python E10, include = FALSE}
#!/usr/bin/env python3
# ================================================================
#  Block-particle PAL experiment (40 cities) with on-disk caching
# ================================================================

from __future__ import annotations
import os
import numpy as np

# ----------------------------- cache -----------------------------
CACHE_DIR  = "wwr/E10"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_lookahead_40.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# -----------------------------------------------------------------
# 1) Load cached results if available
# -----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results  {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# -----------------------------------------------------------------
# 2) If no cache, run the (heavy) original code unmodified
# -----------------------------------------------------------------
else:
    print("[cache] No cache found  running the full simulation ")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>> ORIGINAL CODE  DO NOT EDIT <<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize
    import matplotlib.pyplot as plt

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("wwr/E10"):
        os.makedirs("wwr/E10")

    os.environ['PYTHONHASHSEED'] = '42'
    random.seed(42)
    np.random.seed(42)
    tf.random.set_seed(42)

    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array, dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array, dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array, dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array, dtype=tf.float32)

    df = pd.read_csv("wwr/Data/M40.csv")
    data_array = df.values
    UKmeasles = tf.convert_to_tensor(data_array, dtype=tf.float32)

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype=tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=tf.float32)

    n_cities = tf.constant(40, dtype=tf.int64)
    initial_pop = UKpop[:, 0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14 / tf.cast(intermediate_steps, dtype=tf.float32), dtype=tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(
        T, intermediate_steps, term, school
    )

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype=tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype=tf.float32)
    p = tf.constant(0.759, dtype=tf.float32)
    delta_year = tf.convert_to_tensor([[1 / 50]], dtype=tf.float32) * tf.ones((n_cities, 4), dtype=tf.float32)

    n_experiments = 20

    best_parameters = np.load(os.path.join("wwr/E10", "E10_param_exp.npz"))["E10_param_exp"]
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)
    q_mean = tf.constant(np.mean(np.load("wwr/Data/q_mean.npy")), dtype = tf.float32)

    # --- parameter block (same format, new values) -----------------------
    pi_0_1 = float(best_parameters[0])
    pi_0_2 = float(best_parameters[1])
    pi_0_3 = float(best_parameters[2])
    pi_0   = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(best_parameters[3] * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([best_parameters[4]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([best_parameters[5]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = 100 * tf.convert_to_tensor([[6]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(best_parameters[7], dtype=tf.float32)
    c      = tf.constant(best_parameters[8] , dtype=tf.float32)
    xi_var = 10*tf.convert_to_tensor(best_parameters[9] , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(best_parameters[10], dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=0.7, scale=q_var, low=0.0, high=1.0)

    n_particles = 5000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = x.max()
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_k = np.delete(x, k)
                max_k = x_k.max()
                jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))
            results["se"] = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
        if ess:
            w = np.exp(x - max_x)
            results["ess"] = (w.sum() ** 2) / (w ** 2).sum()
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_lookahead(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    print("Variance of log likelihoods:", variance_log)
    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>> ORIGINAL CODE END <<<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # -------------- cache the array for future runs -----------------
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached  {CACHE_FILE}")

# -------------------------------------------------------------------
# 3) Unified summary  always recomputed (includes jackknife SE)
# -------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))
    jk = np.array([
        (np.delete(x, k).max() +
         np.log(np.mean(np.exp(np.delete(x, k) - np.delete(x, k).max()))))
        for k in range(n)
    ])
    se = (n - 1) * jk.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E10_est = float(lme)    
E10_se  = float(se) 

```


```{python E12, include = FALSE}
#!/usr/bin/env python3
# ================================================================
#  Block-particle PAL experiment (40 cities) with on-disk caching
# ================================================================

from __future__ import annotations
import os
import numpy as np

# ----------------------------- cache -----------------------------
CACHE_DIR  = "wwr/E12"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_res_40.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# -----------------------------------------------------------------
# 1) Load cached results if available
# -----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results  {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# -----------------------------------------------------------------
# 2) If no cache, run the (heavy) original code unmodified
# -----------------------------------------------------------------
else:
    print("[cache] No cache found  running the full simulation ")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>> ORIGINAL CODE  DO NOT EDIT <<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize
    import matplotlib.pyplot as plt

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("wwr/E12"):
        os.makedirs("wwr/E12")

    os.environ['PYTHONHASHSEED'] = '42'
    random.seed(42)
    np.random.seed(42)
    tf.random.set_seed(42)

    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array, dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array, dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array, dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array, dtype=tf.float32)

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype=tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=tf.float32)

    n_cities = tf.constant(40, dtype=tf.int64)
    initial_pop = UKpop[:, 0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14 / tf.cast(intermediate_steps, dtype=tf.float32), dtype=tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(
        T, intermediate_steps, term, school
    )

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype=tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype=tf.float32)
    p = tf.constant(0.759, dtype=tf.float32)
    delta_year = tf.convert_to_tensor([[1 / 50]], dtype=tf.float32) * tf.ones((n_cities, 4), dtype=tf.float32)

    n_experiments = 20

    best_parameters = np.load(os.path.join("wwr/E12", "E12_param_exp.npz"))["E12_param_exp"]
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)
    q_mean = tf.constant(np.mean(np.load("wwr/Data/q_mean.npy")), dtype = tf.float32)

    # --- parameter block (same format, new values) -----------------------
    pi_0_1 = float(best_parameters[0])
    pi_0_2 = float(best_parameters[1])
    pi_0_3 = float(best_parameters[2])
    pi_0   = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(best_parameters[3] * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([best_parameters[4]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([best_parameters[5]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = 100 * tf.convert_to_tensor([[6]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(best_parameters[7], dtype=tf.float32)
    c      = tf.constant(best_parameters[8] , dtype=tf.float32)
    xi_var = 10*tf.convert_to_tensor(best_parameters[9] , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(best_parameters[10], dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=q_mean, scale=q_var, low=0.0, high=1.0)

    n_particles = 5000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = x.max()
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_k = np.delete(x, k)
                max_k = x_k.max()
                jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))
            results["se"] = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
        if ess:
            w = np.exp(x - max_x)
            results["ess"] = (w.sum() ** 2) / (w ** 2).sum()
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_res(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    print("Variance of log likelihoods:", variance_log)
    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>> ORIGINAL CODE END <<<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # -------------- cache the array for future runs -----------------
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached  {CACHE_FILE}")

# -------------------------------------------------------------------
# 3) Unified summary  always recomputed (includes jackknife SE)
# -------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))
    jk = np.array([
        (np.delete(x, k).max() +
         np.log(np.mean(np.exp(np.delete(x, k) - np.delete(x, k).max()))))
        for k in range(n)
    ])
    se = (n - 1) * jk.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E12_est = float(lme)    
E12_se  = float(se) 
```

```{python E13, include = FALSE}
#!/usr/bin/env python3
# ================================================================
#  Block-particle PAL experiment (40 cities) with on-disk caching
# ================================================================

from __future__ import annotations
import os
import numpy as np

# ----------------------------- cache -----------------------------
CACHE_DIR  = "wwr/E13"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_res_40.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# -----------------------------------------------------------------
# 1) Load cached results if available
# -----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results  {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# -----------------------------------------------------------------
# 2) If no cache, run the (heavy) original code unmodified
# -----------------------------------------------------------------
else:
    print("[cache] No cache found  running the full simulation ")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>> ORIGINAL CODE  DO NOT EDIT <<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize
    import matplotlib.pyplot as plt

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("wwr/E13"):
        os.makedirs("wwr/E13")

    os.environ['PYTHONHASHSEED'] = '42'
    random.seed(42)
    np.random.seed(42)
    tf.random.set_seed(42)

    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array, dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array, dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array, dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array, dtype=tf.float32)

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype=tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=tf.float32)

    n_cities = tf.constant(40, dtype=tf.int64)
    initial_pop = UKpop[:, 0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14 / tf.cast(intermediate_steps, dtype=tf.float32), dtype=tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(
        T, intermediate_steps, term, school
    )

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype=tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype=tf.float32)
    p = tf.constant(0.759, dtype=tf.float32)
    delta_year = tf.convert_to_tensor([[1 / 50]], dtype=tf.float32) * tf.ones((n_cities, 4), dtype=tf.float32)

    n_experiments = 20

    best_parameters = np.load(os.path.join("wwr/E12", "E12_param_exp.npz"))["E12_param_exp"]
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)
    q_mean = tf.constant(np.mean(np.load("wwr/Data/q_mean.npy")), dtype = tf.float32)

    # --- parameter block (same format, new values) -----------------------
    pi_0_1 = float(best_parameters[0])
    pi_0_2 = float(best_parameters[1])
    pi_0_3 = float(best_parameters[2])
    pi_0   = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(best_parameters[3] * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([best_parameters[4]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([best_parameters[5]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = 100 * tf.convert_to_tensor([[6]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(best_parameters[7], dtype=tf.float32)
    c      = tf.constant(best_parameters[8] , dtype=tf.float32)
    xi_var = 10*tf.convert_to_tensor(best_parameters[9] , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(best_parameters[10], dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=q_mean, scale=q_var, low=0.0, high=1.0)

    n_particles = 100000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = x.max()
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_k = np.delete(x, k)
                max_k = x_k.max()
                jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))
            results["se"] = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
        if ess:
            w = np.exp(x - max_x)
            results["ess"] = (w.sum() ** 2) / (w ** 2).sum()
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_res(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    print("Variance of log likelihoods:", variance_log)
    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>> ORIGINAL CODE END <<<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # -------------- cache the array for future runs -----------------
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached  {CACHE_FILE}")

# -------------------------------------------------------------------
# 3) Unified summary  always recomputed (includes jackknife SE)
# -------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))
    jk = np.array([
        (np.delete(x, k).max() +
         np.log(np.mean(np.exp(np.delete(x, k) - np.delete(x, k).max()))))
        for k in range(n)
    ])
    se = (n - 1) * jk.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E13_est = float(lme)    
E13_se  = float(se) 


```

```{python E14, include = FALSE}
#!/usr/bin/env python3
# ================================================================
#  PAL-lookahead experiment (40 cities, E10) with on-disk caching
# ================================================================
from __future__ import annotations
import os
import numpy as np

# ---------------------------- cache ------------------------------
CACHE_DIR  = "wwr/E14"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_lookahead_40.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# -----------------------------------------------------------------
# 1) Load cached results if available
# -----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results  {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# -----------------------------------------------------------------
# 2) Otherwise run the original heavy simulation (unmodified)
# -----------------------------------------------------------------
else:
    print("[cache] No cache found  running the full simulation ")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>> ORIGINAL CODE  DO NOT EDIT <<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize
    import matplotlib.pyplot as plt

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("E14"):
        os.makedirs("E14")

    os.environ['PYTHONHASHSEED'] = '42'
    random.seed(42)
    np.random.seed(42)
    tf.random.set_seed(42)

    
    
    #############################################
    # Load the data
    #############################################
    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array, dtype = tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array, dtype = tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array, dtype = tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array, dtype = tf.float32)

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype = tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)

    n_cities = tf.constant(40, dtype = tf.int64)

    initial_pop = UKpop[:,0]

    T = UKmeasles.shape[1]
    intermediate_steps = 4
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(T, intermediate_steps, term, school)

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype = tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype = tf.float32)

    intermediate_steps = tf.constant(4, dtype = tf.float32)
    h = tf.constant(14/tf.cast(intermediate_steps, dtype = tf.float32), dtype = tf.float32)
    T = tf.constant(T, dtype = tf.float32)

    delta_year = tf.convert_to_tensor([[1/50]], dtype = tf.float32)*tf.ones((n_cities, 4), dtype = tf.float32)

    p      = tf.constant(0.759, dtype = tf.float32)
    q_mean = tf.constant(np.mean(np.load("wwr/Data/q_mean.npy")), dtype = tf.float32)

    # increase the n_experiments for proper variance estimates, use n_experiments to just test the log-likelihood
    n_experiments = 20

    # lookahead
    # shared

    best_parameters = np.load("wwr/Data/Parameter/final_parameters_lookahead_A.npy")
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)

    n_cities = tf.constant(40, dtype = tf.int64)

    pi_0_1 = best_parameters[0]
    pi_0_2 = best_parameters[1] 
    pi_0_3 = best_parameters[2] 
    pi_0 = tf.convert_to_tensor([[pi_0_1, pi_0_2, pi_0_3, 1 - pi_0_1 - pi_0_2 - pi_0_3]], dtype = tf.float32)*tf.ones((n_cities, 4), dtype = tf.float32)

    initial_pop = UKpop[:,0]

    beta_bar  = tf.convert_to_tensor((best_parameters[3])*tf.ones((n_cities,1)), dtype = tf.float32)
    rho   = tf.convert_to_tensor([[best_parameters[4]]], dtype = tf.float32)*tf.ones((n_cities, 1), dtype = tf.float32)
    gamma = tf.convert_to_tensor([[best_parameters[5]]], dtype = tf.float32)*tf.ones((n_cities, 1), dtype = tf.float32)

    g = tf.convert_to_tensor([[float(100)*best_parameters[6]]], dtype = tf.float32)*tf.ones((n_cities, 1), dtype = tf.float32) 
    a = tf.constant(best_parameters[7],   dtype = tf.float32)
    c = tf.constant(best_parameters[8],   dtype = tf.float32)
    xi_var = 10*tf.convert_to_tensor(best_parameters[9], dtype = tf.float32)
    q_var  = tf.convert_to_tensor(best_parameters[10], dtype = tf.float32)


    Xi = tfp.distributions.Gamma(concentration = xi_var, rate = xi_var)
    Q  = tfp.distributions.TruncatedNormal( q_mean, q_var, 0, 1)

    n_particles = 5000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = x.max()
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_k = np.delete(x, k)
                max_k = x_k.max()
                jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))
            results["se"] = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
        if ess:
            w = np.exp(x - max_x)
            results["ess"] = (w.sum() ** 2) / (w ** 2).sum()
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (PAL_run_likelihood_lookahead(T, intermediate_steps, UKmeasles, UKbirths, UKpop, g, measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a, is_school_term_array, is_start_school_year_array, h, rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year))[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    print(f"Comp.time: {time.perf_counter() - start_time:.2f} s")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])
    print("Variance:", np.var(log_likelihood_shared, ddof=1))

    np.savetxt(os.path.join("wwr", "E14", "PAL_lookahead_40.csv"),
               log_likelihood_shared, delimiter=",")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>> ORIGINAL CODE END <<<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # -------------- cache the results for future runs ----------------
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached  {CACHE_FILE}")

# -------------------------------------------------------------------
# 3) Unified summary  recomputed every run
# -------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))
    jk = np.array([
        (np.delete(x, k).max() +
         np.log(np.mean(np.exp(np.delete(x, k) - np.delete(x, k).max()))))
        for k in range(n)
    ])
    se = (n - 1) * jk.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E14_est = float(lme)    
E14_se  = float(se)
```

```{python E15, include = FALSE}
#!/usr/bin/env python3
# ================================================================
#  PAL-lookahead experiment (40 cities, E10) with on-disk caching
# ================================================================
from __future__ import annotations
import os
import numpy as np

# ---------------------------- cache ------------------------------
CACHE_DIR  = "wwr/E15"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_lookahead_40.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# -----------------------------------------------------------------
# 1) Load cached results if available
# -----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results  {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# -----------------------------------------------------------------
# 2) Otherwise run the original heavy simulation (unmodified)
# -----------------------------------------------------------------
else:
    print("[cache] No cache found  running the full simulation ")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>> ORIGINAL CODE  DO NOT EDIT <<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize
    import matplotlib.pyplot as plt

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("E15"):
        os.makedirs("E15")

    os.environ['PYTHONHASHSEED'] = '42'
    random.seed(42)
    np.random.seed(42)
    tf.random.set_seed(42)

    
    
    #############################################
    # Load the data
    #############################################
    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array, dtype = tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array, dtype = tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array, dtype = tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array, dtype = tf.float32)

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype = tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)

    n_cities = tf.constant(40, dtype = tf.int64)

    initial_pop = UKpop[:,0]

    T = UKmeasles.shape[1]
    intermediate_steps = 4
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(T, intermediate_steps, term, school)

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype = tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype = tf.float32)

    intermediate_steps = tf.constant(4, dtype = tf.float32)
    h = tf.constant(14/tf.cast(intermediate_steps, dtype = tf.float32), dtype = tf.float32)
    T = tf.constant(T, dtype = tf.float32)

    delta_year = tf.convert_to_tensor([[1/50]], dtype = tf.float32)*tf.ones((n_cities, 4), dtype = tf.float32)

    p      = tf.constant(0.759, dtype = tf.float32)
    q_mean = tf.constant(np.mean(np.load("wwr/Data/q_mean.npy")), dtype = tf.float32)

    # increase the n_experiments for proper variance estimates, use n_experiments to just test the log-likelihood
    n_experiments = 20

    # lookahead
    # shared

    best_parameters = np.load("wwr/Data/Parameter/final_parameters_lookahead_A.npy")
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)

    n_cities = tf.constant(40, dtype = tf.int64)

    pi_0_1 = best_parameters[0]
    pi_0_2 = best_parameters[1] 
    pi_0_3 = best_parameters[2] 
    pi_0 = tf.convert_to_tensor([[pi_0_1, pi_0_2, pi_0_3, 1 - pi_0_1 - pi_0_2 - pi_0_3]], dtype = tf.float32)*tf.ones((n_cities, 4), dtype = tf.float32)

    initial_pop = UKpop[:,0]

    beta_bar  = tf.convert_to_tensor((best_parameters[3])*tf.ones((n_cities,1)), dtype = tf.float32)
    rho   = tf.convert_to_tensor([[best_parameters[4]]], dtype = tf.float32)*tf.ones((n_cities, 1), dtype = tf.float32)
    gamma = tf.convert_to_tensor([[best_parameters[5]]], dtype = tf.float32)*tf.ones((n_cities, 1), dtype = tf.float32)

    g = tf.convert_to_tensor([[float(100)*best_parameters[6]]], dtype = tf.float32)*tf.ones((n_cities, 1), dtype = tf.float32) 
    a = tf.constant(best_parameters[7],   dtype = tf.float32)
    c = tf.constant(best_parameters[8],   dtype = tf.float32)
    xi_var = 10*tf.convert_to_tensor(best_parameters[9], dtype = tf.float32)
    q_var  = tf.convert_to_tensor(best_parameters[10], dtype = tf.float32)


    Xi = tfp.distributions.Gamma(concentration = xi_var, rate = xi_var)
    Q  = tfp.distributions.TruncatedNormal( q_mean, q_var, 0, 1)

    n_particles = 100000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = x.max()
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_k = np.delete(x, k)
                max_k = x_k.max()
                jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))
            results["se"] = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
        if ess:
            w = np.exp(x - max_x)
            results["ess"] = (w.sum() ** 2) / (w ** 2).sum()
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (PAL_run_likelihood_lookahead(T, intermediate_steps, UKmeasles, UKbirths, UKpop, g, measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a, is_school_term_array, is_start_school_year_array, h, rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year))[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    print(f"Comp.time: {time.perf_counter() - start_time:.2f} s")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])
    print("Variance:", np.var(log_likelihood_shared, ddof=1))

    np.savetxt(os.path.join("wwr", "E15", "PAL_lookahead_40.csv"),
               log_likelihood_shared, delimiter=",")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>> ORIGINAL CODE END <<<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # -------------- cache the results for future runs ----------------
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached  {CACHE_FILE}")

# -------------------------------------------------------------------
# 3) Unified summary  recomputed every run
# -------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))
    jk = np.array([
        (np.delete(x, k).max() +
         np.log(np.mean(np.exp(np.delete(x, k) - np.delete(x, k).max()))))
        for k in range(n)
    ])
    se = (n - 1) * jk.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E15_est = float(lme)    
E15_se  = float(se)
```

```{r bridge,include = FALSE}
library(reticulate)

E2_result <- c(py$E2_est,  py$E2_se)    # c(, )
E3_result <- c(py$E3_est,  py$E3_se)
E4_result <- c(py$E4_est,  py$E4_se)
E5_result <- c(py$E5_est,  py$E5_se)
E6_result <- c(py$E6_est,  py$E6_se)
E9_result <- c(py$E9_est,  py$E9_se)
E10_result <- c(py$E10_est, py$E10_se)
E12_result <- c(py$E12_est, py$E12_se)
E13_result <- c(py$E13_est, py$E13_se)
E14_result <- c(py$E14_est, py$E14_se)
E15_result <- c(py$E15_est, py$E15_se)


E1_result_vec <- c(as.numeric(E1_result[1]),  as.numeric(E1_result[2]),as.numeric(tmp_benchmark$total),as.numeric(tmp_negbinom))

E7_result_vec <- c(as.numeric(E7_result[1]),  as.numeric(E7_result[2]),as.numeric(real_one_benchmark$total),as.numeric(real_one_negbinom))

E8_result_vec <- c(as.numeric(E8_result[1]),  as.numeric(E8_result[2]),as.numeric(tmp_benchmark_spat$total),as.numeric(tmp_negbinom_spat))


E11_result_vec <- c(as.numeric(E11_result[1]),  as.numeric(E11_result[2]))

real_vec <- c(as.numeric(realdata_benchmark_spat$total),as.numeric(realdata_negbinom_spat))

E1_lambda <- E1_result_vec[1];  E1_sigma <- E1_result_vec[2]
E2_lambda <- E2_result[1];      E2_sigma <- E2_result[2]
E3_lambda <- E3_result[1];      E3_sigma <- E3_result[2]
E4_lambda <- E4_result[1];      E4_sigma <- E4_result[2]
E5_lambda <- E5_result[1];      E5_sigma <- E5_result[2]
E6_lambda <- E6_result[1];      E6_sigma <- E6_result[2]
E7_lambda <- E7_result_vec[1];  E7_sigma <- E7_result_vec[2]
E8_lambda <- E8_result_vec[1];  E8_sigma <- E8_result_vec[2]
E9_lambda <- E9_result[1];      E9_sigma <- E9_result[2]
E10_lambda<- E10_result[1];     E10_sigma<- E10_result[2]
E11_lambda<- E11_result[1];     E11_sigma<- E11_result[2]
E12_lambda<- E12_result[1];     E12_sigma<- E12_result[2]
E13_lambda<- E13_result[1];     E13_sigma<- E13_result[2]
E14_lambda<- E14_result[1];     E14_sigma<- E14_result[2]
E15_lambda<- E15_result[1];     E15_sigma<- E15_result[2]

E1_AMRA <- E1_result_vec[3];  E1_NegB <- E1_result_vec[4]
E2_AMRA <- E1_result_vec[3];      E2_NegB <- E1_result_vec[4]
E3_AMRA <- E1_result_vec[3];      E3_NegB <- E1_result_vec[4]
E4_AMRA <- E1_result_vec[3];      E4_NegB <- E1_result_vec[4]
E5_AMRA <- E1_result_vec[3];      E5_NegB <- E1_result_vec[4]
E6_AMRA <- E7_result_vec[3];      E6_NegB <- E7_result_vec[4]
E7_AMRA <- E7_result_vec[3];      E7_NegB <- E7_result_vec[4]
E8_AMRA <- E8_result_vec[3];  E8_NegB <- E8_result_vec[4]
E9_AMRA <- E8_result_vec[3];      E9_NegB <- E8_result_vec[4]
E10_AMRA<- E8_result_vec[3];     E10_NegB<- E8_result_vec[4]
E11_AMRA<- real_vec[1];     E11_NegB<- real_vec[2]
E12_AMRA<- real_vec[1];     E12_NegB<- real_vec[2]
E13_AMRA<- real_vec[1];     E13_NegB<- real_vec[2]
E14_AMRA<- real_vec[1];     E14_NegB<- real_vec[2]
E15_AMRA<- real_vec[1];     E15_NegB<- real_vec[2]
```

\begin{table}[ht]
\centering
\begin{tabular}{lcccccccc}
\toprule
\textbf{$E$} & \textbf{$F$} & \textbf{$J$} & \textbf{$U$} & \textbf{$f_C$} &
$\lambda$ & $\sigma$ & ARMA & NegBinom \\
\midrule
$E_1$   & $\mathrm{BPF}$ & $J_2$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E1_lambda)`$  &
$`r sprintf('%.2f', E1_sigma)`$  &
$`r sprintf('%.2f', E1_AMRA)`$   &
$`r sprintf('%.2f', E1_NegB)`$   \\[2pt]

$E_2$   & $\PALV$        & $J_1$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E2_lambda)`$  &
$`r sprintf('%.2f', E2_sigma)`$  &
$`r sprintf('%.2f', E2_AMRA)`$   &
$`r sprintf('%.2f', E2_NegB)`$   \\[2pt]

$E_3$   & $\PALV$        & $J_2$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E3_lambda)`$  &
$`r sprintf('%.2f', E3_sigma)`$  &
$`r sprintf('%.2f', E3_AMRA)`$   &
$`r sprintf('%.2f', E3_NegB)`$   \\[2pt]

$E_4$   & $\PALL$        & $J_1$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E4_lambda)`$  &
$`r sprintf('%.2f', E4_sigma)`$  &
$`r sprintf('%.2f', E4_AMRA)`$   &
$`r sprintf('%.2f', E4_NegB)`$   \\[2pt]

$E_5$   & $\PALL$        & $J_2$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E5_lambda)`$  &
$`r sprintf('%.2f', E5_sigma)`$  &
$`r sprintf('%.2f', E5_AMRA)`$   &
$`r sprintf('%.2f', E5_NegB)`$   \\[2pt]

$E_6$   & $\PALL$        & $J_1$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E6_lambda)`$  &
$`r sprintf('%.2f', E6_sigma)`$  &
$`r sprintf('%.2f', E6_AMRA)`$   &
$`r sprintf('%.2f', E6_NegB)`$   \\[2pt]

$E_7$   & $\mathrm{BPF}$ & $J_1$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E7_lambda)`$  &
$`r sprintf('%.2f', E7_sigma)`$  &
$`r sprintf('%.2f', E7_AMRA)`$   &
$`r sprintf('%.2f', E7_NegB)`$   \\[2pt]

$E_8$   & $\mathrm{BPF}$ & $J_2$ & $U_2$ & $C_2$ &
$`r sprintf('%.2f', E8_lambda)`$  &
$`r sprintf('%.2f', E8_sigma)`$  &
$`r sprintf('%.2f', E8_AMRA)`$   &
$`r sprintf('%.2f', E8_NegB)`$   \\[2pt]

$E_9$   & $\PALV$        & $J_1$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E9_lambda)`$  &
$`r sprintf('%.2f', E9_sigma)`$  &
$`r sprintf('%.2f', E9_AMRA)`$   &
$`r sprintf('%.2f', E9_NegB)`$   \\[2pt]

$E_{10}$ & $\PALL$       & $J_1$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E10_lambda)`$ &
$`r sprintf('%.2f', E10_sigma)`$ &
$`r sprintf('%.2f', E10_AMRA)`$  &
$`r sprintf('%.2f', E10_NegB)`$  \\[2pt]

$E_{11}$ & $\mathrm{BPF}$ & $J_2$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E11_lambda)`$ &
$`r sprintf('%.2f', E11_sigma)`$ &
$`r sprintf('%.2f', E11_AMRA)`$  &
$`r sprintf('%.2f', E11_NegB)`$  \\[2pt]

$E_{12}$ & $\PALV$       & $J_1$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E12_lambda)`$ &
$`r sprintf('%.2f', E12_sigma)`$ &
$`r sprintf('%.2f', E12_AMRA)`$  &
$`r sprintf('%.2f', E12_NegB)`$  \\[2pt]

$E_{13}$ & $\PALV$       & $J_2$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E13_lambda)`$ &
$`r sprintf('%.2f', E13_sigma)`$ &
$`r sprintf('%.2f', E13_AMRA)`$  &
$`r sprintf('%.2f', E13_NegB)`$  \\[2pt]

$E_{14}$ & $\PALL$       & $J_1$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E14_lambda)`$ &
$`r sprintf('%.2f', E14_sigma)`$ &
$`r sprintf('%.2f', E14_AMRA)`$  &
$`r sprintf('%.2f', E14_NegB)`$  \\[2pt]

$E_{15}$ & $\PALL$       & $J_2$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E15_lambda)`$ &
$`r sprintf('%.2f', E15_sigma)`$ &
$`r sprintf('%.2f', E15_AMRA)`$  &
$`r sprintf('%.2f', E15_NegB)`$  \\
\bottomrule
\end{tabular}
\caption{Loglikelihood estimate, $\lambda$,  for each experiment described in Table\ 2.
Estimates derive from averaging 20 replications on a natural scale so that the basic particle filter estimate is unbiased.
The standard error, $\sigma$, is a jack-knife estimate implemented via the logmeanexp function in the pomp R package.
ARMA gives the log-likelihood for an autoregressive moving average benchmark, and NegBinom is an autoregressive negative binomial benchmark.}
\label{tbl:method_comparison}
\end{table}




Here, experiments $E_1$--$E_5$ address Q1 directly, using simulated data on a single unit. 
$E_1$ provides a ground truth for this particular model, a single-city SEIR model.
Comparing $\lambda_2$ and $\lambda_3$ with $\lambda_1$, we see that $\PALV$, without the lookahead, performs as expected for an approximate filter.
On this relatively easy task, it produces stable estimates, with log-likelihood values somewhat below the truth. However, both $E_2$ and $E_3$ failed to outperform the AMRA benchmark in our experiments, suggesting that PAL continues to face challenges when dealing with complex SEIR models and overdispersion.
Since $\PALV$ is filtering using a model that differs slightly from the data generating model, we expect to see a small shortfall, with $\lambda_2<\lambda_1$ and  $\lambda_3<\lambda_1$.
The difference, $\lambda_2-\lambda_3$, is statistically indistinguishable from zero in this experiment, showing that $J_1=5000$ particles is adequate for $\PALV$ on a single city.

$E_4$ and $E_5$ demonstrate the positive bias of $\PALL$ both at a usual number of particles and for an intensive calculation that may not be possible on larger problems.
The best estimates of this bias are $\lambda_4-\lambda_3$ and $\lambda_5-\lambda_3$, since $\PALL$ and $\PALV$ target the same quantity in the limit as $J\to\infty$.
The smaller number of particles for $E_4$ leads to a higher log-likelihood estimate, which suggest a long left tail to the Monte Carlo likelihood estimate, leading to estimators that behave as though the variance is infinite.
In better-behaved settings, a higher number of particles is expected to lead to a higher log-likelihood estimate due to reduced negative bias resulting from Jensen's inequality.


Experiments $E_6$ and $E_7$ introduce the actual data, while still restricting to a single spatial unit.
We see that $\PALL$ reports a higher log-likelihood than BPF.
At face value, this could be because the PAL approximation is a superior model to the partially observed Markov process model implemented by BPF.
Or, it could be because BPF suffers from heavy negative bias due to high Monte Carlo error combined with Jensen's inequality.
The latter is not the case due to BPF's empirically low Monte Carlo error.
We have just discovered in $E_1$--$E_5$ that $\PALL$ reports an over-stated log-likelihood when the truth is known, so the most plausible explanation of $E_6$ and $E_7$ is simply that the same phenomenon occurs on the data.

Experiments $E_8$--$E_{15}$ investigate a 40 unit system.

For $E_8$, $E_9$ and $E_{10}$, we simulated from a model with the coupling parameter between towns set to zero.
That was done to study a situation where a block particle filter gives a consistent and low-variance estimate of the exact log-likelihood.
We see the same story as the single-unit case, where the positive bias of  $\PALL$ is estimated by $(\lambda_{10}-\lambda_{9})/40=$ `r sprintf('%.2f', (E10_lambda-E9_lambda)/40)` per unit.
This bias is large enough that we obtain $\lambda_{10}>\lambda_8$, with the difference being  $(\lambda_{10}-\lambda_{8})/40=$ `r sprintf('%.2f', (E10_lambda-E8_lambda)/40)` per unit.
We see that the $U=40$ results scale approximately linearly compared to $U=1$, providing us with confirmation of our hypothesis Q2.

Experiments $E_{11}$--$E_{15}$ consider coupled models for all 40 cities in the full, real dataset. Here, $E_{11}$ uses the same parameter combination as Model C in Table 3 of WWR for the iBPF optimization, while $E_{12}$--$E_{14}$ share an identical parameter combination (from Model A). Experiments $E_{14}$ and $E_{15}$ directly use the parameters provided by WWR. The parameter choice for Model A in $E_{12}$--$E_{14}$ was primarily motivated by computational convenience and to rapidly demonstrate the persistence of positive bias in the lookahead method for the 40-unit dataset.
However, the results are consistent with the results for a single unit and for simulated data with 40 independent cities.
$\PALL$ reports a log-likelihood which is higher than BPF, which is in turn higher than $\PALV$.
However, increasing from $J_1=5\times 10^3$ to $J_2=10^5$ particles reduced the $\PALL$ estimate by  $\lambda_{15}-\lambda_{14}=$ `r sprintf('%.2f', E15_lambda-E14_lambda)` log units. The performance of $\PALV$ on the 40-unit real dataset still fails to outperform the ARMA benchmark, further indicating that applying PAL to high-dimensional data remains challenging.
A log-likelihood estimate that is greatly reduced when the Monte Carlo estimate is greatly increased is indicative of numerical instability.
Further, we know from $E_8$--$E_{10}$ that $\PALL$ can over-state the true log-likelihood when this is known.
The evidence suggests that neither $E_{14}$ nor $E_{15}$ is a reliable estimator of either the PAL log-likelihood or the exact, unknown, log-likelihood that PAL aims to approximate.

\section{Some theoretical considerations for lookahead PAL}
\label{sec:theory}

 Since vanilla particle filter algorithms are unbiased for the likelihood, it might be reasonable to expect the PAL-SMC algorithm to be unbiased for the PAL likelihood, but this is not true for the lookahead PAL filter used by WWR. 
 This is a property of the lookahead part of the algorithm, developed by \citet{rimella23}, rather than the PAL approximation.
 Therefore, for the remainder of this section, we consider the simpler lookahead filter of \citet{rimella23}.
 
 Briefly, the vanilla particle filter is unbiased because the self-normalization constant happens to coincide with the conditional likelihood estimate. 
 Self-normalization does not always lead to unbiased likelihood estimates, as we can see from the following example.

Let $X$ take values $\{0,1\}$ with equal probability, and let $Y=X$ with probability 1. 
Suppose a single data point, $Y=1$. 
Suppose also an independent sample of $J$ particles, $x_{1:J}$, each with distribution matching $X$. 
Now, resample these particle with probability $p_j = (1-\epsilon) x_j/[\sum_j x_j] + \epsilon(1-x_j)/[\sum_j 1-x_j]$ so that, on average, a fraction $(1-\epsilon)$ of the resampled particles have value 1. 
Take $\epsilon \ll 1/J$, so that most resampled particle swarms contain no particles with value 0. 
Most particle swarms resulting from resampling will have $x_j = 1$ for all $j$, with the proper resampled weight $w_j = 1/(J\, p_j)$ being approximately $1/(2J)$ for all $j$. 
Rare swarms will have a particle with massive weight, approximately $1/(2J\epsilon)$. 
Under self-normalization, particle swarms with a massive weight will estimate the likelihood to be approximately zero, and particle swarms with $x_j=1$ for all $j$ will estimate the likelihood to be 1. 
By setting $\epsilon$ arbitrarily small, we can get an estimate whose expectation approaches 1 since with high probability we see only resampled swarms where every particle has value 1. 
If we take a different limit, with $J \to \infty$, the bias will go away asymptotically, but here we consider the case with fixed $J$ and $\epsilon \to 0$.

Importantly, the bias on the likelihood estimate in this example is positive. 
As mentioned earlier, a suboptimal forecast generally gives, on average, a negative bias on the conditional log-likelihood estimate, since log-likelihood is a proper scoring rule. 
This justifies assessing filters on their log-likelihood estimate in a similar way that one does for parameters in likelihood-based inference. 
A filter with a high log-likelihood estimate on simulated data from the target model is validated as a good likelihood approximation.
However, this does not necessarily apply to algorithms that look at future observations. When implementing lookahead algorithms, if you want the log-likelihood estimate to be conservative, you have to be extra careful to consider the bias. 
For unbiased likelihood estimates, the negative bias on the log-likelihood is a direct consequence of variance, and among such estimates it is reasonable to prefer a filter approximation with the highest log-likelihood estimate. 
For positively biased estimates, that is inappropriate.

\section{Conclusion}
\label{sec:conclusion}

The results in this article reinforce the investigation by \citet{hao24-arxiv} and lead to the conclusion that there is not currently a strong case for using PAL on the epidemiological models used to motivate its development by WWR.
Simpler particle filter methods apply to arbitrary Markov process models, whereas PAL is limited to a specific class of discrete-state Markov process models.
Basic particle filters, and their block particle filter extensions, have the plug-and-play property \citep{breto09,he10}.
Some log-likelihood optimization methods for particle filters, such as iterated filtering \citep{ionides15} and certain automatic differentiation algorithms \citep{tan24}, inherit this convenient plug-and-play property. 
PAL may potentially lead to dramatic computational improvements over particle filters some situations.
However, WWR's overdispersed generalization of PAL also requires a particle filter component, at which point it shares many of the limitations of particle filters.
The results of WWR, together with various other authors \citep{stocks18,he10,li24}, show that overdispersion is frequently necessary for a dynamic model to provide an adequate statistical description of epidemiological data.