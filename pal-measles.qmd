---
title: |
  | Poisson Approximate Likelihood versus the block particle filter for a spatiotemporal measles model
author:
  - "Kunyang He, Yize Hao and Edward L. Ionides"
  - "University of Michigan, Ann Arbor"
date: "July 5, 2025"
abstract: |
  Filtering algorithms for high-dimensional nonlinear non-Gaussian partially observed stochastic processes provide access to the likelihood function and hence enable likelihood-based or Bayesian inference for this methodologically challenging class of models.
  A novel Poisson approximate likelihood (PAL) filter was introduced by Whitehouse et al.\ (2023).
  PAL employs a Poisson approximation to conditional densities, offering a fast approximation to the likelihood function for a certain subset of partially observed Markov process models.
  PAL was demonstrated on an epidemiological metapopulation model for measles, specifically, a spatiotemporal model for disease transmission within and between cities. 
  At face value, Table\ 3 of Whitehouse et al.\ (2023) suggests that PAL considerably out-performs previous analysis as well as an ARMA benchmark model.
  We show that PAL does not outperform a block particle filter and that the lookahead component of PAL was implemented in a way that introduces substantial positive bias in the log-likelihood estimates.
  Therefore, the results of Table\ 3 of Whitehouse et al.\ (2023) do not accurately represent the true capabilities of PAL.

format:
  pdf:
    cite-method: natbib
    biblio-style: apalike
    include-in-header:
      - text: |
          \usepackage{graphicx} % Required for inserting images
          \usepackage{fullpage}
          \usepackage{authblk}
          \newcommand\loglik{\lambda}
          \newcommand\fproc{f_{\mathrm{proc}}}
          \newcommand\fmeas{f_{\mathrm{meas}}}
          \newcommand\sproc{s_{\mathrm{proc}}}
          \newcommand\smeas{s_{\mathrm{meas}}}

          \newcommand\fprocDiscrete{\mathrm{discrete}}
          \newcommand\fprocEuler{\mathrm{Euler}}
          \newcommand\fmeasBinomial{\mathrm{binomial}}
          \newcommand\fmeasGaussian{\mathrm{Gaussian}}

          \newcommand\sprocDiscrete{\mathrm{discrete}}
          \newcommand\sprocEuler{\mathrm{Euler}}
          \newcommand\smeasBinomial{\mathrm{binomial}}
          \newcommand\smeasGaussian{\mathrm{Gaussian}}

          \newcommand{\PALL}{\ensuremath{\mathrm{PAL}_L}}
          \newcommand{\PALV}{\ensuremath{\mathrm{PAL}_V}}
          \usepackage{multirow}
          \usepackage[dvipsnames]{xcolor}
          \newcommand\eic[1]{{\color{Orange} #1}}

    documentclass: article
    pdf-engine: pdflatex
    geometry: "margin=1in"
    number-sections: true
bibliography: bib-pal-measles.bib
---

```{r Setup, include=FALSE}
library(reticulate)
# use_python("/usr/local/bin/python3", required = TRUE)
use_python("/Library/Frameworks/Python.framework/Versions/3.12/bin/python3", required = TRUE)

pomp_dir="pomp/"
```

\section{Introduction}
\label{sec:intro}

Investigations of the metapopulation dynamics of measles (i.e., studying how measles infection moves within and between collections of spatially distinct populations) have motivated various methodological innovations for inference on high-dimensional partially observed stochastic processes \citep{xia04,park20,ionides23-jasa}. The analysis by \citet{whitehouse23} (henceforth, WWR) provides a new approach to model-based inference on population dynamics via the Poisson approximate likelihood (PAL) filtering algorithm.
WWR claimed impressive results on both a low-dimensional rotavirus transmission model and a high-dimensional measles model.
On close inspection, the rotavirus results turned out to be overstated \citep{hao24-arxiv} leading to a published correction \citep{whitehouse25-correction}.
However, the spatiotemporal measles results were unaffected by that correction, and our present purpose is to revisit this example.

In Section\ \ref{sec:numerics} we show by direct numerical experimentation that the WWR implementation of PAL has substantial positive bias in the log-likelihood estimate, and so the use of log-likelihood to support the use of the method is flawed.
While doing this, we show that a widely applicable block particle filter (BPF) is adequate on this problem.
In Section\ \ref{sec:theory} we explain theoretically how the positive bias for PAL in the measles example arises as a result of the lookahead mechanism included in the implementation of PAL that WWR used for this model.
The lookahead mechanism was not used by WWR for the rotavirus analysis since a basic version of PAL was sufficient for that lower-dimensional example. Section\ \ref{sec:conclusion} is a concluding discussion.

For our current purposes, we do not have to delve into the details of the measles data and model, so we provide only a brief overview.
The data are measles case counts aggregated over 2-week intervals for forty of the largest towns in England and Wales, from 1949 to 1964.
The data and the model are derived from \citet{park20}, who built on a long tradition of models described therein.
Recently, weekly data for more towns have become publicly available \citep{korevaar20}, but we limit ourselves to the data used by WWR.
The latent process model describes an integer count of infected, susceptible and recovered individuals in each town.
The rate of disease transmission within cities follows widely used epidemiological equations.
Transmission between pairs of cities follows a power law, diminishing with distance between the cities.
This is known as a gravity model.
Overdispersion for the latent dynamics is achieved by placing multiplicative gamma white noise on the transmission rate.
The measurement model is a discretized Gaussian approximation to an overdispersed binomial \citep{park20} or Gaussian noise on a binomial rate (WWR).
\citet{park20} used a particle filter known as a guided intermediate resampling filter (GIRF).
Recently, BPF has been shown to have good performance on this class of models \citep{ionides23-jasa,ionides24-sinica,ning23}. Therefore, we compare PAL with BPF.

\section{Numerical experiments for PAL and BPF}
\label{sec:numerics}

There are many possible numerical experiments that could be conducted to compare filters on spatiotemporal measles models. 
Here, we choose experiments to investigate two specific hypotheses:

\begin{enumerate}
\item[H1] The lookahead version of the Poisson approximate likelihood estimator of WWR, which we call $\PALL$, can have substantial positive bias on its log-likelihood estimate. This occurs in the spatiotemporal example of WWR.

\item[H2] The bias in the $\PALL$ log-likelihood estimate scales approximately linearly with the number of spatial units.
\end{enumerate}
We consider probabilistic filtering algorithms that are defined in the context of a model, its model parameters, and additional algorithmic parameters. 
Additionally, we require data, and this can either be the real historical measles data or can be simulated from another model that may or may not be the same model with the same parameters as used for the filter.
We also have a choice of how many spatial units to include, with each unit being one UK town in the measles example.
The experimental variables, and the list of values we consider for them, are summarized in @tbl-description and further described below.

A central part of our reasoning is that a probabilistic forecasting filter (i.e., one that solves the one-step prediction problem without looking ahead to future data) cannot, on average, obtain higher log-likelihood than the exact prediction distribution, when the data are generated by the exact model.
This is a restatement of the well-known fact that log-likelihood is a proper scoring rule \citep{gneiting07}.
To apply this property, we must work with simulated data so that the true generating model is known.
In high dimensions, it is generally not possible to calculate the exact prediction distribution to within arbitrarily small error.
Provably consistent Monte\ Carlo methods have intolerable Monte\ Carlo error, and that is the reason why approximation algorithms such as $\PALL$ are being invented.
There are two special situations where we can establish accurately the true log-likelihood for the spatiotemporal measles models of interest: (i) when the number of spatial units is very small; (ii) when there is no spatial coupling, so the filtering problem can be solved independently for each single unit.
In both these cases, a basic particle filter provides the desired, essentially exact, log-likelihood estimate.
The basic particle filter is consistent and unbiased for the likelihood \citep{delmoral04} and so, when its estimates have low empirical variance, it provides the required ground truth.
In practice, order $10^5$ particles give a highly accurate log-likelihood for one unit, but the strong sensitivity of the particle filter to the curse of dimensionality \citep{bengtsson08} means that quantifiably exact estimates rapidly become unfeasible.
Therefore, we consider two choices of size for the system, $U=1$ and $U=40$, with the latter being the size of the system tested by WWR.
To allow for the study of systems without coupling (i.e., measles transmission between cities), we consider two model variations, $C_1$ and $C_2$, where $C_1$ is the original model with gravity coupling used by WWR, and $C_2$ is a modification where direct movement of infection between cities is replaced by a constant background rate of importation of infection.

One approach to address the limitations of particle filters for high-dimensional systems is to take advantage of the possibility to design the filter's prediction distribution for the $n$th observation, at time $t_n$, to build a prediction distribution that incorporates data occurring at, or subsequent to, time $t_n$.
Such lookahead filters cannot be implemented for forecasting, but can be used for likelihood evaluation.
This is the approach adopted by WWR's lookahead filter, $\PALL$, which incorporates a particle filter to address overdispersion in the data.

There is no mathematical theorem prohibiting a lookahead filter estimating a higher likelihood than the truth, and in an extreme case the lookahead filter could just assert a one-step prediction distribution with all its mass on the actual data.
Lookahead filters therefore need careful theoretical guarantees if we want to use a high likelihood estimate as evidence for both the success of the filter and (when doing data analysis) evidence supporting the model used to construct the filter.
We will investigate the theory behind $\PALL$ later, in Section\ \ref{sec:theory}, but for now we just identify the potential hazard.

\begin{table}
\begin{tabular}{lllll}
Variable & Description & Value 1 & Value 2 & Value 3
\\
\hline
$F$ & filter algorithm & \PALL & \PALV & BPF
\\
$J$ & number of particles & $J_1=5\times 10^3$ &  $J_2=10^5$ &
\\
$U$ & number of spatial units & $U_1=1$ &  $U_2=40$ &
\\
$f_C$ & spatiotemporal coupling for filter & $C_1 =(g\neq 0, \iota = 0)$ & $C_2=(g=0, \iota\neq 0)$ &
\\
$\fproc$ & process model for filter & $\fprocEuler$ & $\fprocDiscrete$ &
\\
$\fmeas$ & measurement model for filter & $\fmeasBinomial$ & $\fmeasGaussian$ &
\\
$f_{\theta}$ & parameter for filter & $\hat\theta^*_{BPF}$ & $\hat\theta_{PAL}$ &
\\
$s_C$ & spatiotemporal coupling for simulation & $C_1 =(g\neq 0, \iota = 0)$ & $C_2=(g=0, \iota\neq 0)$ &
\\
$\sproc$ & process model for simulation & $\sprocEuler$ & $\sprocDiscrete$ &
\\
$\smeas$ & measurement model for simulation & $\smeasBinomial$ & $\smeasGaussian$ &
\\
$s_{\theta}$ & parameter for simulation & $\hat\theta^*_{BPF}$ & $\hat\theta_{PAL}$ &
\\
\hline
\end{tabular}
\caption{Variables for the numerical experiments and their set of values.}
{#tbl-description}
\end{table}

The experimental variables listed in @tbl-description are now described in more detail:

\begin{itemize}
\item[F] {\bf The filtering algorithm}. 
{\PALL} is the lookahead filter of WWR, and {\PALV} is the plain, so-called vanilla, implementation. 
BPF is the block particle filter of \citet{rebeschini15} implemented as \texttt{bpfilter} in spatPomp \citep{asfaw24}.
Likelihood optimization for {\PALL} and {\PALV} is conducted using stochastic gradient descent and automatic differentiation, using the implementation by WWR. 
Likelihood optimization for BPF is conducted using the iterated BPF algorithm \citep{ning23,ionides24-sinica} implemented as \texttt{ibpf} in spatPomp.
For $U=1$, and for $U=40$ with $g=0$, BPF is identical to a basic particle filter. 
For simplicity, we use \texttt{bpfilter} from the spatPomp package even when \texttt{pfilter} from the pomp package is equivalent.

\item[$f_C$]  {\bf Spatiotemporal coupling for the filter model}.
The choice $C=C_1$ corresponds to the coupling used by WWR, with spatial movement of infection ($g\neq 0$) and no background immigration of infection from outside the study system ($\iota=0$). 
In order to test the methods on a high-dimensional system for which the true likelihood is known to a good degree of accuracy, we also consider setting $C_2$, without coupling ($g=0$) and with compensating immigration to prevent permanent extinction of measles in small towns ($\iota\neq 0$).
When $U=U_1=1$, we use the largest city, London, for which stochastic extinctions are very unlikely. 
Note that, when $U=1$, the value of $g$ becomes irrelevant.

\item[$\fproc$]  {\bf Latent process transition model for the filter}.
For $\fproc=\fprocDiscrete$,  a single gamma-distributed dynamic noise variable is chosen for each observation interval.
This is the choice made by WWR.
For $\fproc=\fprocEuler$, independent gamma noise variables are included in each Euler time step, so that the limit of the process model (as the Euler time step decreases) corresponds to a continuous-time over-dispersed Markov chain. 
Both $\fproc=\fprocDiscrete$ and $\fproc=\fprocEuler$ are implemented with a step of $1/2$ week for the multinomial transitions conditional on the gamma noise. 

\item[$\fmeas$]  {\bf Measurement model for the filter}. The choice of WWR is $\fmeas=\fmeasBinomial$, corresponding to binomial measurements with truncated multiplicative Gaussian noise on the reporting rate, i.e., the expected fraction of infections that are reported. 
The basic PAL algorithm requires a binomial measurement model, but the SMC-PAL extension permits noise on the measurement probability.
We also consider $\fmeas=\fmeasGaussian$, corresponding to a discretized Gaussian measurement model. 
This choice leads to a measurement model that can directly be evaluated, without costly Monte Carlo calculation, assisting with efficient Monte Carlo inference.

\item[$f_{\theta}$]  {\bf Model parameter vector for the filter}. 
{\PALL} and {\PALV} are evaluated at optimized parameter vectors for each data set. 
For PAL, there is generally no true POMP model for which PAL is an exact filter.
However, we give PAL a reasonable chance to show its capabilities by optimizing it using the code provided by WWR.
When $U>1$, we must decide which parameter are shared between units and which are unit-specific.
For $\PALL$ and $\PALV$, we used case A from WWR, for which parameters are shared between units.
This ran much more quickly than their case C, and is sufficient to make our point.
$E_{12}$-$E_{15}$ use the parameter values published by WWR for their case A.
For $E_{11}$, we used case C.

\item[$\sproc$]  {\bf Process model for the simulation}. 
Always set to $\sproc=\sprocEuler$ since simulations were carried out using an implementation of the model in spatPomp. 

\item[$\smeas$]  {\bf Measurement model for the simulation}. 
Always set to $\smeas=\smeasGaussian$ since simulations were carried out using an implementation of the model in spatPomp. 



\item[$s_{\theta}$]  {\bf Model parameter vector for the simulated data}.
To have an essentially exact likelihood evaluation using BPF, we have $s_\theta=f_\theta$ for all BPF situations. 
The parameters were fixed at a value near the maximum likelihood estimate for the data.

\end{itemize}

We consider two software platforms for the experiments.
All $\PALL$ and $\PALV$ calculations were carried out using the Python code provided by WWR, and all BPF calculations were carried out using the spatPomp R package.
Simulations were carried out using spatPomp, since our reasoning depends critically on the particle filter calculations being carried out with the data drawn from the model assumed by the filter.
We present results for a single simulation for the experimental treatments with simulated data.
That decision simplifies the experimental design and permits the computational effort to focus on a few direct comparisons between the methods on a small number of simulated datasets.
The experiments carried our are described in @tbl-treatments.

\begin{table}{#tbl-treatments}
\centering
\begin{tabular}{llllllllllll}
E & F & J & U & 
  $f_C$ & $\fproc$ &  $\fmeas$ & $f_{\theta}$ &
  $s_C$ & $\sproc$ &  $\smeas$ & $s_{\theta}$ 
\\
\hline
%1 &  \PALV & $J_1$ & $U_1$ & 
%  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
%  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
%\\
$E_1$ &  BPF & $J_2$ & $U_1$ & 
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_2$ &  \PALV & $J_1$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_3$ &  \PALV & $J_2$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_4$ &  \PALL & $J_1$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_5$ &  \PALL & $J_2$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_6$ &  \PALL & $J_1$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_7$ &  BPF & $J_1$ & $U_1$ & 
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_8$ &  BPF & $J_2$ & $U_2$ & 
  $C_2$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$ &
  $C_2$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_9$ &  \PALV & $J_1$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_2$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_{10}$ &  \PALL & $J_1$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_2$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_{11}$ &  BPF & $J_2$ & $U_2$ & 
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_{12}$ &  \PALV & $J_1$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_{13}$ &  \PALV & $J_2$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_{14}$ &  \PALL & $J_1$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_{15}$ &  \PALL & $J_2$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
%4 &  \PALV & $J_2$ & $U_1$ & 
%  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
%  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
%\\
%5 &  \PALV & $J_2$ & $U_1$ & 
%  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
%  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
%\\
\hline
 \end{tabular}
\caption{Combinations of variable values used for each experiment, $E_k$, $k=1\dots 15$. The simulation settings, $s_C$, $\sproc$, $\smeas$ and $s_\theta$, are applicable only when we filter simulated data rather than real data.}
\end{table}

Each experiment, $E_k$, has two primary outcomes, a log-likelihood estimate, $\loglik_k$, and its standard error, $\sigma_k$. 
These results are tabulated in @tbl-method-comparison, together with benchmark log-likelihoods for a log-ARMA(2,1) model and a negative binomial autoregressive model.


```{r E1,include=FALSE}
## ----packages,incluxde=F,echo=F,cache=F----------------------------------------
## A simple SEIR model, with the main code sourced from UMICH STAT 531 Lecture 17 and the Whitehouse et al.\ (2023) code.
## Running on the simulated data.
library("spatPomp")
library("ggplot2")
library("tidyverse")
library("knitr")
library("doRNG")
library("doParallel")


library(pomp)

measles_cases <- read.csv(paste0(pomp_dir,"case1.csv"))
measles_covar <- read.csv(paste0(pomp_dir,"covar2.csv"))

measles_cases<- measles_cases[measles_cases$city == "LONDON", ]
measles_covar <- measles_covar[measles_covar$city == "LONDON", ]


measles_cases <-  measles_cases[,-1]
measles_covar <-  measles_covar[,-1]



colnames(measles_cases) <- c("time","cases1")
colnames(measles_covar) <- c("time",
                             "lag_birthrate1","pop1")


basic_params <- c(
  alpha       = 1,
  iota        = 0,
  betabar     = 6.32,
  c           = 0.219,
  a           = 0.1476,
  rho         = 0.142,
  gamma       = 0.0473,
  delta       = 0.02/(26*4),  # timescale transform
  sigma_xi    = 0.318,
  gaussianrho = 0.7,
  psi         = 0.306,
  g           = 0,
  S_0         = 0.02545,
  E_0         = 0.00422,
  I_0         = 0.000061
)


rproc <- Csnippet("
  double t_mod = fmod(t, 364.0);
  double br1;
  double beta1, seas1;
  double foi1;         
  double xi1;           
  double betafinal1;

  int trans_S1[2], trans_E1[2], trans_I1[2];
  double prob_S1[2], prob_E1[2], prob_I1[2];

  if ((t_mod >= 6 && t_mod < 99) ||
      (t_mod >= 115 && t_mod < 198) ||
      (t_mod >= 252 && t_mod < 299) ||
      (t_mod >= 308 && t_mod < 355)) {
    seas1 = 1.0 + a * 2 * (1 - 0.759);
  } else {
    seas1 = 1.0 - 2 * a * 0.759;
  }

  beta1 = betabar * seas1;

  if (fabs(t_mod - 248.5) < 0.5) {
    br1 = c * lag_birthrate1;
  } else {
    br1 = (1.0 - c) * lag_birthrate1 / 103.0;
  }

  double I_ratio1 = I1 / pop1;

  foi1 = pow((I1 + iota) / pop1, alpha);
 
  xi1 = rgamma(sigma_xi, 1 / sigma_xi);;
  betafinal1 = beta1 * I_ratio1 * xi1;

  int SD1 = rbinom(S1, delta);
  int ED1 = rbinom(E1, delta);
  int ID1 = rbinom(I1, delta);
  int RD1 = rbinom(R1, delta);

  S1 -= SD1;  E1 -= ED1;  I1 -= ID1;  R1 -= RD1;
  
  prob_S1[0] = exp(-dt * betafinal1);
  prob_S1[1] = 1 - exp(-dt * betafinal1);

  prob_E1[0] = exp(-dt * rho);
  prob_E1[1] = 1 - exp(-dt * rho);

  prob_I1[0] = exp(-dt * gamma);
  prob_I1[1] = 1 - exp(-dt * gamma);

  rmultinom(S1, prob_S1, 2, trans_S1);
  rmultinom(E1, prob_E1, 2, trans_E1);
  rmultinom(I1, prob_I1, 2, trans_I1);

  S1 = trans_S1[0] + rpois(br1);
  E1 = trans_E1[0] + trans_S1[1];
  I1 = trans_I1[0] + trans_E1[1];
  R1 += trans_I1[1];
  C1 += trans_I1[1];
");




## ----dmeasure-------------------------------------------------
dmeas <- Csnippet("
  double m = gaussianrho*C1;
  double v = m*(1.0-gaussianrho+psi*psi*m);
  double tol = 0.0;
  if (cases1 > 0.0) {
    lik = pnorm(cases1+0.5,m,sqrt(v)+tol,1,0)
           - pnorm(cases1-0.5,m,sqrt(v)+tol,1,0) + tol;
  } else {
    lik = pnorm(cases1+0.5,m,sqrt(v)+tol,1,0) + tol;
  }
  if (give_log) lik = log(lik);
")

## ----rmeasure-------------------------------------------------
rmeas <- Csnippet("
  double m = gaussianrho*C1;
  double v = m*(1.0-gaussianrho+psi*psi*m);
  double tol = 0.0;
  cases1 = rnorm(m,sqrt(v)+tol);
  if (cases1 > 0.0) {
    cases1 = nearbyint(cases1);
  } else {
    cases1 = 0.0;
  }
")

rinit <- Csnippet("
  double probs1[4];
  probs1[0] = S_0;
  probs1[1] = E_0;
  probs1[2] = I_0;
  probs1[3] = 1.0 - probs1[0] - probs1[1] - probs1[2];

  int counts1[4];
  rmultinom(pop1, probs1, 4, counts1);

  S1 = counts1[0];
  E1 = counts1[1];
  I1 = counts1[2];
  R1 = counts1[3];
  C1 = 0;
");

basic_log_names   <- c("rho", "gamma", "sigma_xi", "betabar", "g", "iota", "delta")
basic_logit_names <- c("a", "alpha", "c", "gaussianrho", "S_0", "E_0", "I_0", "psi")
log_names   <- basic_log_names
logit_names <- basic_logit_names
measles_partrans <- parameter_trans(
  log   = log_names,
  logit = logit_names
)

one_city_pomp <- pomp(
  data       = measles_cases,
  times      = "time",
  t0         = 0,
  rprocess   = euler(rproc, delta.t = 3.5), 
  rinit      = rinit,
  dmeasure   = dmeas,
  rmeasure   = rmeas,
  statenames = c("S1","E1","I1","R1","C1"),
  paramnames = c("alpha","iota","betabar","c","a","rho","gamma",
                 "delta","sigma_xi","g","gaussianrho","psi",
                 "S_0","E_0","I_0"),
  covar      = covariate_table(measles_covar,times = "time"),
  covarnames = c("lag_birthrate1","pop1"),
  accumvars  = c("C1")
)

coef(one_city_pomp) <- basic_params

sim <- simulate(one_city_pomp, params =  basic_params,  nsim   = 1,
                seed   = 154234)

Pomp_dir <- paste0(pomp_dir,"Pomp_E",1,"/")
if(!dir.exists(Pomp_dir)) dir.create(Pomp_dir)

stew(file=paste0(Pomp_dir,"E1_new.rda"),seed=456,{
  
  cat(capture.output(sessionInfo()),
      file=paste0(Pomp_dir,"sessionInfo.txt"),sep="\n")
  
  pf_logLik <- replicate(20,
                         logLik(pfilter(sim,Np = 100000))
  )
  
  
})
E1_result <- logmeanexp(pf_logLik,se = T)

E1_result[1]

tmp_benchmark <- arma_benchmark(sim)

tmp_benchmark$total

E1_sim <- sim@data

E1_sim <- t(E1_sim)

negloglik <- function(x) optim(par=c(0.5,0.5,1),function(theta)-sum(dnbinom(x,mu=theta[1]+theta[2]*c(0,head(x,-1)),size=theta[3],log=T)))$value

tmp_negbinom <- -sum(apply(E1_sim,2,negloglik))

sim.data <- as.data.frame(sim)

londonsim <- sim.data$cases1

df <- as.data.frame(t(londonsim)) 

colnames(df) <- 0:(length(londonsim) - 1)

write.csv(df, "londonsim.csv", row.names = FALSE)
```


```{r E7,include=FALSE}
## ----packages,incluxde=F,echo=F,cache=F----------------------------------------
## A simple SEIR model, with the main code sourced from UMICH STAT 531 Lecture 17 and the Whitehouse et al.\ (2023) code.
## Running on the real data (London unit).
library("spatPomp")
library("ggplot2")
library("tidyverse")
library("knitr")
library("doRNG")
library("doParallel")
library("doFuture")


pomp_dir="pomp/"


library(pomp)

measles_cases <- read.csv(paste0(pomp_dir,"case1.csv"))
measles_covar <- read.csv(paste0(pomp_dir,"covar2.csv"))

measles_cases<- measles_cases[measles_cases$city == "LONDON", ]
measles_covar <- measles_covar[measles_covar$city == "LONDON", ]


measles_cases <-  measles_cases[,-1]
measles_covar <-  measles_covar[,-1]



colnames(measles_cases) <- c("time","cases1")
colnames(measles_covar) <- c("time",
                             "lag_birthrate1","pop1")


basic_params <- c(
  alpha       = 1,
  iota        = 0,
  betabar     = 6.32,
  c           = 0.219,
  a           = 0.1476,
  rho         = 0.142,
  gamma       = 0.0473,
  delta       = 0.02/(26*4),  # timescale transform
  sigma_xi    = 0.318,
  gaussianrho = 0.55,
  psi         = 0.306,
  g           = 0,
  S_0         = 0.02545,
  E_0         = 0.00422,
  I_0         = 0.000061
)


rproc <- Csnippet("
  double t_mod = fmod(t, 364.0);
  double br1;
  double beta1, seas1;
  double foi1;         
  double xi1;           
  double betafinal1;

  int trans_S1[2], trans_E1[2], trans_I1[2];
  double prob_S1[2], prob_E1[2], prob_I1[2];

  if ((t_mod >= 6 && t_mod < 99) ||
      (t_mod >= 115 && t_mod < 198) ||
      (t_mod >= 252 && t_mod < 299) ||
      (t_mod >= 308 && t_mod < 355)) {
    seas1 = 1.0 + a * 2 * (1 - 0.759);
  } else {
    seas1 = 1.0 - 2 * a * 0.759;
  }

  beta1 = betabar * seas1;

  if (fabs(t_mod - 248.5) < 0.5) {
    br1 = c * lag_birthrate1;
  } else {
    br1 = (1.0 - c) * lag_birthrate1 / 103.0;
  }

  double I_ratio1 = I1 / pop1;

  foi1 = pow((I1 + iota) / pop1, alpha);
 
  xi1 = rgamma(sigma_xi, 1 / sigma_xi);;
  betafinal1 = beta1 * I_ratio1 * xi1;

  int SD1 = rbinom(S1, delta);
  int ED1 = rbinom(E1, delta);
  int ID1 = rbinom(I1, delta);
  int RD1 = rbinom(R1, delta);

  S1 -= SD1;  E1 -= ED1;  I1 -= ID1;  R1 -= RD1;
  
  prob_S1[0] = exp(-dt * betafinal1);
  prob_S1[1] = 1 - exp(-dt * betafinal1);

  prob_E1[0] = exp(-dt * rho);
  prob_E1[1] = 1 - exp(-dt * rho);

  prob_I1[0] = exp(-dt * gamma);
  prob_I1[1] = 1 - exp(-dt * gamma);

  rmultinom(S1, prob_S1, 2, trans_S1);
  rmultinom(E1, prob_E1, 2, trans_E1);
  rmultinom(I1, prob_I1, 2, trans_I1);

  S1 = trans_S1[0] + rpois(br1);
  E1 = trans_E1[0] + trans_S1[1];
  I1 = trans_I1[0] + trans_E1[1];
  R1 += trans_I1[1];
  C1 += trans_I1[1];
");




## ----dmeasure-------------------------------------------------
dmeas <- Csnippet("
  double m = gaussianrho*C1;
  double v = m*(1.0-gaussianrho+psi*psi*m);
  double tol = 0.0;
  if (cases1 > 0.0) {
    lik = pnorm(cases1+0.5,m,sqrt(v)+tol,1,0)
           - pnorm(cases1-0.5,m,sqrt(v)+tol,1,0) + tol;
  } else {
    lik = pnorm(cases1+0.5,m,sqrt(v)+tol,1,0) + tol;
  }
  if (give_log) lik = log(lik);
")

## ----rmeasure-------------------------------------------------
rmeas <- Csnippet("
  double m = gaussianrho*C1;
  double v = m*(1.0-gaussianrho+psi*psi*m);
  double tol = 0.0;
  cases1 = rnorm(m,sqrt(v)+tol);
  if (cases1 > 0.0) {
    cases1 = nearbyint(cases1);
  } else {
    cases1 = 0.0;
  }
")

rinit <- Csnippet("
  double probs1[4];
  probs1[0] = S_0;
  probs1[1] = E_0;
  probs1[2] = I_0;
  probs1[3] = 1.0 - probs1[0] - probs1[1] - probs1[2];

  int counts1[4];
  rmultinom(pop1, probs1, 4, counts1);

  S1 = counts1[0];
  E1 = counts1[1];
  I1 = counts1[2];
  R1 = counts1[3];
  C1 = 0;
");

basic_log_names   <- c("rho", "gamma", "sigma_xi", "betabar", "g", "iota", "delta")
basic_logit_names <- c("a", "alpha", "c", "gaussianrho", "S_0", "E_0", "I_0", "psi")
log_names   <- basic_log_names
logit_names <- basic_logit_names
measles_partrans <- parameter_trans(
  log   = log_names,
  logit = logit_names
)

one_city_pomp <- pomp(
  data       = measles_cases,
  times      = "time",
  t0         = 0,
  rprocess   = euler(rproc, delta.t = 3.5), 
  rinit      = rinit,
  dmeasure   = dmeas,
  rmeasure   = rmeas,
  statenames = c("S1","E1","I1","R1","C1"),
  paramnames = c("alpha","iota","betabar","c","a","rho","gamma",
                 "delta","sigma_xi","g","gaussianrho","psi",
                 "S_0","E_0","I_0"),
  covar      = covariate_table(measles_covar,times = "time"),
  covarnames = c("lag_birthrate1","pop1"),
  accumvars  = c("C1")
)

coef(one_city_pomp) <- basic_params


negloglik <- function(x) optim(par=c(0.5,0.5,1),function(theta)-sum(dnbinom(x,mu=theta[1]+theta[2]*c(0,head(x,-1)),size=theta[3],log=T)))$value


Pomp_dir <- paste0(pomp_dir,"Pomp_E",7,"/")
if(!dir.exists(Pomp_dir)) dir.create(Pomp_dir)

bake(file=paste0(Pomp_dir,"E7_search.rds"),{
  foreach(i=1:10,.combine=c,
          .options.future=list(seed=482947940)
  ) %dofuture% {
    one_city_pomp |>
      mif2(
        Np=20000, Nmif=50,
        cooling.fraction.50=0.5,
        rw.sd=rw_sd(c        = 0.02,
                    betabar  = 0.02,
                    a        = 0.02,
                    rho      = 0.02,
                    gamma    = 0.02,
                    sigma_xi = 0.02,
                    psi     = 0.02,
                    gaussianrho = 0.02,
                    S_0      = ivp(0.02),
                    E_0      = ivp(0.02),
                    I_0      = ivp(0.02)),
        partrans=parameter_trans(log=c("rho", "gamma", "sigma_xi", "betabar", "g", "iota", "delta"),logit = c("a", "alpha", "c", "gaussianrho", "S_0", "E_0", "I_0", "psi")),
        paramnames=c("alpha","iota","betabar","c","a","rho","gamma",
                     "delta","sigma_xi","g","gaussianrho","psi",
                     "S_0","E_0","I_0")
      )
  } -> mifs_local
  attr(mifs_local,"ncpu") <- nbrOfWorkers()
  mifs_local
}) -> mifs_local
t_loc <- attr(mifs_local,"system.time")
ncpu_loc <- attr(mifs_local,"ncpu")


mifs_local |>
  traces() |>
  melt() |>
  ggplot(aes(x = iteration, y = value, group = .L1, color = factor(.L1))) +
  geom_line() +
  guides(color = "none") +
  facet_wrap(~ name, scales = "free_y")


bake(file = paste0(Pomp_dir, "E7_local_search.rds"), {
  foreach(mf = mifs_local, .combine = rbind,
          .options.future = list(seed = 482947940)
  ) %dofuture% {
    evals <- replicate(20, logLik(pfilter(mf, Np = 5000)))
    ll <- logmeanexp(evals, se = TRUE)
    mf %>% coef() %>% bind_rows() %>% 
      bind_cols(loglik = ll[1], loglik.se = ll[2])
  } -> local_search
  attr(local_search, "ncpu") <- nbrOfWorkers()
  local_search
}) -> local_search


bind_rows(local_search) %>%
  filter(is.finite(loglik)) %>%
  filter(loglik.se < 2) %>%
  arrange(-loglik) -> best_searches    

head(best_searches)

E7_result <- best_searches[1,16:17]

real_one_benchmark <- arma_benchmark(one_city_pomp)
real_one_benchmark$total

E1_real <- one_city_pomp@data

E1_real <- t(E1_real)

real_one_negbinom <- -sum(apply(E1_real,2,negloglik))
real_one_negbinom
```


```{r E8,include=FALSE}
## ----packages,incluxde=F,echo=F,cache=F----------------------------------------
## The spatPomp code is primarily adapted from Ionides et al. (2024) and the Whitehouse et al.\ (2023) code implementation.
library("spatPomp")
library("ggplot2")
library("tidyverse")
library("knitr")
library("doRNG")
library("doParallel")
cores <-  as.numeric(Sys.getenv('SLURM_NTASKS_PER_NODE', unset=NA))
if(is.na(cores)) cores <- detectCores()
registerDoParallel(cores)
ggplot2::theme_set(ggplot2::theme_bw())

stopifnot(packageVersion("pomp")>="5.0")

pomp_dir="pomp/"

set.seed(42)

basic_params <- c(
  alpha     = 1,
  iota      = 0.1,
  betabar   = 6.32,
  c         = 0.219,
  a         = 0.1467,
  rho       = 0.142,
  gamma     = 0.0473,
  delta     = 0.02/(26*4),
  sigma_xi  = 0.318,
  gaussianrho     = 0.7,
  psi      = 0.306,
  g         = 0,
  S_0       = 0.02545,
  E_0       = 0.00422,
  I_0       = 0.000061
)
expandedParNames <- NULL

dt <- 3.5
U  <- 40

measles_cases  <- read.csv(paste0(pomp_dir,"case1.csv"))

measles_covar  <- read.csv(paste0(pomp_dir,"covar2.csv"))

measles_covarnames <- paste0(rep(c("pop", "lag_birthrate"), each = U), 1:U)
measles_unit_covarnames <- c("pop", "lag_birthrate")

data_measles_distance <- read.csv(paste0(pomp_dir,'data_measles_distance.csv'))




data_measles_distance <- data_measles_distance

v_by_g <- as.matrix(data_measles_distance)

to_C_array <- function(v) paste0("{", paste0(v, collapse = ","), "}")
v_by_g_C_rows  <- apply(v_by_g, 1, to_C_array)
v_by_g_C_array <- to_C_array(v_by_g_C_rows)
v_by_g_C <- Csnippet(paste0("const double v_by_g[", U, "][", U, "] = ", v_by_g_C_array, "; "))

parNames      <- names(basic_params)
fixedParNames <- setdiff(parNames, expandedParNames)

set_expanded <- Csnippet(
  paste0("const int ", expandedParNames, "_unit = 1;\n", collapse = " ")
)
set_fixed <- Csnippet(
  paste0("const int ", fixedParNames, "_unit = 0;\n", collapse = " ")
)
measles_globals <- Csnippet(
  paste(v_by_g_C, set_expanded, set_fixed, sep = "\n")
)

measles_paramnames <- c(
  if (length(fixedParNames) > 0) {
    paste0(fixedParNames, "1")
  },
  if (length(expandedParNames) > 0) {
    paste0(rep(expandedParNames, each = U), 1:U)
  }
)

unit_statenames <- c("S", "E", "I", "R", "C")


measles_rprocess <- spatPomp_Csnippet(
  unit_statenames  = c("S", "E", "I", "R", "C"),
  unit_covarnames  = c("pop", "lag_birthrate"),
  unit_paramnames  = c("alpha", "iota", "betabar", "c", "a",
                       "rho", "gamma", "delta", "sigma_xi", "g"),
  code ="
    // Variables
    double br, beta, seas, foi, births, xi, betafinal;
    int trans_S[2], trans_E[2], trans_I[2];
    double prob_S[2], prob_E[2], prob_I[2];
    int SD[U], ED[U], ID[U], RD[U];
    double powVec[U];
    int u, v;

    // Calculate the day of the year without any offset
    // Pre-computing this saves substantial time
    // powVec[u] = pow(I[u]/pop[u], alpha);
    for (u = 0; u < U; u++) {
        powVec[u] = I[u] / pop[u];
        // IS THIS INTENDED TO BE FIXED TO ALPHA=1?
    }

    for (u = 0; u < U; u++) {
        double t_mod = fmod(t, 364.0);

        // Transmission rate
        if ((t_mod >= 6 && t_mod < 99) || (t_mod >= 115 && t_mod < 198) ||
            (t_mod >= 252 && t_mod < 299) || (t_mod >= 308 && t_mod < 355))
            seas = 1.0 + a[u * a_unit] * 2 * (1 - 0.759);
        else
            seas = 1.0 - 2 * a[u * a_unit] * 0.759;

        beta = betabar[u * betabar_unit] * seas;

        // Birth rate calculation
        if (fabs(t_mod - 248.5) < 0.5) {
            br = c[u * c_unit] * lag_birthrate[u];
        } else {
            br = (1.0 - c[u * c_unit]) * lag_birthrate[u] / 103;
        }

        // Expected force of infection
        if (alpha[u * alpha_unit] == 1.0 && iota[u * iota_unit] == 0.0) {
            foi = I[u] / pop[u];
        } else {
            foi = pow((I[u] + iota[u * iota_unit]) / pop[u], alpha[u * alpha_unit]);
        }

        for (v = 0; v < U; v++) {
            if (v != u) {
                foi += g[u * g_unit] * v_by_g[u][v] * (powVec[v] - powVec[u]) / pop[u];
            }
        }

        xi = rgamma(sigma_xi[u * sigma_xi_unit], 1 / sigma_xi[u * sigma_xi_unit]);
        betafinal = beta * foi * xi;  // Stochastic force of infection

        // Poisson births
        births = rpois(br);

        SD[u] = rbinom(S[u], delta[u * delta_unit]);
        ED[u] = rbinom(E[u], delta[u * delta_unit]);
        ID[u] = rbinom(I[u], delta[u * delta_unit]);
        RD[u] = rbinom(R[u], delta[u * delta_unit]);

        S[u] = S[u] - SD[u];
        E[u] = E[u] - ED[u];
        I[u] = I[u] - ID[u];
        R[u] = R[u] - RD[u];

        // Probabilities for state transitions
        prob_S[0] = exp(-dt * betafinal);
        prob_S[1] = 1 - exp(-dt * betafinal);

        prob_E[0] = exp(-dt * rho[u * rho_unit]);
        prob_E[1] = 1 - exp(-dt * rho[u * rho_unit]);

        prob_I[0] = exp(-dt * gamma[u * gamma_unit]);
        prob_I[1] = 1 - exp(-dt * gamma[u * gamma_unit]);

        // Multinomial transitions
        rmultinom(S[u], &prob_S[0], 2, &trans_S[0]); // B, (S-F)-B
        rmultinom(E[u], &prob_E[0], 2, &trans_E[0]); // C, (E-F)-C
        rmultinom(I[u], &prob_I[0], 2, &trans_I[0]); // E, (I-F)-D

        // Update compartments
        S[u] = trans_S[0] + births;
        E[u] = trans_E[0] + trans_S[1];
        I[u] = trans_I[0] + trans_E[1];
        R[u] = R[u] + trans_I[1];
        C[u] += trans_I[1];  // True incidence
    }
"
)


measles_dmeasure <-  spatPomp_Csnippet(
  unit_statenames = 'C',
  unit_obsnames = 'cases',
  unit_paramnames = c('gaussianrho','psi'),
  code="
      double m,v;
      double tol = 1e-300;
      double mytol = 1e-5;
      int u;
      lik = 0;
      for (u = 0; u < U; u++) {
        m = gaussianrho[u*gaussianrho_unit]*(C[u]+mytol);
        v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);

        // Deal with NA measurements by omitting them
        if(!(ISNA(cases[u]))){
          // C < 0 can happen in bootstrap methods such as bootgirf
          if (C[u] < 0) {lik += log(tol);} else {
            if (cases[u] > tol) {
              lik += log(pnorm(cases[u]+0.5,m,sqrt(v)+tol,1,0)-
                pnorm(cases[u]-0.5,m,sqrt(v)+tol,1,0)+tol);
            } else {
                lik += log(pnorm(cases[u]+0.5,m,sqrt(v)+tol,1,0)+tol);
            }
          }
        }
      }
      if(!give_log) lik = (lik > log(tol)) ? exp(lik) : tol;
    "
)

measles_rmeasure <- spatPomp_Csnippet(
  method='rmeasure',
  unit_paramnames=c('gaussianrho','psi'),
  unit_statenames='C',
  unit_obsnames='cases',
  code="
      double m,v;
      double tol = 1.0e-300;
      int u;
      for (u = 0; u < U; u++) {
        m = gaussianrho[u*gaussianrho_unit]*(C[u]+tol);
        v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);
        cases[u] = rnorm(m,sqrt(v)+tol);
        if (cases[u] > 0.0) {
          cases[u] = nearbyint(cases[u]);
        } else {
          cases[u] = 0.0;
        }
      }
    "
)

measles_dunit_measure <- spatPomp_Csnippet(
  unit_paramnames=c('gaussianrho','psi'),
  code="
      double mytol = 1e-5;
      double m = gaussianrho[u*gaussianrho_unit]*(C+mytol);
      double v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);
      double tol = 1e-300;
      // C < 0 can happen in bootstrap methods such as bootgirf
      if(ISNA(cases)) {lik=1;} else { 
        if (C < 0) {lik = 0;} else {
          if (cases > tol) {
            lik = pnorm(cases+0.5,m,sqrt(v)+tol,1,0)-
              pnorm(cases-0.5,m,sqrt(v)+tol,1,0)+tol;
          } else {
            lik = pnorm(cases+0.5,m,sqrt(v)+tol,1,0)+tol;
          }
        }
      }
      if(give_log) lik = log(lik);
    "
)

measles_rinit <- spatPomp_Csnippet(
  unit_paramnames = c("S_0", "E_0", "I_0"),
  unit_statenames = c("S", "E", "I", "R", "C"),
  unit_covarnames = "pop",
  code = "
    int u;
    for (u = 0; u < U; u++) {
        double probs[4];
        probs[0] = S_0[u * S_0_unit];
        probs[1] = E_0[u * E_0_unit];
        probs[2] = I_0[u * I_0_unit];
        probs[3] = 1.0 - probs[0] - probs[1] - probs[2];
        int counts[4];
        rmultinom(pop[u], &probs[0], 4, &counts[0]);
        S[u] = counts[0];
        E[u] = counts[1];
        I[u] = counts[2];
        R[u] = counts[3];
        C[u] = 0;
    }
")




### === Parameter Transformation Settings ===

basic_log_names   <- c("rho", "gamma", "sigma_xi", "betabar", "g", "iota", "delta")
basic_log_names   <- setdiff(basic_log_names, fixedParNames)

basic_logit_names <- c("a", "alpha", "c", "gaussianrho", "S_0", "E_0", "I_0",'psi')
basic_logit_names <- setdiff(basic_logit_names, fixedParNames)
log_names   <- unlist(lapply(basic_log_names, function(x, U) paste0(x, 1:U), U))
logit_names <- unlist(lapply(basic_logit_names, function(x, U) paste0(x, 1:U), U))
measles_partrans <- parameter_trans(log = log_names, logit = logit_names)

m9 <- spatPomp(
  measles_cases,
  units           = "city",
  times           = "days",
  t0              = min(measles_cases$days) - 14,
  unit_statenames = unit_statenames,
  covar           = measles_covar,
  rprocess        = euler(measles_rprocess, delta.t = 3.5),
  unit_accumvars  = c("C"),
  paramnames      = measles_paramnames,
  globals         = measles_globals,
  rinit           = measles_rinit,
  dmeasure        = measles_dmeasure,
  rmeasure        = measles_rmeasure,
  dunit_measure   = measles_dunit_measure,
  partrans = measles_partrans
)

measles_params <- rep(0, length = length(measles_paramnames))

names(measles_params) <- measles_paramnames

for (p in fixedParNames)
  measles_params[paste0(p, 1)] <- basic_params[p]
for (p in expandedParNames)
  measles_params[paste0(p, 1:U)] <- basic_params[p]
coef(m9) <- measles_params

sim <- simulate(m9, params =  measles_params,  nsim   = 1,
                seed   = 154234)
##


spatPomp_dir <- paste0(pomp_dir,"E_",8,"/")
if(!dir.exists(spatPomp_dir)) dir.create(spatPomp_dir)

stew(file=paste0(spatPomp_dir,"E8.rda"),seed=124,{
  cat(capture.output(sessionInfo()),
      file=paste0(spatPomp_dir,"sessionInfo.txt"),sep="\n")
  
  bpf_logLik_40 <- foreach(i = 1:20, .combine = c) %dopar% {
    logLik(bpfilter(sim, Np = 100000, block_size = 1))
  }
})


E8_result <- logmeanexp(bpf_logLik_40,se = T,ess = T)

tmp_benchmark_spat <- arma_benchmark(sim)

tmp_benchmark_spat$total

E8_sim <- sim@data

E8_sim <- t(E8_sim)

negloglik <- function(x) optim(par=c(0.5,0.5,1),function(theta)-sum(dnbinom(x,mu=theta[1]+theta[2]*c(0,head(x,-1)),size=theta[3],log=T)))$value

tmp_negbinom_spat <- -sum(apply(E8_sim,2,negloglik))

realdata_benchmark_spat <- arma_benchmark(m9)

realdata_benchmark_spat$total

E8_real <- m9@data

E8_real <- t(E8_real)

realdata_negbinom_spat <- -sum(apply(E8_real,2,negloglik))

realdata_negbinom_spat
## Prepare the simulated data for python.
simdata <- as.data.frame(sim)

simdata <- simdata[order(simdata$city),]

yt <- simdata$cases

M40 <- matrix(yt, nrow = 40, byrow = TRUE)

M40 <- as.data.frame(M40)

colnames(M40) <- as.character(0:415)

write.csv(M40,file = "M40.csv",row.names = F)
```

```{r E11,include=FALSE}
## ----packages,incluxde=F,echo=F,cache=F----------------------------------------
## The spatPomp code is primarily adapted from Ionides et al. (2024) and the Whitehouse et al.\ (2023) code implementation.
library("spatPomp")
library("ggplot2")
library("tidyverse")
library("knitr")
library("doRNG")
library("doParallel")
cores <-  as.numeric(Sys.getenv('SLURM_NTASKS_PER_NODE', unset=NA))
if(is.na(cores)) cores <- detectCores()
registerDoParallel(cores)
ggplot2::theme_set(ggplot2::theme_bw())

stopifnot(packageVersion("pomp")>="5.0")

pomp_dir="pomp/"

set.seed(42)

basic_params <- c(
  alpha     = 1,
  iota      = 0,
  betabar   = 6.32,
  c         = 0.219,
  a         = 0.1467,
  rho       = 0.142,
  gamma     = 0.0473,
  delta     = 0.02/(26*4),
  sigma_xi  = 0.318,
  gaussianrho     = 0.7,
  psi      = 0.306,
  g         = 716,
  S_0       = 0.02545,
  E_0       = 0.00422,
  I_0       = 0.000061
)

U <- 40                                 

shared_unit_values <- list(
  alpha        = 1,
  iota         = 0,
  c            = 0.032121729,
  a            = 0.37107605,
  rho          = 0.11784376,
  gamma        = 0.10496352,
  delta        = 0.0001923076923076923,
  sigma_xi     = 0.1776823,
  psi          = 0.20831605,
  g            = 621.00315
)


betabar_vals <- c(
  0.88894647, 0.59131747, 1.3571352,  0.99088466, 1.1142877,
  1.073081,   1.5383922,  0.90597922, 0.82007521, 0.85654908,
  1.0977113,  0.86888778, 1.1310883,  0.9737336,  0.94829279,
  0.62169701, 0.94519132, 0.59131724, 0.59131730, 0.76468575,
  1.188135,   0.96471751, 1.3417162,  1.2114047,  1.2141544,
  1.4035884,  0.98535007, 0.78903711, 1.3568828,  0.92531413,
  0.85022265, 1.1698065,  1.747846,   1.0385478,  0.94807833,
  0.90443295, 1.564422,   1.3209716,  1.4434845,  0.89985961
)

gaussianrho_vals <- c(
  0.512295, 0.5846346, 0.73733903, 0.64644552, 0.68464784,
  0.62805587, 0.59746622, 0.62227199, 0.57668756, 0.68815189,
  0.75385802, 0.63758144, 0.65418166, 0.59742807, 0.58928316,
  0.7143576, 0.64989739, 0.52764322, 0.51610781, 0.61995668,
  0.63451302, 0.64841047, 0.66641999, 0.58411126, 0.74216196,
  0.65110744, 0.57886699, 0.6156806, 0.51067983, 0.47119888,
  0.70101722, 0.59482185, 0.76027942, 0.66776662, 0.52625039,
  0.60342687, 0.68950329, 0.5869341,  0.70792855, 0.57809921
)

S0_vals <- c(
  0.075939171, 0.045746908, 0.072099209, 0.093366429, 0.12166313,
  0.082433015, 0.076253399, 0.073552363, 0.091899686, 0.11601865,
  0.11060591,  0.086007304, 0.090033337, 0.068562545, 0.08527638,
  0.099353261, 0.071033828, 0.057539228, 0.0457469,   0.056584992,
  0.066623405, 0.071289554, 0.10958349,  0.057944749, 0.098137014,
  0.083878987, 0.073387422, 0.065006606, 0.076856337, 0.089820623,
  0.078178115, 0.11018048,  0.1181622,   0.098707311, 0.09720967,
  0.076138817, 0.066983677, 0.081948154, 0.083776928, 0.063756049
)

E0_vals <- c(
  6.5060573e-05, 4.5087443e-05, 5.2817129e-05, 6.8182577e-05, 6.4530977e-05,
  4.4786735e-05, 7.5147931e-05, 9.4057032e-05, 4.7683639e-05, 4.2368429e-05,
  6.2710962e-05, 7.0720002e-05, 6.5743807e-05, 9.3865732e-05, 9.4957613e-05,
  5.9195183e-05, 4.9652379e-05, 4.4339824e-05, 5.5780311e-05, 3.4462537e-05,
  6.6237786e-05, 4.6263423e-05, 4.9870680e-05, 4.3129025e-05, 7.4057010e-05,
  6.1690727e-05, 7.0626847e-05, 7.1526090e-05, 5.3734158e-05, 6.6238754e-05,
  5.7956771e-05, 6.3070715e-05, 8.4001782e-05, 8.3680527e-05, 3.8972536e-05,
  5.9952090e-05, 5.8326987e-05, 5.7037920e-05, 8.9273613e-05, 6.1138249e-05
)

I0_vals <- c(
  1.25675637e-04, 1.28807835e-04, 0.0025939241, 0.0011983996, 5.3031814e-05,
  0.00043914493, 0.00035218208, 0.0020560939, 0.00058301724, 0.00017986108,
  7.6727294e-05, 6.1292536e-05, 0.00021765193, 0.00083288341, 1.3892895e-08,
  0.0016802071, 0.0013289195, 0.00024760532, 0.00020639459, 0.0014469447,
  0.00066906348, 0.00023718234, 1.6463227e-08, 0.00067811750, 0.00091942732,
  0.00076855475, 0.0029490807, 0.0024151804, 0.0013736223, 0.0010850356,
  0.0030775415, 0.0039083785, 0.00043130366, 0.0025644207, 0.0023132868,
  0.00070547545, 0.00088096713, 0.00051698624, 0.00079949765, 0.00021292546
)


expand_units <- function(value, name) {
  vals <- if (length(value) == 1) rep(value, U) else value
  setNames(vals, paste0(name, seq_len(U)))
}

basic_paramsC <- c(
  unlist(lapply(names(shared_unit_values), function(nm)
    expand_units(shared_unit_values[[nm]], nm)
  )),
  
  expand_units(betabar_vals,     "betabar"),
  expand_units(gaussianrho_vals, "gaussianrho"),
  
  expand_units(S0_vals, "S_0"),
  expand_units(E0_vals, "E_0"),
  expand_units(I0_vals, "I_0")
)

length(basic_paramsC)          
head(basic_paramsC, 20)       

expandedParNames <- names(basic_params)

dt <- 3.5
U  <- 40

measles_cases  <- read.csv(paste0(pomp_dir,"case1.csv"))

measles_covar  <- read.csv(paste0(pomp_dir,"covar2.csv"))

measles_covarnames <- paste0(rep(c("pop", "lag_birthrate"), each = U), 1:U)
measles_unit_covarnames <- c("pop", "lag_birthrate")

data_measles_distance <- read.csv(paste0(pomp_dir,'data_measles_distance.csv'))




data_measles_distance <- data_measles_distance

v_by_g <- as.matrix(data_measles_distance)

to_C_array <- function(v) paste0("{", paste0(v, collapse = ","), "}")
v_by_g_C_rows  <- apply(v_by_g, 1, to_C_array)
v_by_g_C_array <- to_C_array(v_by_g_C_rows)
v_by_g_C <- Csnippet(paste0("const double v_by_g[", U, "][", U, "] = ", v_by_g_C_array, "; "))

parNames      <- names(basic_params)
fixedParNames <- setdiff(parNames, expandedParNames)

set_expanded <- Csnippet(
  paste0("const int ", expandedParNames, "_unit = 1;\n", collapse = " ")
)
set_fixed <- Csnippet(
  paste0("const int ", fixedParNames, "_unit = 0;\n", collapse = " ")
)
measles_globals <- Csnippet(
  paste(v_by_g_C, set_expanded, set_fixed, sep = "\n")
)

measles_paramnames <- c(
  if (length(fixedParNames) > 0) {
    paste0(fixedParNames, "1")
  },
  if (length(expandedParNames) > 0) {
    paste0(rep(expandedParNames, each = U), 1:U)
  }
)

unit_statenames <- c("S", "E", "I", "R", "C")


measles_rprocess <- spatPomp_Csnippet(
  unit_statenames  = c("S", "E", "I", "R", "C"),
  unit_covarnames  = c("pop", "lag_birthrate"),
  unit_paramnames  = c("alpha", "iota", "betabar", "c", "a",
                       "rho", "gamma", "delta", "sigma_xi", "g"),
  code ="
    // Variables
    double br, beta, seas, foi, births, xi, betafinal;
    int trans_S[2], trans_E[2], trans_I[2];
    double prob_S[2], prob_E[2], prob_I[2];
    int SD[U], ED[U], ID[U], RD[U];
    double powVec[U];
    int u, v;

    // Calculate the day of the year without any offset
    // Pre-computing this saves substantial time
    // powVec[u] = pow(I[u]/pop[u], alpha);
    for (u = 0; u < U; u++) {
        powVec[u] = I[u] / pop[u];
        // IS THIS INTENDED TO BE FIXED TO ALPHA=1?
    }

    for (u = 0; u < U; u++) {
        double t_mod = fmod(t, 364.0);

        // Transmission rate
        if ((t_mod >= 6 && t_mod < 99) || (t_mod >= 115 && t_mod < 198) ||
            (t_mod >= 252 && t_mod < 299) || (t_mod >= 308 && t_mod < 355))
            seas = 1.0 + a[u * a_unit] * 2 * (1 - 0.759);
        else
            seas = 1.0 - 2 * a[u * a_unit] * 0.759;

        beta = betabar[u * betabar_unit] * seas;

        // Birth rate calculation
        if (fabs(t_mod - 248.5) < 0.5) {
            br = c[u * c_unit] * lag_birthrate[u];
        } else {
            br = (1.0 - c[u * c_unit]) * lag_birthrate[u] / 103;
        }

        // Expected force of infection
        if (alpha[u * alpha_unit] == 1.0 && iota[u * iota_unit] == 0.0) {
            foi = I[u] / pop[u];
        } else {
            foi = pow((I[u] + iota[u * iota_unit]) / pop[u], alpha[u * alpha_unit]);
        }

        for (v = 0; v < U; v++) {
            if (v != u) {
                foi += g[u * g_unit] * v_by_g[u][v] * (powVec[v] - powVec[u]) / pop[u];
            }
        }

        xi = rgamma(sigma_xi[u * sigma_xi_unit], 1 / sigma_xi[u * sigma_xi_unit]);
        betafinal = beta * foi * xi;  // Stochastic force of infection

        // Poisson births
        births = rpois(br);

        SD[u] = rbinom(S[u], delta[u * delta_unit]);
        ED[u] = rbinom(E[u], delta[u * delta_unit]);
        ID[u] = rbinom(I[u], delta[u * delta_unit]);
        RD[u] = rbinom(R[u], delta[u * delta_unit]);

        S[u] = S[u] - SD[u];
        E[u] = E[u] - ED[u];
        I[u] = I[u] - ID[u];
        R[u] = R[u] - RD[u];

        // Probabilities for state transitions
        prob_S[0] = exp(-dt * betafinal);
        prob_S[1] = 1 - exp(-dt * betafinal);

        prob_E[0] = exp(-dt * rho[u * rho_unit]);
        prob_E[1] = 1 - exp(-dt * rho[u * rho_unit]);

        prob_I[0] = exp(-dt * gamma[u * gamma_unit]);
        prob_I[1] = 1 - exp(-dt * gamma[u * gamma_unit]);

        // Multinomial transitions
        rmultinom(S[u], &prob_S[0], 2, &trans_S[0]); // B, (S-F)-B
        rmultinom(E[u], &prob_E[0], 2, &trans_E[0]); // C, (E-F)-C
        rmultinom(I[u], &prob_I[0], 2, &trans_I[0]); // E, (I-F)-D

        // Update compartments
        S[u] = trans_S[0] + births;
        E[u] = trans_E[0] + trans_S[1];
        I[u] = trans_I[0] + trans_E[1];
        R[u] = R[u] + trans_I[1];
        C[u] += trans_I[1];  // True incidence
    }
"
)


measles_dmeasure <-  spatPomp_Csnippet(
  unit_statenames = 'C',
  unit_obsnames = 'cases',
  unit_paramnames = c('gaussianrho','psi'),
  code="
      double m,v;
      double tol = 1e-300;
      double mytol = 1e-5;
      int u;
      lik = 0;
      for (u = 0; u < U; u++) {
        m = gaussianrho[u*gaussianrho_unit]*(C[u]+mytol);
        v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);

        // Deal with NA measurements by omitting them
        if(!(ISNA(cases[u]))){
          // C < 0 can happen in bootstrap methods such as bootgirf
          if (C[u] < 0) {lik += log(tol);} else {
            if (cases[u] > tol) {
              lik += log(pnorm(cases[u]+0.5,m,sqrt(v)+tol,1,0)-
                pnorm(cases[u]-0.5,m,sqrt(v)+tol,1,0)+tol);
            } else {
                lik += log(pnorm(cases[u]+0.5,m,sqrt(v)+tol,1,0)+tol);
            }
          }
        }
      }
      if(!give_log) lik = (lik > log(tol)) ? exp(lik) : tol;
    "
)

measles_rmeasure <- spatPomp_Csnippet(
  method='rmeasure',
  unit_paramnames=c('gaussianrho','psi'),
  unit_statenames='C',
  unit_obsnames='cases',
  code="
      double m,v;
      double tol = 1.0e-300;
      int u;
      for (u = 0; u < U; u++) {
        m = gaussianrho[u*gaussianrho_unit]*(C[u]+tol);
        v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);
        cases[u] = rnorm(m,sqrt(v)+tol);
        if (cases[u] > 0.0) {
          cases[u] = nearbyint(cases[u]);
        } else {
          cases[u] = 0.0;
        }
      }
    "
)

measles_dunit_measure <- spatPomp_Csnippet(
  unit_paramnames=c('gaussianrho','psi'),
  code="
      double mytol = 1e-5;
      double m = gaussianrho[u*gaussianrho_unit]*(C+mytol);
      double v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);
      double tol = 1e-300;
      // C < 0 can happen in bootstrap methods such as bootgirf
      if(ISNA(cases)) {lik=1;} else { 
        if (C < 0) {lik = 0;} else {
          if (cases > tol) {
            lik = pnorm(cases+0.5,m,sqrt(v)+tol,1,0)-
              pnorm(cases-0.5,m,sqrt(v)+tol,1,0)+tol;
          } else {
            lik = pnorm(cases+0.5,m,sqrt(v)+tol,1,0)+tol;
          }
        }
      }
      if(give_log) lik = log(lik);
    "
)

measles_rinit <- spatPomp_Csnippet(
  unit_paramnames = c("S_0", "E_0", "I_0"),
  unit_statenames = c("S", "E", "I", "R", "C"),
  unit_covarnames = "pop",
  code = "
    int u;
    for (u = 0; u < U; u++) {
        double probs[4];
        probs[0] = S_0[u * S_0_unit];
        probs[1] = E_0[u * E_0_unit];
        probs[2] = I_0[u * I_0_unit];
        probs[3] = 1.0 - probs[0] - probs[1] - probs[2];
        int counts[4];
        rmultinom(pop[u], &probs[0], 4, &counts[0]);
        S[u] = counts[0];
        E[u] = counts[1];
        I[u] = counts[2];
        R[u] = counts[3];
        C[u] = 0;
    }
")




### === Parameter Transformation Settings ===

basic_log_names   <- c("rho", "gamma", "sigma_xi", "betabar", "g", "iota", "delta")
basic_log_names   <- setdiff(basic_log_names, fixedParNames)

basic_logit_names <- c("a", "alpha", "c", "gaussianrho", "S_0", "E_0", "I_0",'psi')
basic_logit_names <- setdiff(basic_logit_names, fixedParNames)
log_names   <- unlist(lapply(basic_log_names, function(x, U) paste0(x, 1:U), U))
logit_names <- unlist(lapply(basic_logit_names, function(x, U) paste0(x, 1:U), U))
measles_partrans <- parameter_trans(log = log_names, logit = logit_names)

m9 <- spatPomp(
  measles_cases,
  units           = "city",
  times           = "days",
  t0              = min(measles_cases$days) - 14,
  unit_statenames = unit_statenames,
  covar           = measles_covar,
  rprocess        = euler(measles_rprocess, delta.t = 3.5),
  unit_accumvars  = c("C"),
  paramnames      = measles_paramnames,
  globals         = measles_globals,
  rinit           = measles_rinit,
  dmeasure        = measles_dmeasure,
  rmeasure        = measles_rmeasure,
  dunit_measure   = measles_dunit_measure,
  partrans = measles_partrans
)

measles_params <- rep(0, length = length(measles_paramnames))

names(measles_params) <- measles_paramnames

for (p in fixedParNames)
  measles_params[paste0(p, 1)] <- basic_params[p]
for (p in expandedParNames)
  measles_params[paste0(p, 1:U)] <- basic_params[p]
coef(m9) <- measles_params

coef(m9) <- basic_paramsC

sim <- simulate(m9)

## ----bm-model,echo=T,eval=T---------------------------------------------------

library(spatPomp)
i <- 2

measles_dir <- paste0(pomp_dir,"measles_more",i,"/")
if(!dir.exists(measles_dir)) dir.create(measles_dir)

## -------- new rw.sd specification ----------------------------------
par_rw <- setdiff(names(basic_params), c("alpha", "iota","detla","gaussianrho","psi")) 

U       <- 40          
ivp_sd  <- 0.001        
rp_sd   <- 0.001       

IVP_names   <- unlist(lapply(c("S_0", "E_0", "I_0"),
                             function(p) paste0(p, 1:U)))

OTHER_names <- unlist(lapply(setdiff(par_rw, c("S_0", "E_0", "I_0")),
                             function(p) paste0(p, 1:U)))


string_rwsd <- paste0(
  "rw_sd(",
  paste0(IVP_names,   "=ivp(", ivp_sd, ")", collapse = ", "),
  if (length(OTHER_names) > 0)
      paste0(", ", paste0(OTHER_names, "=", rp_sd, collapse = ", ")),
  ")"
)

measles_rw.sd <- eval(parse(text = string_rwsd))

## ----ibpf-mle-eval,eval=T,echo=F----------------------------------------------
stew(file=paste0(measles_dir,"ibpf_mle.rda"),seed=999,{
  tic <- Sys.time()
  params_start <- coef(m9)
  ibpf_mle_searches <- foreach(reps=1:switch(i,3,10))%dopar%{
    ibpf(m9,params=params_start,
      Nbpf=switch(i,2,50),Np=switch(i,10,10000),
      rw.sd=measles_rw.sd ,
      unitParNames= c("betabar","S_0","E_0","I_0") ,
      sharedParNames = c("a","c","rho","gamma","sigma_xi","g"),
      block_size=1,
      spat_regression=0.1,
      cooling.fraction.50=0.5
    )
  }
  toc <- Sys.time()
  })

  prof1time <- toc-tic


m1 <- ibpf_mle_searches[[9]]

spatPomp_dir <- paste0(pomp_dir,"E_",11,"/")
if(!dir.exists(spatPomp_dir)) dir.create(spatPomp_dir)

stew(file=paste0(spatPomp_dir,"E11.rda"),seed=124,{
  cat(capture.output(sessionInfo()),
      file=paste0(spatPomp_dir,"sessionInfo.txt"),sep="\n")
  
  bpf_logLik_40 <- foreach(i = 1:20, .combine = c) %dopar% {
    logLik(bpfilter(m1, Np = 100000, block_size = 1))
  }
})

E11_result <- logmeanexp(bpf_logLik_40,se = T,ess = T)
```

```{python E_python,include=FALSE}
# ======================================================================
#  PALmeasles experiment 
#  author : hky   20250703
# ======================================================================
from __future__ import annotations
from pathlib import Path                   #  New
import os, sys, time, random, numpy as np, pandas as pd, tensorflow as tf
import tensorflow_probability as tfp
from typing import Dict, Any    
# 

ROOT = Path("wwr").resolve()      
BASE = str(ROOT)
DATA = os.path.join(BASE, "Data")
PARAM = os.path.join(DATA, "Parameter")

# Shared data (to avoid repeated disk IO)
UKbirths_arr        = np.load(os.path.join(DATA, "UKbirths_array.npy"))
UKpop_arr           = np.load(os.path.join(DATA, "UKpop_array.npy"))
distance_arr        = np.load(os.path.join(DATA, "measles_distance_matrix_array.npy"))

sys.path.append(os.path.join(BASE, "Scripts"))
from measles_simulator import *
from measles_PALSMC import *

#  Utility functions 
def set_seeds(seed: int) -> None:
    random.seed(seed); np.random.seed(seed); tf.random.set_seed(seed)

def logmeanexp(x: np.ndarray, jackknife: bool = True) -> tuple[float, float]:
    n, x_max = x.size, x.max()
    lme = x_max + np.log(np.mean(np.exp(x - x_max)))
    if not jackknife:
        return lme, np.nan
    jk = np.array([
        np.delete(x, k).max() +
        np.log(np.mean(np.exp(np.delete(x, k) - np.delete(x, k).max())))
        for k in range(n)
    ])
    se = (n - 1) * jk.std(ddof=1) / np.sqrt(n)
    return lme, se

#  Core runner 
def run_experiment(cfg: Dict) -> None:
    """Single experiment: cache  data preparation  MonteCarlo  summary print"""
    cache_path = os.path.join(cfg["cache_dir"], cfg["cache_file"])
    os.makedirs(cfg["cache_dir"], exist_ok=True)

    # ---------- 1. Load / compute ----------
    if os.path.exists(cache_path):
        log_like = np.load(cache_path)[cfg["cache_key"]]
    else:
        # ---------- 2. Data loading ----------
        if cfg["n_cities"] == 1:     # Single city (London)
            births  = tf.convert_to_tensor(UKbirths_arr[18:19], tf.float32)
            pop     = tf.convert_to_tensor(UKpop_arr[18:19],  tf.float32)
            dist    = tf.convert_to_tensor(distance_arr[18:19, 18:19], tf.float32)

            measles = tf.convert_to_tensor(
                pd.read_csv(os.path.join(DATA, "londonsim.csv")).values,
                tf.float32
            )
        else:                        # 40 cities
            births = tf.convert_to_tensor(UKbirths_arr, tf.float32)
            pop    = tf.convert_to_tensor(UKpop_arr,  tf.float32)
            dist   = tf.convert_to_tensor(distance_arr, tf.float32)

            # Compatible with both CSV and NPY formats
            dfile = os.path.join(DATA, cfg["data_file"])
            measles = (
                tf.convert_to_tensor(pd.read_csv(dfile).values, tf.float32)
                if dfile.endswith(".csv") else
                tf.convert_to_tensor(np.load(dfile), tf.float32)
            )

        # Academic year calendar
        term   = tf.constant([6, 99, 115, 198, 252, 299, 308, 355, 366], tf.float32)
        school = tf.constant([0, 1, 0, 1, 0, 1, 0, 1, 0], tf.float32)
        T_int  = int(measles.shape[1])
        inter_steps = 4
        h = tf.constant(14 / inter_steps, tf.float32)
        is_term, is_year, *_ = school_term_and_school_year(T_int, inter_steps, term, school)
        is_term = tf.convert_to_tensor(is_term, tf.float32)
        is_year = tf.convert_to_tensor(is_year, tf.float32)

        # ---------- 3. Load best parameters (.npz or .npy) ----------
        pfile = os.path.join(cfg["param_dir"], cfg["param_file"])
        raw   = np.load(pfile, allow_pickle=False)
        best_par = (
            raw[cfg["param_key"]].astype(np.float32) if cfg["param_key"] else
            raw.astype(np.float32)
        )

        # ---------- 4. Assemble fixed tensors ----------
        n_cities = cfg["n_cities"]
        pi_0 = tf.constant(
            [[best_par[0], best_par[1], best_par[2],
              1.0 - best_par[0] - best_par[1] - best_par[2]]],
            tf.float32
        ) * tf.ones((n_cities, 4), tf.float32)

        beta_bar = tf.ones((n_cities, 1), tf.float32) * best_par[3]
        rho      = tf.ones((n_cities, 1), tf.float32) * best_par[4]
        gamma    = tf.ones((n_cities, 1), tf.float32) * best_par[5]
        g_scalar = cfg["g_factory"](best_par)
        g        = tf.ones((n_cities, 1), tf.float32) * g_scalar

        a       = tf.constant(best_par[cfg["idx_a"]], tf.float32)
        c       = tf.constant(best_par[cfg["idx_c"]], tf.float32)
        xi_var  = 10.0 * tf.constant(best_par[cfg["idx_xi"]], tf.float32)
        q_var   = tf.constant(best_par[cfg["idx_q"]], tf.float32)

        Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
        Q  = tfp.distributions.TruncatedNormal(
                 loc=cfg["q_loc_factory"](best_par),
                 scale=q_var,
                 low=0.0,
                 high=1.0,
             )

        p_const       = tf.constant(0.759, tf.float32)
        delta_year    = tf.constant([[1 / 50]], tf.float32) * tf.ones((n_cities, 4), tf.float32)

        # ---------- 5. PAL_vanilla / PAL_lookahead ----------
        log_like = np.empty(cfg["n_experiments"], dtype=np.float32)
        fn_call  = PAL_run_likelihood_res if cfg["mode"] == "res" else PAL_run_likelihood_lookahead

        for i in range(cfg["n_experiments"]):
            set_seeds(cfg["seed_offset"] + i)
            log_like[i] = fn_call(
                T_int, inter_steps, measles, births, pop, g, dist, pop[:, 0],
                pi_0, beta_bar, p_const, a, is_term, is_year, h,
                rho, gamma, Xi, Q, c, n_cities, cfg["n_particles"], delta_year
            )[0].numpy()

        # Cache
        np.savez(cache_path, **{cfg["cache_key"]: log_like})

    # ---------- 6. Summary ----------
    lme, se = logmeanexp(log_like)
    RESULTS[cfg["label"]] = (float(lme), float(se))
    print(f"[{cfg['label']}]  LME={lme:.4f}  SE={se:.4f}")
    return float(lme), float(se)

q_mean_path = os.path.join(DATA, "q_mean.npy")      
Q_MEAN: float = float(np.mean(np.load(q_mean_path)))    

#  Experiment configurations 
EXPERIMENTS = [
    # label  n_cities  mode   n_particles  seed_offset
    # g_factory                    q_loc_factory
    # cache dir / file / key
    # param dir / file / key
    # data_file
    # aidx cidx xiidx qidx
    dict(
        label="E2",  n_cities=1,  mode="res",       n_particles=5_000,   seed_offset=113,
        g_factory=lambda bp: 0.0,
        q_loc_factory=lambda bp: 0.7,
        cache_dir=os.path.join(BASE, "E2"),
        cache_file="PAL_vanilla_new.npz",
        cache_key="log_likelihood_shared",
        param_dir=os.path.join(BASE, "E2"),
        param_file="E2_param_exp.npz",
        param_key="E2_param_exp",
        data_file="londonsim.csv",
        n_experiments=20,
        idx_a=6, idx_c=7, idx_xi=8, idx_q=9,
    ),
    dict(
        label="E3",  n_cities=1,  mode="res",       n_particles=100_000, seed_offset=123,
        g_factory=lambda bp: 0.0,
        q_loc_factory=lambda bp: 0.7,
        cache_dir=os.path.join(BASE, "E3"),
        cache_file="PAL_vanilla.npz",
        cache_key="log_likelihood_shared",
        param_dir=os.path.join(BASE, "E2"),
        param_file="E2_param_exp.npz",
        param_key="E2_param_exp",
        data_file="londonsim.csv",
        n_experiments=20,
        idx_a=6, idx_c=7, idx_xi=8, idx_q=9,
    ),
    dict(
        label="E4",  n_cities=1,  mode="lookahead", n_particles=5_000,   seed_offset=123,
        g_factory=lambda bp: 0.0,
        q_loc_factory=lambda bp: 0.7,
        cache_dir=os.path.join(BASE, "E4"),
        cache_file="PAL_lookahead.npz",
        cache_key="log_likelihood_shared",
        param_dir=os.path.join(BASE, "E4"),
        param_file="E4_param_exp.npz",
        param_key="E4_param_exp",
        data_file="londonsim.csv",
        n_experiments=20,
        idx_a=6, idx_c=7, idx_xi=8, idx_q=9,
    ),
    dict(
        label="E5",  n_cities=1,  mode="lookahead", n_particles=100_000, seed_offset=123,
        g_factory=lambda bp: 0.0,
        q_loc_factory=lambda bp: 0.7,
        cache_dir=os.path.join(BASE, "E5"),
        cache_file="PAL_lookahead.npz",
        cache_key="log_likelihood_shared",
        param_dir=os.path.join(BASE, "E4"),
        param_file="E4_param_exp.npz",
        param_key="E4_param_exp",
        data_file="londonsim.csv",
        n_experiments=20,
        idx_a=6, idx_c=7, idx_xi=8, idx_q=9,
    ),
    dict(
        label="E6",  n_cities=1,  mode="lookahead", n_particles=5_000,   seed_offset=123,
        g_factory=lambda bp: 0.0,
        q_loc_factory=lambda bp: float(bp[10]),
        cache_dir=os.path.join(BASE, "E6"),
        cache_file="PAL_lookahead.npz",
        cache_key="log_likelihood_shared",
        param_dir=os.path.join(BASE, "E6"),
        param_file="E6_param_exp.npz",
        param_key="E6_param_exp",
        data_file="UKmeasles_array.npy",
        n_experiments=20,
        idx_a=6, idx_c=7, idx_xi=8, idx_q=9,
    ),
    #  40city series 
    dict(
        label="E9",  n_cities=40, mode="res",       n_particles=5_000,   seed_offset=123,
        g_factory=lambda bp: 100.0 * bp[6],         
        q_loc_factory=lambda bp: 0.7,
        cache_dir=os.path.join(BASE, "E9"),
        cache_file="PAL_res_40.npz",
        cache_key="log_likelihood_shared",
        param_dir=os.path.join(BASE, "E9"),
        param_file="E9_param_exp.npz",
        param_key="E9_param_exp",
        data_file="M40.csv",
        n_experiments=20,
        idx_a=7, idx_c=8, idx_xi=9, idx_q=10,   # E9 shifted by 1
    ),
    dict(
        label="E10", n_cities=40, mode="lookahead", n_particles=5_000,   seed_offset=100,
        g_factory=lambda bp: 100.0 * bp[6],
        q_loc_factory=lambda bp: 0.7,
        cache_dir=os.path.join(BASE, "E10"),
        cache_file="PAL_lookahead_40.npz",
        cache_key="log_likelihood_shared",
        param_dir=os.path.join(BASE, "E10"),
        param_file="E10_param_exp.npz",
        param_key="E10_param_exp",
        data_file="M40.csv",
        n_experiments=20,
        idx_a=7, idx_c=8, idx_xi=9, idx_q=10,
    ),
    dict(
        label="E12", n_cities=40, mode="res",       n_particles=5_000,   seed_offset=123,
        g_factory=lambda bp: 100.0 * bp[6],
        q_loc_factory=lambda bp: Q_MEAN,
        cache_dir=os.path.join(BASE, "E12"),
        cache_file="PAL_res_40.npz",
        cache_key="log_likelihood_shared",
        param_dir=os.path.join(BASE, "E12"),
        param_file="E12_param_exp.npz",
        param_key="E12_param_exp",
        data_file="UKmeasles_array.npy",
        n_experiments=20,
        idx_a=7, idx_c=8, idx_xi=9, idx_q=10,
    ),
    dict(
        label="E13", n_cities=40, mode="res",       n_particles=100_000, seed_offset=123,
        g_factory=lambda bp: 100.0 * bp[6],
        q_loc_factory=lambda bp: Q_MEAN,
        cache_dir=os.path.join(BASE, "E13"),
        cache_file="PAL_res_40.npz",
        cache_key="log_likelihood_shared",
        param_dir=os.path.join(BASE, "E12"),     # Same parameters as E12
        param_file="E12_param_exp.npz",
        param_key="E12_param_exp",
        data_file="UKmeasles_array.npy",
        n_experiments=20,
        idx_a=7, idx_c=8, idx_xi=9, idx_q=10,
    ),
    dict(
        label="E14", n_cities=40, mode="lookahead", n_particles=5_000,   seed_offset=123,
        g_factory=lambda bp: 100.0 * bp[6],
        q_loc_factory=lambda bp: Q_MEAN,
        cache_dir=os.path.join(BASE, "E14"),
        cache_file="PAL_lookahead_40.npz",
        cache_key="log_likelihood_shared",
        param_dir=PARAM,                          # final_parameters_lookahead_A.npy
        param_file="final_parameters_lookahead_A.npy",
        param_key=None,
        data_file="UKmeasles_array.npy",
        n_experiments=20,
        idx_a=7, idx_c=8, idx_xi=9, idx_q=10,
    ),
    dict(
        label="E15", n_cities=40, mode="lookahead", n_particles=100_000, seed_offset=123,
        g_factory=lambda bp: 100.0 * bp[6],
        q_loc_factory=lambda bp: Q_MEAN,
        cache_dir=os.path.join(BASE, "E15"),
        cache_file="PAL_lookahead_40.npz",
        cache_key="log_likelihood_shared",
        param_dir=PARAM,
        param_file="final_parameters_lookahead_A.npy",
        param_key=None,
        data_file="UKmeasles_array.npy",
        n_experiments=20,
        idx_a=7, idx_c=8, idx_xi=9, idx_q=10,
    ),
]

RESULTS: dict[str, tuple[float, float]] = {}

if __name__ == "__main__":
    for cfg in EXPERIMENTS:
        run_experiment(cfg)          

    print("\n All experiments ")
    print(f"{'Experiment':<10} {'LME_est':>12} {'SE':>10}")
    for lbl in sorted(RESULTS, key=lambda x: int(x[1:])):    
        est, se = RESULTS[lbl]
        print(f"{lbl:<10} {est:>12.4f} {se:>10.4f}")

    globals().update({f"{lbl}_est": RESULTS[lbl][0] for lbl in RESULTS})
    globals().update({f"{lbl}_se":  RESULTS[lbl][1] for lbl in RESULTS})
 
```



```{r bridge,include = FALSE}
library(reticulate)

E2_result <- c(py$E2_est,  py$E2_se)    # c(, )
E3_result <- c(py$E3_est,  py$E3_se)
E4_result <- c(py$E4_est,  py$E4_se)
E5_result <- c(py$E5_est,  py$E5_se)
E6_result <- c(py$E6_est,  py$E6_se)
E9_result <- c(py$E9_est,  py$E9_se)
E10_result <- c(py$E10_est, py$E10_se)
E12_result <- c(py$E12_est, py$E12_se)
E13_result <- c(py$E13_est, py$E13_se)
E14_result <- c(py$E14_est, py$E14_se)
E15_result <- c(py$E15_est, py$E15_se)


E1_result_vec <- c(as.numeric(E1_result[1]),  as.numeric(E1_result[2]),as.numeric(tmp_benchmark$total),as.numeric(tmp_negbinom))

E7_result_vec <- c(as.numeric(E7_result[1]),  as.numeric(E7_result[2]),as.numeric(real_one_benchmark$total),as.numeric(real_one_negbinom))

E8_result_vec <- c(as.numeric(E8_result[1]),  as.numeric(E8_result[2]),as.numeric(tmp_benchmark_spat$total),as.numeric(tmp_negbinom_spat))


E11_result_vec <- c(as.numeric(E11_result[1]),  as.numeric(E11_result[2]))

real_vec <- c(as.numeric(realdata_benchmark_spat$total),as.numeric(realdata_negbinom_spat))

E1_lambda <- E1_result_vec[1];  E1_sigma <- E1_result_vec[2]
E2_lambda <- E2_result[1];      E2_sigma <- E2_result[2]
E3_lambda <- E3_result[1];      E3_sigma <- E3_result[2]
E4_lambda <- E4_result[1];      E4_sigma <- E4_result[2]
E5_lambda <- E5_result[1];      E5_sigma <- E5_result[2]
E6_lambda <- E6_result[1];      E6_sigma <- E6_result[2]
E7_lambda <- E7_result_vec[1];  E7_sigma <- E7_result_vec[2]
E8_lambda <- E8_result_vec[1];  E8_sigma <- E8_result_vec[2]
E9_lambda <- E9_result[1];      E9_sigma <- E9_result[2]
E10_lambda<- E10_result[1];     E10_sigma<- E10_result[2]
E11_lambda<- E11_result[1];     E11_sigma<- E11_result[2]
E12_lambda<- E12_result[1];     E12_sigma<- E12_result[2]
E13_lambda<- E13_result[1];     E13_sigma<- E13_result[2]
E14_lambda<- E14_result[1];     E14_sigma<- E14_result[2]
E15_lambda<- E15_result[1];     E15_sigma<- E15_result[2]

E1_AMRA <- E1_result_vec[3];  E1_NegB <- E1_result_vec[4]
E2_AMRA <- E1_result_vec[3];      E2_NegB <- E1_result_vec[4]
E3_AMRA <- E1_result_vec[3];      E3_NegB <- E1_result_vec[4]
E4_AMRA <- E1_result_vec[3];      E4_NegB <- E1_result_vec[4]
E5_AMRA <- E1_result_vec[3];      E5_NegB <- E1_result_vec[4]
E6_AMRA <- E7_result_vec[3];      E6_NegB <- E7_result_vec[4]
E7_AMRA <- E7_result_vec[3];      E7_NegB <- E7_result_vec[4]
E8_AMRA <- E8_result_vec[3];  E8_NegB <- E8_result_vec[4]
E9_AMRA <- E8_result_vec[3];      E9_NegB <- E8_result_vec[4]
E10_AMRA<- E8_result_vec[3];     E10_NegB<- E8_result_vec[4]
E11_AMRA<- real_vec[1];     E11_NegB<- real_vec[2]
E12_AMRA<- real_vec[1];     E12_NegB<- real_vec[2]
E13_AMRA<- real_vec[1];     E13_NegB<- real_vec[2]
E14_AMRA<- real_vec[1];     E14_NegB<- real_vec[2]
E15_AMRA<- real_vec[1];     E15_NegB<- real_vec[2]
```

\begin{table}
\centering
\begin{tabular}{llcccrrrr}
\toprule
\textbf{$E$} & \textbf{$F$} & \textbf{$J$} & \textbf{$U$} & \textbf{$f_C$} &
$\lambda$ & $\sigma$ & ARMA & NegBinom \\
\midrule
$E_1$   & $\mathrm{BPF}$ & $J_2$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E1_lambda)`$  &
$`r sprintf('%.2f', E1_sigma)`$  &
$`r sprintf('%.2f', E1_AMRA)`$   &
$`r sprintf('%.2f', E1_NegB)`$   \\[2pt]

$E_2$   & $\PALV$        & $J_1$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E2_lambda)`$  &
$`r sprintf('%.2f', E2_sigma)`$  &
$`r sprintf('%.2f', E2_AMRA)`$   &
$`r sprintf('%.2f', E2_NegB)`$   \\[2pt]

$E_3$   & $\PALV$        & $J_2$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E3_lambda)`$  &
$`r sprintf('%.2f', E3_sigma)`$  &
$`r sprintf('%.2f', E3_AMRA)`$   &
$`r sprintf('%.2f', E3_NegB)`$   \\[2pt]

$E_4$   & $\PALL$        & $J_1$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E4_lambda)`$  &
$`r sprintf('%.2f', E4_sigma)`$  &
$`r sprintf('%.2f', E4_AMRA)`$   &
$`r sprintf('%.2f', E4_NegB)`$   \\[2pt]

$E_5$   & $\PALL$        & $J_2$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E5_lambda)`$  &
$`r sprintf('%.2f', E5_sigma)`$  &
$`r sprintf('%.2f', E5_AMRA)`$   &
$`r sprintf('%.2f', E5_NegB)`$   \\[2pt]

$E_6$   & $\PALL$        & $J_1$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E6_lambda)`$  &
$`r sprintf('%.2f', E6_sigma)`$  &
$`r sprintf('%.2f', E6_AMRA)`$   &
$`r sprintf('%.2f', E6_NegB)`$   \\[2pt]

$E_7$   & $\mathrm{BPF}$ & $J_1$ & $U_1$ & $C_1$ &
$`r sprintf('%.2f', E7_lambda)`$  &
$`r sprintf('%.2f', E7_sigma)`$  &
$`r sprintf('%.2f', E7_AMRA)`$   &
$`r sprintf('%.2f', E7_NegB)`$   \\[2pt]

$E_8$   & $\mathrm{BPF}$ & $J_2$ & $U_2$ & $C_2$ &
$`r sprintf('%.2f', E8_lambda)`$  &
$`r sprintf('%.2f', E8_sigma)`$  &
$`r sprintf('%.2f', E8_AMRA)`$   &
$`r sprintf('%.2f', E8_NegB)`$   \\[2pt]

$E_9$   & $\PALV$        & $J_1$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E9_lambda)`$  &
$`r sprintf('%.2f', E9_sigma)`$  &
$`r sprintf('%.2f', E9_AMRA)`$   &
$`r sprintf('%.2f', E9_NegB)`$   \\[2pt]

$E_{10}$ & $\PALL$       & $J_1$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E10_lambda)`$ &
$`r sprintf('%.2f', E10_sigma)`$ &
$`r sprintf('%.2f', E10_AMRA)`$  &
$`r sprintf('%.2f', E10_NegB)`$  \\[2pt]

$E_{11}$ & $\mathrm{BPF}$ & $J_2$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E11_lambda)`$ &
$`r sprintf('%.2f', E11_sigma)`$ &
$`r sprintf('%.2f', E11_AMRA)`$  &
$`r sprintf('%.2f', E11_NegB)`$  \\[2pt]

$E_{12}$ & $\PALV$       & $J_1$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E12_lambda)`$ &
$`r sprintf('%.2f', E12_sigma)`$ &
$`r sprintf('%.2f', E12_AMRA)`$  &
$`r sprintf('%.2f', E12_NegB)`$  \\[2pt]

$E_{13}$ & $\PALV$       & $J_2$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E13_lambda)`$ &
$`r sprintf('%.2f', E13_sigma)`$ &
$`r sprintf('%.2f', E13_AMRA)`$  &
$`r sprintf('%.2f', E13_NegB)`$  \\[2pt]

$E_{14}$ & $\PALL$       & $J_1$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E14_lambda)`$ &
$`r sprintf('%.2f', E14_sigma)`$ &
$`r sprintf('%.2f', E14_AMRA)`$  &
$`r sprintf('%.2f', E14_NegB)`$  \\[2pt]

$E_{15}$ & $\PALL$       & $J_2$ & $U_2$ & $C_1$ &
$`r sprintf('%.2f', E15_lambda)`$ &
$`r sprintf('%.2f', E15_sigma)`$ &
$`r sprintf('%.2f', E15_AMRA)`$  &
$`r sprintf('%.2f', E15_NegB)`$  \\
\bottomrule
\end{tabular}
\caption{Loglikelihood estimate, $\lambda$,  for each experiment described in Table\ 2.
Estimates derive from averaging 20 replications on a natural scale so that the basic particle filter estimate is unbiased.
The standard error, $\sigma$, is a jack-knife estimate implemented via the logmeanexp function in the pomp R package.
When $\sigma\gg 1$, this standard error is unreliable and we conclude only that the error is large.
ARMA gives the log-likelihood for an autoregressive moving average benchmark, and NegBinom is an autoregressive negative binomial benchmark.}
{#tbl-method-comparison}
\end{table}




Here, experiments $E_1$--$E_5$ provide a computationally tractable comparison using simulated data on a single unit. 
$E_1$ provides a ground truth for this particular model, a single-city SEIR model.
Comparing $\lambda_2$ and $\lambda_3$ with $\lambda_1$, we see that $\PALV$, without the lookahead, performs as expected for an approximate filter.
On this relatively easy task, it produces stable estimates, with log-likelihood values somewhat below the truth. However, both $E_2$ and $E_3$ failed to outperform the ARMA benchmark in our experiments, suggesting that PAL continues to face challenges when dealing with complex SEIR models and overdispersion.
Since $\PALV$ is filtering using a model that differs slightly from the data generating model, we expect to see a small shortfall, with $\lambda_2<\lambda_1$ and  $\lambda_3<\lambda_1$.
The difference, $\lambda_2-\lambda_3$, is statistically indistinguishable from zero in this experiment, showing that $J_1=5000$ particles is adequate for $\PALV$ on a single city.

$E_4$ and $E_5$ demonstrate the positive bias of $\PALL$ both at a usual number of particles and for an intensive calculation that may not be possible on larger problems.
The best estimates of this bias are $\lambda_4-\lambda_3$ and $\lambda_5-\lambda_3$, since $\PALL$ and $\PALV$ target the same quantity in the limit as $J\to\infty$.
Comparison of $E_5$ and $E_3$ shows that, even with a large number of particles and a low-dimensional dynamic model, $\PALL$ provides an over-stated log-likelihood estimate.

Comparing $E_4$ and $E_5$, we see that the smaller number of particles for $E_4$ leads to a higher log-likelihood estimate.
This would not happen in a situation where the likelihood estimate is unbiased with finite variance: in that situation, a higher number of particles is expected to lead to a higher log-likelihood estimate due to reduced negative bias resulting from Jensen's inequality.

Experiments $E_6$ and $E_7$ introduce the actual data, while still restricting to a single spatial unit.
We see that $\PALL$ reports a higher log-likelihood than BPF.
At face value, this could be because the PAL approximation is a superior model to the partially observed Markov process model implemented by BPF.
Or, it could be because BPF suffers from heavy negative bias due to high Monte Carlo error combined with Jensen's inequality.
The latter is not the case due to BPF's empirically low Monte Carlo error.
We have just discovered in $E_1$--$E_5$ that $\PALL$ reports an over-stated log-likelihood when the truth is known, so the most plausible explanation of $E_6$ and $E_7$ is simply that the same phenomenon occurs on the data.

Experiments $E_8$--$E_{15}$ investigate a 40 unit system.
For $E_8$, $E_9$ and $E_{10}$, we simulated from a model with the coupling parameter between towns set to zero.
That was done to study a situation where a block particle filter gives a consistent and low-variance estimate of the exact log-likelihood, calculated as $\lambda_8$.
We see the same story as the single-unit case, where the positive bias of  $\PALL$ is estimated by $(\lambda_{10}-\lambda_{9})/40=$ `r sprintf('%.2f', (E10_lambda-E9_lambda)/40)` per unit.
This bias is large enough that we also obtain $\lambda_{10}>\lambda_8$, with the difference being  $(\lambda_{10}-\lambda_{8})/40=$ `r sprintf('%.2f', (E10_lambda-E8_lambda)/40)` per unit.
We see that the $U=40$ results scale approximately linearly compared to $U=1$, providing us with supporting evidence for our hypothesis H2.

Experiments $E_{11}$--$E_{15}$ consider coupled models for all 40 cities in the full, real dataset.
Here, $E_{11}$ uses the same choice of shared and unit-specific parameters as model C in Table\ 3 of WWR, re-optimized using iterated BPF to account for differences between this model and the model of WWR.
Experiments $E_{12}$--$E_{15}$ directly use the parameters provided by WWR for their model A.
The choice of model A for PAL was motivated by computational convenience, since this simpler case is sufficient to demonstrate the positive bias in the lookahead method for the 40-unit dataset.

A comparison of $E_{14}$ and $E_{15}$ shows the bias of $\PALL$ using the code and parameter values of WWR, changing only the number of particles.
This provides the most direct evidence for H1, since the decrease of the log-likelihood by  $\lambda_{14}-\lambda_{15}=$ `r sprintf('%.2f', E15_lambda-E14_lambda)` log units when moving from $J_1=5\times 10^3$ to $J_2=10^5$ suggests strongly that the log-likelihood estimate with $J_1=5\times 10^3$ is over-optimistic.

Based on the comparison of $E_4$ and $E_5$, $\PALL$ does not report an accurate log-likelihood even on a single unit for $J=10^5$, and $40$ units is a considerably harder problem.
Further, we know from $E_8$--$E_{10}$ that $\PALL$ can over-state the true log-likelihood for the measles model with 40 units in a situation where the truth is known.
The evidence suggests that neither $E_{14}$ nor $E_{15}$ is a reliable estimator of either the PAL log-likelihood or the exact, unknown, log-likelihood that PAL aims to approximate for the data.
The performance of $\PALV$ on the 40-unit real dataset fails to outperform the ARMA benchmark, further indicating that applying PAL to high-dimensional data remains challenging.

\section{Some theoretical considerations for lookahead PAL}
\label{sec:theory}

 Since vanilla particle filter algorithms are unbiased for the likelihood, it might be reasonable to expect the PAL-SMC algorithm to be unbiased for the PAL likelihood, but this is not true for the lookahead PAL filter used by WWR. 
 This is a property of the lookahead part of the algorithm, developed by \citet{rimella23}, rather than the PAL approximation.
 Therefore, for the remainder of this section, we consider the simpler lookahead filter of \citet{rimella23}.
 
 Briefly, the vanilla particle filter is unbiased for the likelihood (and, therefore, negatively biased for the log-likelihood) because the self-normalization denominator in the resampling step happens to coincide with the conditional likelihood estimate on the numerator, leading to a fortuitous cancellation.
 Self-normalization does not always lead to unbiased likelihood estimates, as we can see from the following example.

Let $X$ take values $\{0,1\}$ with equal probability, and let $Y=X$ with probability 1. 
Suppose a single data point, $Y=1$. 
Suppose also an independent sample of $J$ particles, $x_{1:J}$, each with distribution matching $X$. 
Now, resample these particle with probability $p_j = (1-\epsilon) x_j/[\sum_j x_j] + \epsilon(1-x_j)/[\sum_j 1-x_j]$ so that, on average, a fraction $(1-\epsilon)$ of the resampled particles have value 1. 
Take $\epsilon \ll 1/J$, so that most resampled particle swarms contain no particles with value 0. 
Most particle swarms resulting from resampling will have $x_j = 1$ for all $j$, with the proper resampled weight $w_j = 1/(J\, p_j)$ being approximately $1/(2J)$ for all $j$. 
Rare swarms will have a particle with massive weight, approximately $1/(2J\epsilon)$. 
Under self-normalization, particle swarms with a massive weight will estimate the likelihood to be approximately zero, and particle swarms with $x_j=1$ for all $j$ will estimate the likelihood to be 1. 
By setting $\epsilon$ arbitrarily small, we can get an estimate whose expectation approaches 1 since with high probability we see only resampled swarms where every particle has value 1. 
If we take a different limit, with $J \to \infty$, the bias will go away asymptotically, but here we consider the case with fixed $J$ and $\epsilon \to 0$.

Importantly, the bias on the likelihood estimate in this example is positive. 
As mentioned earlier, a suboptimal forecast generally gives, on average, a negative bias on the conditional log-likelihood estimate, since log-likelihood is a proper scoring rule. 
This justifies assessing filters on their log-likelihood estimate in a similar way that one does for parameters in likelihood-based inference. 
A filter with a high log-likelihood estimate on simulated data from the target model is validated as a good likelihood approximation.
However, this does not necessarily apply to algorithms that look at future observations. When implementing lookahead algorithms, if you want the log-likelihood estimate to be conservative, you have to be extra careful to consider the bias. 
For unbiased likelihood estimates, the negative bias on the log-likelihood is a direct consequence of variance, and among such estimates it is reasonable to prefer a filter approximation with the highest log-likelihood estimate. 
For positively biased estimates, that is inappropriate.

\section{Conclusion}
\label{sec:conclusion}

The results in this article reinforce the investigation by \citet{hao24-arxiv} and lead to the conclusion that there is not currently a strong case for using PAL on the epidemiological models used by WWR to motivate its development.
Simpler particle filter methods apply to arbitrary Markov process models, whereas PAL is limited to a specific class of discrete-state Markov process models.
Basic particle filters, and their block particle filter extensions, have the plug-and-play property \citep{breto09,he10}.
Iterated block particle filters can be effective tools for data analysis of spatiotemporal epidemiological data \citep{wheeler24,li24}.
Certain automatically differentiable particle filter algorithms inherit this convenient plug-and-play property while providing derivative estimates with low bias and variance \citep{tan24}, and these may in future be extended to block particle filters.

PAL may potentially lead to dramatic computational improvements over particle filters some situations.
However, WWR's overdispersed generalization of PAL also involves a block particle filter component, at which point it shares many of the properties of particle filters.
The results of WWR, together with various other authors \citep{stocks18,he10,li24}, show that overdispersion is frequently necessary for a dynamic model to provide an adequate statistical description of epidemiological data.