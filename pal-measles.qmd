---
title: |
  | Poisson Approximate Likelihood versus block particle filter for a spatiotemporal measles model
  | __Preliminary version, not for circulation__
author:
  - "Kunyang He, Yize Hao and Edward L. Ionides"
date: "May 28, 2025"
abstract: |
 Filtering algorithms for high-dimensional nonlinear non-Gaussian partially observed stochastic processes provide access to the likelihood function and hence enable likelihood-based or Bayesian inference for complex dynamic systems. A novel Poisson approximate likelihood (PAL) filter was introduced by Whitehouse et al. (2023). PAL employs a Poisson approximation to conditional densities, offering a fast approximation to the likelihood function for a certain subset of partially observed Markov process models. PAL was demonstrated on an epidemiological metapopulation model for measles, specifically, a spatiotemporal model for disease transmission within and between cities. At face value, Table 3 of Whitehouse et al. (2023) suggests that PAL considerably out-performs previous analysis as well as an ARMA benchmark model. We show that PAL does not outperform a block particle filter and that the lookahead component of PAL was implemented in a way that introduces positive bias in the log-likelihood estimates.

format:
  pdf:
    cite-method: natbib
    biblio-style: apalike
    include-in-header:
      - text: |
          \usepackage{graphicx} % Required for inserting images
          \usepackage{fullpage}

          \newcommand\loglik{\lambda}
          \newcommand\fproc{f_{\mathrm{proc}}}
          \newcommand\fmeas{f_{\mathrm{meas}}}
          \newcommand\sproc{s_{\mathrm{proc}}}
          \newcommand\smeas{s_{\mathrm{meas}}}

          \newcommand\fprocDiscrete{\mathrm{discrete}}
          \newcommand\fprocEuler{\mathrm{Euler}}
          \newcommand\fmeasBinomial{\mathrm{binomial}}
          \newcommand\fmeasGaussian{\mathrm{Gaussian}}

          \newcommand\sprocDiscrete{\mathrm{discrete}}
          \newcommand\sprocEuler{\mathrm{Euler}}
          \newcommand\smeasBinomial{\mathrm{binomial}}
          \newcommand\smeasGaussian{\mathrm{Gaussian}}

          \newcommand{\PALL}{\ensuremath{\mathrm{PAL}_L}}
          \newcommand{\PALV}{\ensuremath{\mathrm{PAL}_V}}
          \usepackage{multirow}
          \usepackage[dvipsnames]{xcolor}
          \newcommand\eic[1]{{\color{Orange} #1}}

    documentclass: article
    pdf-engine: pdflatex
    geometry: "margin=1in"
    number-sections: true
bibliography: bib-pal-measles.bib
---

```{r Setup, include=FALSE}
library(reticulate)
# use_python("/usr/local/bin/python3", required = TRUE)
use_python("/Library/Frameworks/Python.framework/Versions/3.12/bin/python3", required = TRUE)

pomp_dir="pomp/"
```

\section{Introduction}
\label{sec:intro}

Investigations of the metapopulation dynamics of measles (i.e., studying how measles infection moves within and between collections of spatially distinct populations) have motivated various methodological innovations for inference on high-dimensional partially observed stochastic processes \citep{xia04,park20,ionides23-jasa}. The analysis by \citet{whitehouse23} (henceforth, WWR) provides a new approach to model-based inference on population dynamics via the Poisson approximate likelihood (PAL) filtering algorithm.
WWR claimed impressive results on both a low-dimensional rotavirus transmission model and a high-dimensional measles model.
On close inspection, the rotavirus results turned out to be overstated \citep{hao24-arxiv} leading to a published correction \citep{whitehouse25-correction}.
However, the spatiotemporal measles results were unaffected by that correction, and our present goal is to revisit this evidence.

In Section\ \ref{sec:numerics} we show by direct numerical experimentation that the WWR implementation of PAL has substantial positive bias in the log-likelihood estimate, and so the use of log-likelihood to support the use of the method is flawed.
While doing this, we show that a widely applicable block particle filter (BPF) is adequate on this problem.
In Section\ \ref{sec:theory} we explain theoretically how the positive bias for PAL arises as a result of the lookahead mechanism included in the implementation of PAL for this model.
The lookahead mechanism was not used by WWR for the rotavirus analysis since its purpose was to address numerical issues involved in high-dimensional filtering. Section\ \ref{sec:conclusion} is a concluding discussion.

For our current purposes, we do not have to delve into the details of the measles data and model, so we simply provide an overview.
The data are measles case counts aggregated over 2-week intervals for forty of the largest towns in England and Wales, from 1949 to 1964.
The data and the model are derived from \citet{park20} which builds on a long tradition of models described therein.
Recently, weekly data for more towns have become publicly available \citep{korevaar20}, but we limit ourselves to the data used by WWR.
The latent process model describes an integer count of infected, susceptible and recovered individuals in each town.
The rate of disease transmission within cities follows widely used epidemiological equations.
Transmission between pairs of cities follows a power law, diminishing with distance between the cities.
This is known as a gravity model.
Overdispersion for the latent dynamics is achieved by placing multiplicative gamma white noise on the transmission rate.
The measurement model is a discretized Gaussian approximation to an overdispersed binomial \citep{park20} or Gaussian noise on a binomial rate (WWR).
\citet{park20} used a particle filter known as a guided intermediate resampling filter (GIRF).
Recently, BPF has been shown to have good performance on this class of models \citep{ionides23-jasa,ionides24-sinica,ning23}. Therefore, we compare PAL with BPF.

\section{Numerical experiments for PAL and BPF}
\label{sec:numerics}

There are many possible numerical experiments comparing filters on spatiotemporal measles models. 
Here, we focus on developing experiments aimed at establishing two specific hypotheses:

\begin{enumerate}
\item[Q1] The lookahead version of the Poisson approximate likelihood estimator of WWR (\PALL) can have substantial positive bias on its log-likelihood estimate.

\item[Q2] The bias scales with the number of spatial units.
\end{enumerate}
We consider probabilistic filtering algorithms that are defined in the context of a model, its model parameters, and additional algorithmic parameters. 
Additionally, we require data which can either be the real historical measles data or can be simulated from another model that may or may not be the same model with the same parameters as used for the filter. We also have a choice of how many spatial units to include, these being UK towns in the measles example. The experimental variables, and the list of values we consider for them, are summarized in Table\ \ref{tbl:description} and further described below.

A critical part of our reasoning is that a probabilistic forecasting filter (i.e., one that solves the one-step prediction problem without looking ahead to future data) cannot, on average, obtain higher log-likelihood than the exact prediction distribution, when the data are generated by the exact model.
This is a restatement of the well-known fact that log-likelihood is a proper scoring rule \citep{gneiting07}.
To apply this property, we must work with simulated data so that the true generating model is known.
In high dimensions, it is generally not possible to calculate the exact prediction distribution with a small known bound on the error.
That is the reason why algorithms such as $\PALL$ are being invented.
There are two special situations where we can establish the true log-likelihood for the spatiotemporal measles models of interest: (i) when the number of spatial units is very small; (ii) when there is no spatial coupling, so it is enough to solve the filtering problem exactly for a single unit.
In both these cases, a basic particle filter provides the desired, essentially exact, log-likelihood estimate.
The basic particle filter is consistent and unbiased for the likelihood \citep{delmoral04} and so, when its estimates have low empirical variance, it provides the required ground truth.
In practice, order $10^5$ particles give a highly accurate log-likelihood for one unit, but the strong sensitivity of the particle filter to the curse of dimensionality \citep{bengtsson08} means that quantifiably exact estimates rapidly become unfeasible.
Therefore, we consider two choices of size for the system, $U=1$ and $U=40$, with the latter being the size of the system tested by WWR.
To allow for the study of systems without coupling, we consider two model variations, $C_1$ and $C_2$, where $C_1$ is the original model with gravity coupling used by WWR, and $C_2$ is a modification where direct movement of infection between cities is replaced by a constant background rate of importation of infection.

Evidently, the basic particle filter is not a powerful tool for general spatiotemporal systems, and so alternative approaches are needed.
The filtering and prediction problems are useful for situations where we no not have to make a forecast, and so the filter's prediction distribution for the $n$th observation, at time $t_n$, can be designed taking advantage of data ccurring at, or subsequent to, time $t_n$.
This can allow the construction of numerically efficient algorithms, and that is the goal of the lookahead filter, $\PALL$.
There is no mathematical theorem prohibiting a lookahead filter estimating a higher likelihood than the truth, and in an extreme case the lookahead filter could just assert a one-step prediction distribution with all its mass on the actual data.
Lookahead filters therefore need careful theoretical guarantees if we want to use a high likelihood estimate as evidence for both the success of the filter and (when doing data analysis) evidence supporting the model used to construct the filter.
We will investigate the theory behind $\PALL$ later, in Section\ \ref{sec:theory}, but for now we just need to note the potential hazard.

\begin{table}
\begin{tabular}{lllll}
Variable & Description & Value 1 & Value 2 & Value 3
\\
\hline
$F$ & filter algorithm & \PALL & \PALV & BPF
\\
$J$ & number of particles & $J_1=5\times 10^3$ &  $J_2=10^5$ &
\\
$U$ & number of spatial units & $U_1=1$ &  $U_2=40$ &
\\
$f_C$ & spatiotemporal coupling for filter & $C_1 =(g\neq 0, \iota = 0)$ & $C_2=(g=0, \iota\neq 0)$ &
\\
$\fproc$ & process model for filter & $\fprocEuler$ & $\fprocDiscrete$ &
\\
$\fmeas$ & measurement model for filter & $\fmeasBinomial$ & $\fmeasGaussian$ &
\\
$f_{\theta}$ & parameter for filter & $\hat\theta^*_{BPF}$ & $\hat\theta_{PAL}$ &
\\
$s_C$ & spatiotemporal coupling for simulation & $C_1 =(g\neq 0, \iota = 0)$ & $C_2=(g=0, \iota\neq 0)$ &
\\
$\sproc$ & process model for simulation & $\sprocEuler$ & $\sprocDiscrete$ &
\\
$\smeas$ & measurement model for simulation & $\smeasBinomial$ & $\smeasGaussian$ &
\\
$s_{\theta}$ & parameter for simulation & $\hat\theta^*_{BPF}$ & $\hat\theta_{PAL}$ &
\\
\hline
\end{tabular}
\caption{Variables for the numerical experiments and their set of values.}
\label{tbl:description}
\end{table}

\begin{itemize}
\item[F] The filtering algorithm. 
{\PALL} is the lookahead filter of WWR, and {\PALV} is the plain, so-called vanilla, implementation. 
BPF is the block particle filter of \citet{rebeschini15} implemented as bpfilter in spatPomp \citep{asfaw24}.
Likelihood optimization for {\PALL} and {\PALV} is conducted using stochastic gradient descent and automatic differentation, using the implementation by WWR. 
Likelihood optimization for BPF is conducted using the iterated block particle filter algorithm \citep{ning23,ionides24-sinica} implemented as ibpf in spatPomp.
For $U=1$, and for $U=40$ with $g=0$, BPF is identical to a basic particle filter. 
For simplicity, we use spatPomp::bpfilter even when pomp::pfilter is equivalent.

\item[$f_C$] Spatiotemporal coupling for the filter model. 
$C_1$ corresponds to the coupling used by WWR, with spatial movement of infection ($g\neq 0$) and no background immigration of infection from outside the study system ($\iota=0$). 
In order to test the methods on a high-dimensional system for which the true likelihood is known to a good degree of accuracy, we also consider setting $C_2$, without coupling ($g=0$) and with compensating immigration to prevent permanent extinction of measles in small towns ($\iota\neq 0$).
Note that when $U=U_1=1$, we use the largest city, London, for which stochastic extinctions are very unlikely. 
Also, for $U=1$, the value of $g$ becomes irrelevant.

\item[$\fproc$] Latent process transition model for the filter.
$\fproc=\fprocDiscrete$ corresponds to the choice of WWR where a single gamma-distributed dynamic noise variable is chosen for each observation interval. $\fproc=\fprocEuler$ corresponds to independent gamma noise variables on each Euler time step, so that the limit of the process model (as the Euler time step decreases) corresponds to a continuous-time over-dispersed Markov chain. 
Both $\fproc=\fprocDiscrete$ and $\fproc=\fprocEuler$ are implemented with a step of $1/2$ week for the multinomial transitions conditional on the gamma noise. 

\item[$\fmeas$] Measurement model for the filter. $\fmeas=\fmeasBinomial$ corresponds to binomial measurements with truncated multiplicative Gaussian noise on the reporting rate, i.e., the expected fraction of infections that are reported. 
The basic PAL algorithm requires a binomial measurement model, but the SMC-PAL extension permits noise on the measurement probability.
$\fmeas=\fmeasGaussian$ corresponds to a discretized Gaussian measurement model. 
Using a measurement model that can directly be evaluated, without costly Monte Carlo calculation, assists with efficient Monte Carlo inference.
Beta-binomial or negative binomial distributions would also meet this criterion.

\item[$f_{\theta}$] Model parameter vector for the filter. 
{\PALL} and {\PALV} are evaluated at optimized parameter vectors for each data set. 
For PAL, there is generally no true POMP model for which PAL is an exact filter.
However, we give PAL a reasonable chance to show its capabilities by optimizing it using the code provided by WWR. 

\item[$\sproc$] Process model for the simulation. 
Always set to $\sproc=\sprocEuler$ since simulations were carried out using an implementation of the model in spatPomp. 

\item[$\smeas$] Measurement model for the simulation. 
Always set to $\smeas=\smeasGaussian$ since simulations were carried out using an implementation of the model in spatPomp. 



\item[$s_{\theta}$] Model parameter vector for the simulated data.
To have an essentially exact likelihood evaluation using BPF, we have $s_\theta=f_\theta$ for all BPF situations. 
The exact value of $s_\theta$ is not critical to the argument, but we use a value obtained by maximizing the BPF likelihood.

\end{itemize}

We consider two software platforms for the experiments: all $\PALL$ and $\PALV$ calculations were carried out using the code provided by WWR, and all BPF calculations were carried out using the spatPomp R package.
Simulations were carried out using spatPomp, since our reasoning depends critically on the particle filter calculations being carried out with the data drawn from the model assumed by the filter.
We present results for a single simulation for the experimental treatments with simulated data.
That simplifies the experimental design and permits the computional effort to focus on a few direct comparisons between the methods on a small number of simulated datasets.

\begin{table}\label{tbl:treatments}
\centering
\begin{tabular}{llllllllllll}
E & F & J & U & 
  $f_C$ & $\fproc$ &  $\fmeas$ & $f_{\theta}$ &
  $s_C$ & $\sproc$ &  $\smeas$ & $s_{\theta}$ 
\\
\hline
%1 &  \PALV & $J_1$ & $U_1$ & 
%  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
%  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
%\\
$E_1$ &  BPF & $J_2$ & $U_1$ & 
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_2$ &  \PALV & $J_1$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_3$ &  \PALV & $J_2$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_4$ &  \PALL & $J_1$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_5$ &  \PALL & $J_2$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_6$ &  \PALL & $J_1$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_7$ &  BPF & $J_1$ & $U_1$ & 
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_8$ &  BPF & $J_2$ & $U_2$ & 
  $C_2$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$ &
  $C_2$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_9$ &  \PALV & $J_1$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_2$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_{10}$ &  \PALL & $J_1$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  $C_2$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
\\
$E_{11}$ &  BPF & $J_2$ & $U_2$ & 
  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_{12}$ &  \PALV & $J_1$ & $U_2$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
$E_{13}$ &  \PALL & $J_1$ & $U_1$ & 
  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
\\
%4 &  \PALV & $J_2$ & $U_1$ & 
%  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
%  \multicolumn{4}{c}{\hfill --------------- \hfill data \hfill --------------- \hfill}
%\\
%5 &  \PALV & $J_2$ & $U_1$ & 
%  $C_1$ & $\fprocDiscrete$ & $\fmeasBinomial$ & $\hat\theta_{PAL}$ &
%  $C_1$ & $\fprocEuler$ & $\fmeasGaussian$ & $\hat\theta^*_{BPF}$
%\\
\hline
 \end{tabular}
 \caption{Combinations of variable values used for each experiment, $E_k$, $k=1\dots 13$. The simulation settings, $s_C$, $\sproc$, $\smeas$ and $s_\theta$, are applicable only when we filter simulated data rather than real data.}
\end{table}

Experiment $E_k$ has two primary outcomes, a log-likelihood estimate, $\loglik_k$, and its standard error, $\sigma_k$. 
These results are tabulated in Table\ \ref{tbl:method_comparison}, together with log-ARMA and autoregressive negative binomial benchmarks \eic{BENCHMARKS STILL NEEDED - PROBABLY IN TABLE \ref{tbl:method_comparison}.}


```{r E1,include=FALSE}
## ----packages,incluxde=F,echo=F,cache=F----------------------------------------
library("spatPomp")
library("ggplot2")
library("tidyverse")
library("knitr")
library("doRNG")
library("doParallel")


library(pomp)

measles_cases <- read.csv(paste0(pomp_dir,"case1.csv"))
measles_covar <- read.csv(paste0(pomp_dir,"covar2.csv"))

measles_cases<- measles_cases[measles_cases$city == "LONDON", ]
measles_covar <- measles_covar[measles_covar$city == "LONDON", ]


measles_cases <-  measles_cases[,-1]
measles_covar <-  measles_covar[,-1]



colnames(measles_cases) <- c("time","cases1")
colnames(measles_covar) <- c("time",
                             "lag_birthrate1","pop1")


basic_params <- c(
  alpha       = 1,
  iota        = 0,
  betabar     = 6.32,
  c           = 0.219,
  a           = 0.1476,
  rho         = 0.142,
  gamma       = 0.0473,
  delta       = 0.02/(26*4),  # timescale transform
  sigma_xi    = 0.318,
  gaussianrho = 0.7,
  psi         = 0.306,
  g           = 0,
  S_0         = 0.02545,
  E_0         = 0.00422,
  I_0         = 0.000061
)


rproc <- Csnippet("
  double t_mod = fmod(t, 364.0);
  double br1;
  double beta1, seas1;
  double foi1;         
  double xi1;           
  double betafinal1;

  int trans_S1[2], trans_E1[2], trans_I1[2];
  double prob_S1[2], prob_E1[2], prob_I1[2];

  if ((t_mod >= 6 && t_mod < 99) ||
      (t_mod >= 115 && t_mod < 198) ||
      (t_mod >= 252 && t_mod < 299) ||
      (t_mod >= 308 && t_mod < 355)) {
    seas1 = 1.0 + a * 2 * (1 - 0.759);
  } else {
    seas1 = 1.0 - 2 * a * 0.759;
  }

  beta1 = betabar * seas1;

  if (fabs(t_mod - 248.5) < 0.5) {
    br1 = c * lag_birthrate1;
  } else {
    br1 = (1.0 - c) * lag_birthrate1 / 103.0;
  }

  double I_ratio1 = I1 / pop1;

  foi1 = pow((I1 + iota) / pop1, alpha);
 
  xi1 = rgamma(sigma_xi, 1 / sigma_xi);;
  betafinal1 = beta1 * I_ratio1 * xi1;

  int SD1 = rbinom(S1, delta);
  int ED1 = rbinom(E1, delta);
  int ID1 = rbinom(I1, delta);
  int RD1 = rbinom(R1, delta);

  S1 -= SD1;  E1 -= ED1;  I1 -= ID1;  R1 -= RD1;
  
  prob_S1[0] = exp(-dt * betafinal1);
  prob_S1[1] = 1 - exp(-dt * betafinal1);

  prob_E1[0] = exp(-dt * rho);
  prob_E1[1] = 1 - exp(-dt * rho);

  prob_I1[0] = exp(-dt * gamma);
  prob_I1[1] = 1 - exp(-dt * gamma);

  rmultinom(S1, prob_S1, 2, trans_S1);
  rmultinom(E1, prob_E1, 2, trans_E1);
  rmultinom(I1, prob_I1, 2, trans_I1);

  S1 = trans_S1[0] + rpois(br1);
  E1 = trans_E1[0] + trans_S1[1];
  I1 = trans_I1[0] + trans_E1[1];
  R1 += trans_I1[1];
  C1 += trans_I1[1];
");




## ----dmeasure-------------------------------------------------
dmeas <- Csnippet("
  double m = gaussianrho*C1;
  double v = m*(1.0-gaussianrho+psi*psi*m);
  double tol = 0.0;
  if (cases1 > 0.0) {
    lik = pnorm(cases1+0.5,m,sqrt(v)+tol,1,0)
           - pnorm(cases1-0.5,m,sqrt(v)+tol,1,0) + tol;
  } else {
    lik = pnorm(cases1+0.5,m,sqrt(v)+tol,1,0) + tol;
  }
  if (give_log) lik = log(lik);
")

## ----rmeasure-------------------------------------------------
rmeas <- Csnippet("
  double m = gaussianrho*C1;
  double v = m*(1.0-gaussianrho+psi*psi*m);
  double tol = 0.0;
  cases1 = rnorm(m,sqrt(v)+tol);
  if (cases1 > 0.0) {
    cases1 = nearbyint(cases1);
  } else {
    cases1 = 0.0;
  }
")

rinit <- Csnippet("
  double probs1[4];
  probs1[0] = S_0;
  probs1[1] = E_0;
  probs1[2] = I_0;
  probs1[3] = 1.0 - probs1[0] - probs1[1] - probs1[2];

  int counts1[4];
  rmultinom(pop1, probs1, 4, counts1);

  S1 = counts1[0];
  E1 = counts1[1];
  I1 = counts1[2];
  R1 = counts1[3];
  C1 = 0;
");

basic_log_names   <- c("rho", "gamma", "sigma_xi", "betabar", "g", "iota", "delta")
basic_logit_names <- c("a", "alpha", "c", "gaussianrho", "S_0", "E_0", "I_0", "psi")
log_names   <- basic_log_names
logit_names <- basic_logit_names
measles_partrans <- parameter_trans(
  log   = log_names,
  logit = logit_names
)

one_city_pomp <- pomp(
  data       = measles_cases,
  times      = "time",
  t0         = 0,
  rprocess   = euler(rproc, delta.t = 3.5), 
  rinit      = rinit,
  dmeasure   = dmeas,
  rmeasure   = rmeas,
  statenames = c("S1","E1","I1","R1","C1"),
  paramnames = c("alpha","iota","betabar","c","a","rho","gamma",
                 "delta","sigma_xi","g","gaussianrho","psi",
                 "S_0","E_0","I_0"),
  covar      = covariate_table(measles_covar,times = "time"),
  covarnames = c("lag_birthrate1","pop1"),
  accumvars  = c("C1")
)

coef(one_city_pomp) <- basic_params

sim <- simulate(one_city_pomp, params =  basic_params,  nsim   = 1,
                seed   = 154234)

Pomp_dir <- paste0(pomp_dir,"Pomp_E",1,"/")
if(!dir.exists(Pomp_dir)) dir.create(Pomp_dir)

stew(file=paste0(Pomp_dir,"E1_non_optimize.rda"),seed=456,{
  
  cat(capture.output(sessionInfo()),
      file=paste0(Pomp_dir,"sessionInfo.txt"),sep="\n")
  
  pf_logLik <- replicate(20,
                         logLik(pfilter(sim,Np = 100000))
  )
  
  
})
E1_result <- logmeanexp(pf_logLik,se = T)

E1_result[1]

tmp_benchmark <- arma_benchmark(sim)

tmp_benchmark$total

E1_sim <- sim@data

E1_sim <- t(E1_sim)

negloglik <- function(x) optim(par=c(0.5,0.5,1),function(theta)-sum(dnbinom(x,mu=theta[1]+theta[2]*c(0,head(x,-1)),size=theta[3],log=T)))$value

tmp_negbinom <- -sum(apply(E1_sim,2,negloglik))

sim.data <- as.data.frame(sim)

londonsim <- sim.data$cases1

df <- as.data.frame(t(londonsim)) 

colnames(df) <- 0:(length(londonsim) - 1)

write.csv(df, "londonsim.csv", row.names = FALSE)
```


```{r E8,include=FALSE}
## ----packages,incluxde=F,echo=F,cache=F----------------------------------------
library("spatPomp")
library("ggplot2")
library("tidyverse")
library("knitr")
library("doRNG")
library("doParallel")
cores <-  as.numeric(Sys.getenv('SLURM_NTASKS_PER_NODE', unset=NA))
if(is.na(cores)) cores <- detectCores()
registerDoParallel(cores)
ggplot2::theme_set(ggplot2::theme_bw())

stopifnot(packageVersion("pomp")>="5.0")

set.seed(42)

basic_params <- c(
  alpha     = 1,
  iota      = 0.1,
  betabar   = 6.32,
  c         = 0.219,
  a         = 0.1467,
  rho       = 0.142,
  gamma     = 0.0473,
  delta     = 0.02/(26*4),
  sigma_xi  = 0.318,
  gaussianrho     = 0.7,
  psi      = 0.306,
  g         = 0,
  S_0       = 0.02545,
  E_0       = 0.00422,
  I_0       = 0.000061
)

expandedParNames <- NULL

dt <- 3.5
U  <- 40

measles_cases  <- read.csv(paste0(pomp_dir,"case1.csv"))

measles_covar  <- read.csv(paste0(pomp_dir,"covar2.csv"))

measles_covarnames <- paste0(rep(c("pop", "lag_birthrate"), each = U), 1:U)
measles_unit_covarnames <- c("pop", "lag_birthrate")

data_measles_distance <- read.csv(paste0(pomp_dir,'data_measles_distance.csv'))




data_measles_distance <- data_measles_distance

v_by_g <- as.matrix(data_measles_distance)

to_C_array <- function(v) paste0("{", paste0(v, collapse = ","), "}")
v_by_g_C_rows  <- apply(v_by_g, 1, to_C_array)
v_by_g_C_array <- to_C_array(v_by_g_C_rows)
v_by_g_C <- Csnippet(paste0("const double v_by_g[", U, "][", U, "] = ", v_by_g_C_array, "; "))

parNames      <- names(basic_params)
fixedParNames <- setdiff(parNames, expandedParNames)

set_expanded <- Csnippet(
  paste0("const int ", expandedParNames, "_unit = 1;\n", collapse = " ")
)
set_fixed <- Csnippet(
  paste0("const int ", fixedParNames, "_unit = 0;\n", collapse = " ")
)
measles_globals <- Csnippet(
  paste(v_by_g_C, set_expanded, set_fixed, sep = "\n")
)

measles_paramnames <- c(
  if (length(fixedParNames) > 0) {
    paste0(fixedParNames, "1")
  },
  if (length(expandedParNames) > 0) {
    paste0(rep(expandedParNames, each = U), 1:U)
  }
)

unit_statenames <- c("S", "E", "I", "R", "C")


measles_rprocess <- spatPomp_Csnippet(
  unit_statenames  = c("S", "E", "I", "R", "C"),
  unit_covarnames  = c("pop", "lag_birthrate"),
  unit_paramnames  = c("alpha", "iota", "betabar", "c", "a",
                       "rho", "gamma", "delta", "sigma_xi", "g"),
  code ="
    // Variables
    double br, beta, seas, foi, births, xi, betafinal;
    int trans_S[2], trans_E[2], trans_I[2];
    double prob_S[2], prob_E[2], prob_I[2];
    int SD[U], ED[U], ID[U], RD[U];
    double powVec[U];
    int u, v;

    // Calculate the day of the year without any offset
    // Pre-computing this saves substantial time
    // powVec[u] = pow(I[u]/pop[u], alpha);
    for (u = 0; u < U; u++) {
        powVec[u] = I[u] / pop[u];
        // IS THIS INTENDED TO BE FIXED TO ALPHA=1?
    }

    for (u = 0; u < U; u++) {
        double t_mod = fmod(t, 364.0);

        // Transmission rate
        if ((t_mod >= 6 && t_mod < 99) || (t_mod >= 115 && t_mod < 198) ||
            (t_mod >= 252 && t_mod < 299) || (t_mod >= 308 && t_mod < 355))
            seas = 1.0 + a[u * a_unit] * 2 * (1 - 0.759);
        else
            seas = 1.0 - 2 * a[u * a_unit] * 0.759;

        beta = betabar[u * betabar_unit] * seas;

        // Birth rate calculation
        if (fabs(t_mod - 248.5) < 0.5) {
            br = c[u * c_unit] * lag_birthrate[u];
        } else {
            br = (1.0 - c[u * c_unit]) * lag_birthrate[u] / 103;
        }

        // Expected force of infection
        if (alpha[u * alpha_unit] == 1.0 && iota[u * iota_unit] == 0.0) {
            foi = I[u] / pop[u];
        } else {
            foi = pow((I[u] + iota[u * iota_unit]) / pop[u], alpha[u * alpha_unit]);
        }

        for (v = 0; v < U; v++) {
            if (v != u) {
                foi += g[u * g_unit] * v_by_g[u][v] * (powVec[v] - powVec[u]) / pop[u];
            }
        }

        xi = rgamma(sigma_xi[u * sigma_xi_unit], 1 / sigma_xi[u * sigma_xi_unit]);
        betafinal = beta * foi * xi;  // Stochastic force of infection

        // Poisson births
        births = rpois(br);

        SD[u] = rbinom(S[u], delta[u * delta_unit]);
        ED[u] = rbinom(E[u], delta[u * delta_unit]);
        ID[u] = rbinom(I[u], delta[u * delta_unit]);
        RD[u] = rbinom(R[u], delta[u * delta_unit]);

        S[u] = S[u] - SD[u];
        E[u] = E[u] - ED[u];
        I[u] = I[u] - ID[u];
        R[u] = R[u] - RD[u];

        // Probabilities for state transitions
        prob_S[0] = exp(-dt * betafinal);
        prob_S[1] = 1 - exp(-dt * betafinal);

        prob_E[0] = exp(-dt * rho[u * rho_unit]);
        prob_E[1] = 1 - exp(-dt * rho[u * rho_unit]);

        prob_I[0] = exp(-dt * gamma[u * gamma_unit]);
        prob_I[1] = 1 - exp(-dt * gamma[u * gamma_unit]);

        // Multinomial transitions
        rmultinom(S[u], &prob_S[0], 2, &trans_S[0]); // B, (S-F)-B
        rmultinom(E[u], &prob_E[0], 2, &trans_E[0]); // C, (E-F)-C
        rmultinom(I[u], &prob_I[0], 2, &trans_I[0]); // E, (I-F)-D

        // Update compartments
        S[u] = trans_S[0] + births;
        E[u] = trans_E[0] + trans_S[1];
        I[u] = trans_I[0] + trans_E[1];
        R[u] = R[u] + trans_I[1];
        C[u] += trans_I[1];  // True incidence
    }
"
)


measles_dmeasure <-  spatPomp_Csnippet(
  unit_statenames = 'C',
  unit_obsnames = 'cases',
  unit_paramnames = c('gaussianrho','psi'),
  code="
      double m,v;
      double tol = 1e-300;
      double mytol = 1e-5;
      int u;
      lik = 0;
      for (u = 0; u < U; u++) {
        m = gaussianrho[u*gaussianrho_unit]*(C[u]+mytol);
        v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);

        // Deal with NA measurements by omitting them
        if(!(ISNA(cases[u]))){
          // C < 0 can happen in bootstrap methods such as bootgirf
          if (C[u] < 0) {lik += log(tol);} else {
            if (cases[u] > tol) {
              lik += log(pnorm(cases[u]+0.5,m,sqrt(v)+tol,1,0)-
                pnorm(cases[u]-0.5,m,sqrt(v)+tol,1,0)+tol);
            } else {
                lik += log(pnorm(cases[u]+0.5,m,sqrt(v)+tol,1,0)+tol);
            }
          }
        }
      }
      if(!give_log) lik = (lik > log(tol)) ? exp(lik) : tol;
    "
)

measles_rmeasure <- spatPomp_Csnippet(
  method='rmeasure',
  unit_paramnames=c('gaussianrho','psi'),
  unit_statenames='C',
  unit_obsnames='cases',
  code="
      double m,v;
      double tol = 1.0e-300;
      int u;
      for (u = 0; u < U; u++) {
        m = gaussianrho[u*gaussianrho_unit]*(C[u]+tol);
        v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);
        cases[u] = rnorm(m,sqrt(v)+tol);
        if (cases[u] > 0.0) {
          cases[u] = nearbyint(cases[u]);
        } else {
          cases[u] = 0.0;
        }
      }
    "
)

measles_dunit_measure <- spatPomp_Csnippet(
  unit_paramnames=c('gaussianrho','psi'),
  code="
      double mytol = 1e-5;
      double m = gaussianrho[u*gaussianrho_unit]*(C+mytol);
      double v = m*(1.0-gaussianrho[u*gaussianrho_unit]+psi[u*psi_unit]*psi[u*psi_unit]*m);
      double tol = 1e-300;
      // C < 0 can happen in bootstrap methods such as bootgirf
      if(ISNA(cases)) {lik=1;} else { 
        if (C < 0) {lik = 0;} else {
          if (cases > tol) {
            lik = pnorm(cases+0.5,m,sqrt(v)+tol,1,0)-
              pnorm(cases-0.5,m,sqrt(v)+tol,1,0)+tol;
          } else {
            lik = pnorm(cases+0.5,m,sqrt(v)+tol,1,0)+tol;
          }
        }
      }
      if(give_log) lik = log(lik);
    "
)

measles_rinit <- spatPomp_Csnippet(
  unit_paramnames = c("S_0", "E_0", "I_0"),
  unit_statenames = c("S", "E", "I", "R", "C"),
  unit_covarnames = "pop",
  code = "
    int u;
    for (u = 0; u < U; u++) {
        double probs[4];
        probs[0] = S_0[u * S_0_unit];
        probs[1] = E_0[u * E_0_unit];
        probs[2] = I_0[u * I_0_unit];
        probs[3] = 1.0 - probs[0] - probs[1] - probs[2];
        int counts[4];
        rmultinom(pop[u], &probs[0], 4, &counts[0]);
        S[u] = counts[0];
        E[u] = counts[1];
        I[u] = counts[2];
        R[u] = counts[3];
        C[u] = 0;
    }
")




### === Parameter Transformation Settings ===

basic_log_names   <- c("rho", "gamma", "sigma_xi", "betabar", "g", "iota", "delta")
basic_log_names   <- setdiff(basic_log_names, fixedParNames)

basic_logit_names <- c("a", "alpha", "c", "gaussianrho", "S_0", "E_0", "I_0",'psi')
basic_logit_names <- setdiff(basic_logit_names, fixedParNames)
log_names   <- unlist(lapply(basic_log_names, function(x, U) paste0(x, 1:U), U))
logit_names <- unlist(lapply(basic_logit_names, function(x, U) paste0(x, 1:U), U))
measles_partrans <- parameter_trans(log = log_names, logit = logit_names)

m9 <- spatPomp(
  measles_cases,
  units           = "city",
  times           = "days",
  t0              = min(measles_cases$days) - 14,
  unit_statenames = unit_statenames,
  covar           = measles_covar,
  rprocess        = euler(measles_rprocess, delta.t = 3.5),
  unit_accumvars  = c("C"),
  paramnames      = measles_paramnames,
  globals         = measles_globals,
  rinit           = measles_rinit,
  dmeasure        = measles_dmeasure,
  rmeasure        = measles_rmeasure,
  dunit_measure   = measles_dunit_measure,
  partrans = measles_partrans
)

measles_params <- rep(0, length = length(measles_paramnames))

names(measles_params) <- measles_paramnames

for (p in fixedParNames)
  measles_params[paste0(p, 1)] <- basic_params[p]
for (p in expandedParNames)
  measles_params[paste0(p, 1:U)] <- basic_params[p]
coef(m9) <- measles_params

sim <- simulate(m9, params =  measles_params,  nsim   = 1,
                seed   = 154234)
##

spatPomp_dir <- paste0(pomp_dir,"E_",8,"/")
if(!dir.exists(spatPomp_dir)) dir.create(spatPomp_dir)

stew(file=paste0(spatPomp_dir,"E8.rda"),seed=124,{
  cat(capture.output(sessionInfo()),
      file=paste0(spatPomp_dir,"sessionInfo.txt"),sep="\n")
  
  bpf_logLik_40 <- foreach(i = 1:20, .combine = c) %dopar% {
    logLik(bpfilter(sim, Np = 5000, block_size = 1))
  }
})


E8_result <- logmeanexp(bpf_logLik_40,se = T,ess = T)

tmp_benchmark_spat <- arma_benchmark(sim)

tmp_benchmark_spat$total

E8_sim <- sim@data

E8_sim <- t(E8_sim)

negloglik <- function(x) optim(par=c(0.5,0.5,1),function(theta)-sum(dnbinom(x,mu=theta[1]+theta[2]*c(0,head(x,-1)),size=theta[3],log=T)))$value

tmp_negbinom_spat <- -sum(apply(E8_sim,2,negloglik))

## Prepare the simulated data for python.
simdata <- as.data.frame(sim)

simdata <- simdata[order(simdata$city),]

yt <- simdata$cases

M6 <- matrix(yt, nrow = 40, byrow = TRUE)

M6 <- as.data.frame(M6)

colnames(M6) <- as.character(0:415)

write.csv(M6,file = "M6.csv",row.names = F)
```


```{python E2,include=FALSE}
import os
import numpy as np

# ------------- cache settings -----------------------------------
CACHE_DIR  = "wwr/E2"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_vanilla.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# ----------------------------------------------------------------
# 1) Try to load cached results
# ----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results → {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# ----------------------------------------------------------------
# 2) No cache → run the original simulation code unmodified
# ----------------------------------------------------------------
else:
    print("[cache] No cache found – running the full simulation …")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>  ORIGINAL CODE — DO NOT EDIT  <<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    #  (Everything between the double lines is your untouched script;
    #   only indentation has changed so it nests inside this block.)
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize

    import matplotlib.pyplot as plt
    from scipy.special import logsumexp
    from sympy.polys.benchmarks.bench_solvers import uk_10

    from Scripts.measles_PALSMC_perstep import PAL_run_likelihood_lookahead

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("E2"):
        os.makedirs("E2")

    os.environ['PYTHONHASHSEED'] = '45'
    random.seed(45)
    np.random.seed(45)
    tf.random.set_seed(45)

    UKbirths_array = np.load("Data/UKbirths_array.npy")
    UKpop_array = np.load("Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("Data/UKmeasles_array.npy")
    modelA_array = np.load("Data/Parameter/final_parameters_lookahead_A.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array[18:19, :], dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array[18:19, :], dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array[18:19, :], dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array[18:19, 18:19],
                                                   dtype=tf.float32)
    df = pd.read_csv("Data/londonsim.csv")

    data_array = df.values
    UKmeasles = tf.convert_to_tensor(data_array, dtype=tf.float32)

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype = tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)
    initial_pop = UKpop[:,0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14/tf.cast(intermediate_steps, dtype = tf.float32), dtype = tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(T, intermediate_steps, term, school)

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype = tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype = tf.float32)
    p      = tf.constant(0.759, dtype = tf.float32)
    delta_year = tf.convert_to_tensor([[1/50]], dtype = tf.float32)*tf.ones((n_cities, 4), dtype = tf.float32)

    # increase the n_experiments for proper variance estimates, use n_experiments to just test the log-likelihood
    n_experiments = 20

    # lookahead
    # shared
    best_parameters = np.load("Data/Parameter/final_parameters_lookahead_A.npy")
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)
    q_mean = tf.constant(np.mean(np.load("Data/q_mean.npy")), dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)

    # --- parameter block (same format, new values) -----------------------
    pi_0_1 = 0.02536
    pi_0_2 = 0.0042
    pi_0_3 = 0.000061
    pi_0   = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(6.30 * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([0.142], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([0.0473], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = tf.convert_to_tensor([[0.0]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(0.1476, dtype=tf.float32)
    c      = tf.constant(0.219 , dtype=tf.float32)
    xi_var = tf.convert_to_tensor(0.318 , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(0.305, dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=0.7, scale=q_var, low=0.0, high=1.0)

    n_particles = 5000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = np.max(x)
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_minus_k = np.delete(x, k)
                max_x_mk = np.max(x_minus_k)
                jk_vals[k] = max_x_mk + np.log(np.mean(np.exp(x_minus_k - max_x_mk)))
            xse = (n - 1) * np.std(jk_vals, ddof=1) / np.sqrt(n)
            results["se"] = xse
        if ess:
            w = np.exp(x - max_x)
            xss = np.sum(w) ** 2 / np.sum(w**2)
            results["ess"] = xss
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_res(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    mean_log = np.mean(log_likelihood_shared)

    print("Variance of log likelihoods:", variance_log)
    print("mean of log likelihoods:", mean_log)

    out_file_path = os.path.join("E2", "PAL_vanilla.csv")
    np.savetxt(out_file_path, log_likelihood_shared, delimiter=",")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>  ORIGINAL CODE END  <<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # ------ save array to cache so future runs can skip heavy work ---
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached → {CACHE_FILE}")

# --------------------------------------------------------------------
# 3) Quick summary (identical whether loaded or freshly computed)
# --------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    """
    Return (log-mean-exp, jackknife SE) for a 1-D array of log-likelihoods.
    """
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))

    # jackknife
    jk_vals = np.empty(n)
    for k in range(n):
        x_k = np.delete(x, k)
        max_k = x_k.max()
        jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))

    se = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E2_est = float(lme)    
E2_se  = float(se)       
```


```{python E3, include=FALSE}
from __future__ import annotations

import os
import numpy as np

# ------------- cache settings -----------------------------------
CACHE_DIR  = "wwr/E3"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_vanilla.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# ----------------------------------------------------------------
# 1) Try to load cached results
# ----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results → {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# ----------------------------------------------------------------
# 2) No cache → run the original simulation code unmodified
# ----------------------------------------------------------------
else:
    print("[cache] No cache found – running the full simulation …")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>  ORIGINAL CODE — DO NOT EDIT  <<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    #  (Everything between the double lines is your untouched script;
    #   only indentation has changed so it nests inside this block.)
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize

    import matplotlib.pyplot as plt
    from scipy.special import logsumexp
    from sympy.polys.benchmarks.bench_solvers import uk_10

    from Scripts.measles_PALSMC_perstep import PAL_run_likelihood_lookahead

    plt.ioff()

    import sys
    sys.path.append('Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("E3"):
        os.makedirs("E3")

    os.environ['PYTHONHASHSEED'] = '45'
    random.seed(45)
    np.random.seed(45)
    tf.random.set_seed(45)

    UKbirths_array = np.load("Data/UKbirths_array.npy")
    UKpop_array = np.load("Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("Data/UKmeasles_array.npy")
    modelA_array = np.load("Data/Parameter/final_parameters_lookahead_A.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array[18:19, :], dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array[18:19, :], dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array[18:19, :], dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array[18:19, 18:19],
                                                   dtype=tf.float32)
    df = pd.read_csv("Data/londonsim.csv")

    data_array = df.values
    UKmeasles = tf.convert_to_tensor(data_array, dtype=tf.float32)

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype = tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)
    initial_pop = UKpop[:,0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14/tf.cast(intermediate_steps, dtype = tf.float32), dtype = tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(T, intermediate_steps, term, school)

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype = tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype = tf.float32)
    p      = tf.constant(0.759, dtype = tf.float32)
    delta_year = tf.convert_to_tensor([[1/50]], dtype = tf.float32)*tf.ones((n_cities, 4), dtype = tf.float32)

    # increase the n_experiments for proper variance estimates, use n_experiments to just test the log-likelihood
    n_experiments = 20

    # lookahead
    # shared
    best_parameters = np.load("Data/Parameter/final_parameters_lookahead_A.npy")
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)
    q_mean = tf.constant(np.mean(np.load("Data/q_mean.npy")), dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)

    # --- parameter block (same format, new values) -----------------------
    pi_0_1 = 0.02536
    pi_0_2 = 0.0042
    pi_0_3 = 0.000061
    pi_0   = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(6.30 * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([0.142], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([0.0473], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = tf.convert_to_tensor([[0.0]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(0.1476, dtype=tf.float32)
    c      = tf.constant(0.219 , dtype=tf.float32)
    xi_var = tf.convert_to_tensor(0.318 , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(0.305, dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=0.7, scale=q_var, low=0.0, high=1.0)

    n_particles = 100000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = np.max(x)
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_minus_k = np.delete(x, k)
                max_x_mk = np.max(x_minus_k)
                jk_vals[k] = max_x_mk + np.log(np.mean(np.exp(x_minus_k - max_x_mk)))
            xse = (n - 1) * np.std(jk_vals, ddof=1) / np.sqrt(n)
            results["se"] = xse
        if ess:
            w = np.exp(x - max_x)
            xss = np.sum(w) ** 2 / np.sum(w**2)
            results["ess"] = xss
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_res(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    mean_log = np.mean(log_likelihood_shared)

    print("Variance of log likelihoods:", variance_log)
    print("mean of log likelihoods:", mean_log)

    out_file_path = os.path.join("E3", "PAL_vanilla.csv")
    np.savetxt(out_file_path, log_likelihood_shared, delimiter=",")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>  ORIGINAL CODE END  <<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # ------ save array to cache so future runs can skip heavy work ---
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached → {CACHE_FILE}")

# --------------------------------------------------------------------
# 3) Quick summary (identical whether loaded or freshly computed)
# --------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    """
    Return (log-mean-exp, jackknife SE) for a 1-D array of log-likelihoods.
    """
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))

    # jackknife
    jk_vals = np.empty(n)
    for k in range(n):
        x_k = np.delete(x, k)
        max_k = x_k.max()
        jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))

    se = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E3_est = float(lme)    
E3_se  = float(se) 
```

```{python E4, include = FALSE}
from __future__ import annotations

import os
import numpy as np

# ------------- cache settings -----------------------------------
CACHE_DIR  = "wwr/E4"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_lookahead.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# ----------------------------------------------------------------
# 1) Try to load cached results
# ----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results → {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# ----------------------------------------------------------------
# 2) No cache → run the original simulation code unmodified
# ----------------------------------------------------------------
else:
    print("[cache] No cache found – running the full simulation …")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>  ORIGINAL CODE — DO NOT EDIT  <<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    #  (Everything between the double lines is your untouched script;
    #   only indentation has changed so it nests inside this block.)
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize

    import matplotlib.pyplot as plt
    from scipy.special import logsumexp
    from sympy.polys.benchmarks.bench_solvers import uk_10

    from Scripts.measles_PALSMC_perstep import PAL_run_likelihood_lookahead

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("wwr/E4"):
        os.makedirs("wwr/E4")

    os.environ['PYTHONHASHSEED'] = '45'
    random.seed(45)
    np.random.seed(45)
    tf.random.set_seed(45)

    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")
    modelA_array = np.load("wwr/Data/Parameter/final_parameters_lookahead_A.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array[18:19, :], dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array[18:19, :], dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array[18:19, :], dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array[18:19, 18:19],
                                                   dtype=tf.float32)
    df = pd.read_csv("wwr/Data/londonsim.csv")

    data_array = df.values
    UKmeasles = tf.convert_to_tensor(data_array, dtype=tf.float32)

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype = tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)
    initial_pop = UKpop[:,0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14/tf.cast(intermediate_steps, dtype = tf.float32), dtype = tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(T, intermediate_steps, term, school)

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype = tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype = tf.float32)
    p      = tf.constant(0.759, dtype = tf.float32)
    delta_year = tf.convert_to_tensor([[1/50]], dtype = tf.float32)*tf.ones((n_cities, 4), dtype = tf.float32)

    # increase the n_experiments for proper variance estimates, use n_experiments to just test the log-likelihood
    n_experiments = 20

    # lookahead
    # shared
    best_parameters = np.load("wwr/Data/Parameter/final_parameters_lookahead_A.npy")
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)
    q_mean = tf.constant(np.mean(np.load("wwr/Data/q_mean.npy")), dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)

    # --- parameter block (same format, new values) -----------------------
    pi_0_1 = 0.02536
    pi_0_2 = 0.0042
    pi_0_3 = 0.000061
    pi_0   = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(6.30 * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([0.142], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([0.0473], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = tf.convert_to_tensor([[0.0]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(0.1476, dtype=tf.float32)
    c      = tf.constant(0.219 , dtype=tf.float32)
    xi_var = tf.convert_to_tensor(0.318 , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(0.305, dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=0.7, scale=q_var, low=0.0, high=1.0)

    n_particles = 5000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = np.max(x)
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_minus_k = np.delete(x, k)
                max_x_mk = np.max(x_minus_k)
                jk_vals[k] = max_x_mk + np.log(np.mean(np.exp(x_minus_k - max_x_mk)))
            xse = (n - 1) * np.std(jk_vals, ddof=1) / np.sqrt(n)
            results["se"] = xse
        if ess:
            w = np.exp(x - max_x)
            xss = np.sum(w) ** 2 / np.sum(w**2)
            results["ess"] = xss
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_lookahead(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    mean_log = np.mean(log_likelihood_shared)

    print("Variance of log likelihoods:", variance_log)
    print("mean of log likelihoods:", mean_log)

    out_file_path = os.path.join("wwr", "E4", "PAL_lookahead.csv")
    np.savetxt(out_file_path, log_likelihood_shared, delimiter=",")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>  ORIGINAL CODE END  <<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # ------ save array to cache so future runs can skip heavy work ---
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached → {CACHE_FILE}")

# --------------------------------------------------------------------
# 3) Quick summary (identical whether loaded or freshly computed)
# --------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    """
    Return (log-mean-exp, jackknife SE) for a 1-D array of log-likelihoods.
    """
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))

    # jackknife
    jk_vals = np.empty(n)
    for k in range(n):
        x_k = np.delete(x, k)
        max_k = x_k.max()
        jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))

    se = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E4_est = float(lme)    
E4_se  = float(se) 
```

```{python E5, include = FALSE}
from __future__ import annotations

import os
import numpy as np

# ------------- cache settings -----------------------------------
CACHE_DIR  = "wwr/E5"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_lookahead.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# ----------------------------------------------------------------
# 1) Try to load cached results
# ----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results → {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# ----------------------------------------------------------------
# 2) No cache → run the original simulation code unmodified
# ----------------------------------------------------------------
else:
    print("[cache] No cache found – running the full simulation …")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>  ORIGINAL CODE — DO NOT EDIT  <<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    #  (Everything between the double lines is your untouched script;
    #   only indentation has changed so it nests inside this block.)
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize

    import matplotlib.pyplot as plt
    from scipy.special import logsumexp
    from sympy.polys.benchmarks.bench_solvers import uk_10

    from Scripts.measles_PALSMC_perstep import PAL_run_likelihood_lookahead

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("wwr/E5"):
        os.makedirs("wwr/E5")

    os.environ['PYTHONHASHSEED'] = '45'
    random.seed(45)
    np.random.seed(45)
    tf.random.set_seed(45)

    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")
    modelA_array = np.load("wwr/Data/Parameter/final_parameters_lookahead_A.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array[18:19, :], dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array[18:19, :], dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array[18:19, :], dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array[18:19, 18:19],
                                                   dtype=tf.float32)
    df = pd.read_csv("Data/londonsim.csv")

    data_array = df.values
    UKmeasles = tf.convert_to_tensor(data_array, dtype=tf.float32)

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype = tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)
    initial_pop = UKpop[:,0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14/tf.cast(intermediate_steps, dtype = tf.float32), dtype = tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(T, intermediate_steps, term, school)

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype = tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype = tf.float32)
    p      = tf.constant(0.759, dtype = tf.float32)
    delta_year = tf.convert_to_tensor([[1/50]], dtype = tf.float32)*tf.ones((n_cities, 4), dtype = tf.float32)

    # increase the n_experiments for proper variance estimates, use n_experiments to just test the log-likelihood
    n_experiments = 20

    # lookahead
    # shared
    best_parameters = np.load("wwr/Data/Parameter/final_parameters_lookahead_A.npy")
    best_parameters = np.ndarray.astype(best_parameters, dtype = np.float32)
    q_mean = tf.constant(np.mean(np.load("wwr/Data/q_mean.npy")), dtype = tf.float32)

    n_cities = tf.constant(1, dtype = tf.int64)

    # --- parameter block (same format, new values) -----------------------
    pi_0_1 = 0.02536
    pi_0_2 = 0.0042
    pi_0_3 = 0.000061
    pi_0   = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(6.30 * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([0.142], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([0.0473], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = tf.convert_to_tensor([[0.0]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(0.1476, dtype=tf.float32)
    c      = tf.constant(0.219 , dtype=tf.float32)
    xi_var = tf.convert_to_tensor(0.318 , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(0.305, dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=0.7, scale=q_var, low=0.0, high=1.0)

    n_particles = 100000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = np.max(x)
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_minus_k = np.delete(x, k)
                max_x_mk = np.max(x_minus_k)
                jk_vals[k] = max_x_mk + np.log(np.mean(np.exp(x_minus_k - max_x_mk)))
            xse = (n - 1) * np.std(jk_vals, ddof=1) / np.sqrt(n)
            results["se"] = xse
        if ess:
            w = np.exp(x - max_x)
            xss = np.sum(w) ** 2 / np.sum(w**2)
            results["ess"] = xss
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_lookahead(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    mean_log = np.mean(log_likelihood_shared)

    print("Variance of log likelihoods:", variance_log)
    print("mean of log likelihoods:", mean_log)

    out_file_path = os.path.join("wwr", "E5", "PAL_lookahead.csv")
    np.savetxt(out_file_path, log_likelihood_shared, delimiter=",")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>  ORIGINAL CODE END  <<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # ------ save array to cache so future runs can skip heavy work ---
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached → {CACHE_FILE}")

# --------------------------------------------------------------------
# 3) Quick summary (identical whether loaded or freshly computed)
# --------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    """
    Return (log-mean-exp, jackknife SE) for a 1-D array of log-likelihoods.
    """
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))

    # jackknife
    jk_vals = np.empty(n)
    for k in range(n):
        x_k = np.delete(x, k)
        max_k = x_k.max()
        jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))

    se = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E5_est = float(lme)    
E5_se  = float(se) 
```

```{python E9, include = FALSE}
#!/usr/bin/env python3
# ================================================================
#  Block-particle PAL experiment (40 cities) with on-disk caching
# ================================================================

from __future__ import annotations
import os
import numpy as np

# ----------------------------- cache -----------------------------
CACHE_DIR  = "wwr/E9"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_res_40.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# -----------------------------------------------------------------
# 1) Load cached results if available
# -----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results → {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# -----------------------------------------------------------------
# 2) If no cache, run the (heavy) original code unmodified
# -----------------------------------------------------------------
else:
    print("[cache] No cache found – running the full simulation …")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>> ORIGINAL CODE — DO NOT EDIT <<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize
    import matplotlib.pyplot as plt

    plt.ioff()

    import sys
    sys.path.append('wwr/Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("wwr/E9"):
        os.makedirs("wwr/E9")

    os.environ['PYTHONHASHSEED'] = '42'
    random.seed(42)
    np.random.seed(42)
    tf.random.set_seed(42)

    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array, dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array, dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array, dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array, dtype=tf.float32)

    df = pd.read_csv("wwr/Data/M6.csv")
    data_array = df.values
    UKmeasles = tf.convert_to_tensor(data_array, dtype=tf.float32)

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype=tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=tf.float32)

    n_cities = tf.constant(40, dtype=tf.int64)
    initial_pop = UKpop[:, 0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14 / tf.cast(intermediate_steps, dtype=tf.float32), dtype=tf.float32)
    is_school_term_array, is_start_school_year_array, times_total, times_obs = school_term_and_school_year(
        T, intermediate_steps, term, school
    )

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype=tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype=tf.float32)
    p = tf.constant(0.759, dtype=tf.float32)
    delta_year = tf.convert_to_tensor([[1 / 50]], dtype=tf.float32) * tf.ones((n_cities, 4), dtype=tf.float32)

    n_experiments = 20

    best_parameters = np.load("wwr/Data/Parameter/final_parameters_lookahead_A.npy")
    best_parameters = np.ndarray.astype(best_parameters, dtype=np.float32)

    n_cities = tf.constant(40, dtype=tf.int64)

    pi_0_1, pi_0_2, pi_0_3 = 0.0304, 0.0056, 0.000034
    pi_0 = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    initial_pop = UKpop[:, 0]

    beta_bar = tf.convert_to_tensor(6.35 * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([0.159], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([0.045], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = tf.convert_to_tensor([[595]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(0.052, dtype=tf.float32)
    c      = tf.constant(0.077, dtype=tf.float32)
    xi_var = tf.convert_to_tensor(0.219, dtype=tf.float32)
    q_var  = tf.convert_to_tensor(0.185, dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=0.7, scale=q_var, low=0.0, high=1.0)

    n_particles = 5000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = x.max()
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_k = np.delete(x, k)
                max_k = x_k.max()
                jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))
            results["se"] = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
        if ess:
            w = np.exp(x - max_x)
            results["ess"] = (w.sum() ** 2) / (w ** 2).sum()
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_res(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    elapsed_time = time.perf_counter() - start_time
    print(f"Comp.time: {elapsed_time:.4f} seconds")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])

    variance_log = np.var(log_likelihood_shared, ddof=1)
    print("Variance of log likelihoods:", variance_log)

    np.savetxt(os.path.join("wwr", "E9", "PAL_res_40.csv"), log_likelihood_shared, delimiter=",")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>> ORIGINAL CODE END <<<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # -------------- cache the array for future runs -----------------
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached → {CACHE_FILE}")

# -------------------------------------------------------------------
# 3) Unified summary – always recomputed (includes jackknife SE)
# -------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))
    jk = np.array([
        (np.delete(x, k).max() +
         np.log(np.mean(np.exp(np.delete(x, k) - np.delete(x, k).max()))))
        for k in range(n)
    ])
    se = (n - 1) * jk.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E9_est = float(lme)    
E9_se  = float(se) 
```


```{python E10, include = FALSE}
#!/usr/bin/env python3
# ================================================================
#  PAL-lookahead experiment (40 cities, E10) with on-disk caching
# ================================================================
from __future__ import annotations
import os
import numpy as np

# ---------------------------- cache ------------------------------
CACHE_DIR  = "wwr/E10"
CACHE_FILE = os.path.join(CACHE_DIR, "PAL_lookahead_40.npz")
CACHE_KEY  = "log_likelihood_shared"

os.makedirs(CACHE_DIR, exist_ok=True)

# -----------------------------------------------------------------
# 1) Load cached results if available
# -----------------------------------------------------------------
if os.path.exists(CACHE_FILE):
    print(f"[cache] Found existing results → {CACHE_FILE}")
    log_likelihood_shared = np.load(CACHE_FILE)[CACHE_KEY]

# -----------------------------------------------------------------
# 2) Otherwise run the original heavy simulation (unmodified)
# -----------------------------------------------------------------
else:
    print("[cache] No cache found – running the full simulation …")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>> ORIGINAL CODE — DO NOT EDIT <<<<<<<<<<<<<<<
    # ----------------------------------------------------------------
    import numpy as np
    import time
    import random
    import pandas as pd
    import tensorflow as tf
    import tensorflow_probability as tfp
    import os
    import matplotlib.pyplot as plt
    from scipy.optimize import minimize
    import matplotlib.pyplot as plt

    plt.ioff()

    import sys
    sys.path.append('Scripts/')
    from measles_simulator import *
    from measles_PALSMC import *

    if not os.path.exists("E10"):
        os.makedirs("E10")

    os.environ['PYTHONHASHSEED'] = '42'
    random.seed(42)
    np.random.seed(42)
    tf.random.set_seed(42)

    UKbirths_array = np.load("wwr/Data/UKbirths_array.npy")
    UKpop_array = np.load("wwr/Data/UKpop_array.npy")
    measles_distance_matrix_array = np.load("wwr/Data/measles_distance_matrix_array.npy")
    UKmeasles_array = np.load("wwr/Data/UKmeasles_array.npy")

    UKbirths = tf.convert_to_tensor(UKbirths_array, dtype=tf.float32)
    UKpop = tf.convert_to_tensor(UKpop_array, dtype=tf.float32)
    measles_distance_matrix = tf.convert_to_tensor(measles_distance_matrix_array, dtype=tf.float32)
    UKmeasles = tf.convert_to_tensor(UKmeasles_array, dtype=tf.float32)

    df = pd.read_csv("wwr/Data/M6.csv")
    UKmeasles = tf.convert_to_tensor(df.values, dtype=tf.float32)

    term   = tf.convert_to_tensor([6, 99, 115, 198, 252, 299, 308, 355, 366], dtype=tf.float32)
    school = tf.convert_to_tensor([0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=tf.float32)

    n_cities = tf.constant(40, dtype=tf.int64)
    initial_pop = UKpop[:, 0]

    T = 416
    print(T)

    intermediate_steps = 4
    h = tf.constant(14 / tf.cast(intermediate_steps, dtype=tf.float32), dtype=tf.float32)
    is_school_term_array, is_start_school_year_array, *_ = school_term_and_school_year(
        T, intermediate_steps, term, school
    )

    is_school_term_array = tf.convert_to_tensor(is_school_term_array, dtype=tf.float32)
    is_start_school_year_array = tf.convert_to_tensor(is_start_school_year_array, dtype=tf.float32)
    p = tf.constant(0.759, dtype=tf.float32)
    delta_year = tf.convert_to_tensor([[1 / 50]], dtype=tf.float32) * tf.ones((n_cities, 4), dtype=tf.float32)

    n_experiments = 20

    best_parameters = np.load("Data/Parameter/final_parameters_lookahead_A.npy").astype(np.float32)

    pi_0_1, pi_0_2, pi_0_3 = 0.02536, 0.0042, 0.000061
    pi_0 = (
        tf.convert_to_tensor(
            [[pi_0_1, pi_0_2, pi_0_3, 1.0 - pi_0_1 - pi_0_2 - pi_0_3]],
            dtype=tf.float32
        )
        * tf.ones((n_cities, 4), dtype=tf.float32)
    )

    beta_bar = tf.convert_to_tensor(6.30 * tf.ones((n_cities, 1)), dtype=tf.float32)
    rho      = tf.convert_to_tensor([0.142], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)
    gamma    = tf.convert_to_tensor([0.0473], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    g = tf.convert_to_tensor([[700]], dtype=tf.float32) * tf.ones((n_cities, 1), dtype=tf.float32)

    a      = tf.constant(0.1476, dtype=tf.float32)
    c      = tf.constant(0.219 , dtype=tf.float32)
    xi_var = tf.convert_to_tensor(0.318 , dtype=tf.float32)
    q_var  = tf.convert_to_tensor(0.305, dtype=tf.float32)

    Xi = tfp.distributions.Gamma(concentration=xi_var, rate=xi_var)
    Q  = tfp.distributions.TruncatedNormal(loc=0.7, scale=q_var, low=0.0, high=1.0)

    n_particles = 5000
    log_likelihood_shared = np.zeros(n_experiments)

    def logmeanexp(x, se=False, ess=False):
        x = np.asarray(x, dtype=float)
        n = x.shape[0]
        max_x = x.max()
        est = max_x + np.log(np.mean(np.exp(x - max_x)))
        if not se and not ess:
            return est
        results = {"est": est}
        if se:
            jk_vals = np.empty(n)
            for k in range(n):
                x_k = np.delete(x, k)
                max_k = x_k.max()
                jk_vals[k] = max_k + np.log(np.mean(np.exp(x_k - max_k)))
            results["se"] = (n - 1) * jk_vals.std(ddof=1) / np.sqrt(n)
        if ess:
            w = np.exp(x - max_x)
            results["ess"] = (w.sum() ** 2) / (w ** 2).sum()
        return results

    start_time = time.perf_counter()

    for i in range(n_experiments):
        seed_i = 123 + i
        random.seed(seed_i)
        np.random.seed(seed_i)
        tf.random.set_seed(seed_i)

        value = (
            PAL_run_likelihood_lookahead(
                T, intermediate_steps, UKmeasles, UKbirths, UKpop, g,
                measles_distance_matrix, initial_pop, pi_0, beta_bar, p, a,
                is_school_term_array, is_start_school_year_array, h,
                rho, gamma, Xi, Q, c, n_cities, n_particles, delta_year
            )
        )[0].numpy()

        log_likelihood_shared[i] = value
        print(value)

    print(f"Comp.time: {time.perf_counter() - start_time:.2f} s")

    res = logmeanexp(log_likelihood_shared, se=True, ess=True)
    print("Est =", res["est"])
    print("SE  =", res["se"])
    print("ESS =", res["ess"])
    print("Variance:", np.var(log_likelihood_shared, ddof=1))

    np.savetxt(os.path.join("wwr", "E10", "PAL_lookahead_40.csv"),
               log_likelihood_shared, delimiter=",")

    # ----------------------------------------------------------------
    # >>>>>>>>>>>>>>>>>>>>> ORIGINAL CODE END <<<<<<<<<<<<<<<<<<<<<<<<
    # ----------------------------------------------------------------

    # -------------- cache the results for future runs ----------------
    np.savez(CACHE_FILE, **{CACHE_KEY: log_likelihood_shared})
    print(f"[cache] Results cached → {CACHE_FILE}")

# -------------------------------------------------------------------
# 3) Unified summary – recomputed every run
# -------------------------------------------------------------------
def logmeanexp_and_se(x: np.ndarray) -> tuple[float, float]:
    n = x.size
    max_x = x.max()
    lme   = max_x + np.log(np.mean(np.exp(x - max_x)))
    jk = np.array([
        (np.delete(x, k).max() +
         np.log(np.mean(np.exp(np.delete(x, k) - np.delete(x, k).max()))))
        for k in range(n)
    ])
    se = (n - 1) * jk.std(ddof=1) / np.sqrt(n)
    return lme, se

lme, se = logmeanexp_and_se(log_likelihood_shared)

print("\n[summary]")
print("  log-mean-exp :", lme)
print("  SE           :", se)
print("  mean         :", log_likelihood_shared.mean())
print("  variance     :", log_likelihood_shared.var(ddof=1))

E10_est = float(lme)    
E10_se  = float(se) 
```


```{r bridge,include = FALSE}
library(reticulate)

E2_result <- c(py$E2_est,  py$E2_se)    # c(λ, σ)
E3_result <- c(py$E3_est,  py$E3_se)
E4_result <- c(py$E4_est,  py$E4_se)
E5_result <- c(py$E5_est,  py$E5_se)
E9_result <- c(py$E9_est,  py$E9_se)
E10_result <- c(py$E10_est, py$E10_se)


E1_result_vec <- c(as.numeric(E1_result[1]),  as.numeric(E1_result[2]),as.numeric(tmp_benchmark$total),as.numeric(tmp_negbinom))
E8_result_vec <- c(as.numeric(E8_result[1]),  as.numeric(E8_result[2]),as.numeric(tmp_benchmark_spat$total),as.numeric(tmp_negbinom_spat))

E1_lambda <- E1_result_vec[1];  E1_sigma <- E1_result_vec[2]
E2_lambda <- E2_result[1];      E2_sigma <- E2_result[2]
E3_lambda <- E3_result[1];      E3_sigma <- E3_result[2]
E4_lambda <- E4_result[1];      E4_sigma <- E4_result[2]
E5_lambda <- E5_result[1];      E5_sigma <- E5_result[2]
E8_lambda <- E8_result_vec[1];  E8_sigma <- E8_result_vec[2]
E9_lambda <- E9_result[1];      E9_sigma <- E9_result[2]
E10_lambda<- E10_result[1];     E10_sigma<- E10_result[2]

E1_AMRA <- E1_result_vec[3];  E1_NegB <- E1_result_vec[4]
E2_AMRA <- E1_result_vec[3];      E2_NegB <- E1_result_vec[4]
E3_AMRA <- E1_result_vec[3];      E3_NegB <- E1_result_vec[4]
E4_AMRA <- E1_result_vec[3];      E4_NegB <- E1_result_vec[4]
E5_AMRA <- E1_result_vec[3];      E5_NegB <- E1_result_vec[4]
E8_AMRA <- E8_result_vec[3];  E8_NegB <- E8_result_vec[4]
E9_AMRA <- E8_result_vec[3];      E9_NegB <- E8_result_vec[4]
E10_AMRA<- E8_result_vec[3];     E10_NegB<- E8_result_vec[4]
```

\begin{table}[ht]
\centering
\begin{tabular}{lrrrr}
\toprule
\textbf{$E$} & $\lambda$ & $\sigma$ & ARMA & NegBinom \\
\midrule
$E_1$  & $`r sprintf('%.2f', E1_lambda)`$  & 
$`r sprintf('%.2f', E1_sigma)`$  & 
$`r sprintf('%.2f', E1_AMRA)`$  & 
$`r sprintf('%.2f', E1_NegB)`$ \\
$E_2$  & $`r sprintf('%.2f', E2_lambda)`$  & 
$`r sprintf('%.2f', E2_sigma)`$  & 
$`r sprintf('%.2f', E2_AMRA)`$  & 
$`r sprintf('%.2f', E2_NegB)`$ \\
$E_3$  & $`r sprintf('%.2f', E3_lambda)`$  & 
$`r sprintf('%.2f', E3_sigma)`$  & 
$`r sprintf('%.2f', E3_AMRA)`$  & 
$`r sprintf('%.2f', E3_NegB)`$ \\
$E_4$  & $`r sprintf('%.2f', E4_lambda)`$  & 
$`r sprintf('%.2f', E4_sigma)`$  & 
$`r sprintf('%.2f', E4_AMRA)`$  & 
$`r sprintf('%.2f', E4_NegB)`$ \\
$E_5$  & $`r sprintf('%.2f', E5_lambda)`$  & 
$`r sprintf('%.2f', E5_sigma)`$  & 
$`r sprintf('%.2f', E5_AMRA)`$  & 
$`r sprintf('%.2f', E5_NegB)`$ \\
$E_6$  &  &  &  &  \\      
$E_7$  &  &  &  &  \\      
$E_8$  & $`r sprintf('%.2f', E8_lambda)`$  & $`r sprintf('%.2f', E8_sigma)`$  & $`r sprintf('%.2f', E8_AMRA)`$  & $`r sprintf('%.2f', E8_NegB)`$ \\
$E_9$  & $`r sprintf('%.2f', E9_lambda)`$  & $`r sprintf('%.2f', E9_sigma)`$  & $`r sprintf('%.2f', E9_AMRA)`$  & $`r sprintf('%.2f', E9_NegB)`$ \\
$E_{10}$ & $`r sprintf('%.2f', E10_lambda)`$ & $`r sprintf('%.2f', E10_sigma)`$ & $`r sprintf('%.2f', E10_AMRA)`$ & $`r sprintf('%.2f', E10_NegB)`$ \\
$E_{11}$ &  &  &  &  \\    
$E_{12}$ &  &  &  &  \\    
$E_{13}$ &  &  &  &  \\    
\bottomrule
\end{tabular}
\caption{Log‐likelihood estimate, $\lambda$,  for each experiment described in Table\ 2.
Estimates derive from averaging 20 replications on a natural scale so that the basic particle filter estimate is unbiased.
The standard error, $\sigma$, is a jack-knife estimate implemented via the logmeanexp function in the pomp R package.
ARMA gives the log-likelihood for an autoregressive moving average benchmark, and NegBinom is an autoregressive negative binomial benchmark.}
\label{tbl:method_comparison}
\end{table}



Here, experiments $E_1$--$E_5$ address Q1 directly, using simulated data on a single unit. 
$E_1$ provides a ground truth for this particular particular model, a single-city SIR model.
Comparing $\lambda_2$ and $\lambda_3$ with $\lambda_1$, we see that vanilla PAL, without the lookahead, performs as expected.
On this relatively easy task, $\PALV$ produces stable estimates.
Since $\PALV$ is filtering using a model that differs slightly from the data generating model, we expect to see a small shortfall, with $lambad_2<\lambda_1$ and  $lambad_3<\lambda_1$.
The difference, $\lambda_2-\lambda_3$, is statistically indistinguishable from zero in this experiment.

$E_4$ and $E_5$ demonstrate the positive bias of $\PALL$ both at a usual number of particles and for an intensive calculation that may not be possible on larger problems.
The best estimates of this bias are $\lambda_4-\lambda_3$ and $\lambda_5-\lambda_3$, since $\PALL$ and $\PALV$ target the same quantity.
The smaller number of particles leads for $E_4$ leads to a higher log-likelihood estimate and a lower Monte Carlo error estimate, both of which suggest a long left tail to the Monte Carlo likelihood estimate, leading to estimators that behave as though the variance is infinite.


Experiments $E_6$ and $E_7$ introduce the actual data. Comparing the resulting log-likelihoods with the benchmark values, we see \eic{TO DO. I EXPECT TO SEE TYPICAL VALUES FOR HOW THE METHODS COMPARE TO THE BENCHMARKS ON REAL AND SIMULATED DATA, FOR ONE UNIT}

Experiments $E_8$, and subsequent, investigate a 40 unit system.
For $E_8$, $E_9$ and $E_{10}$, we simulated from a model with the coupling parameter between towns set to zero.
That was done to study a situation where a block particle filter gives an asymptotically exact answer.
We see the same story as the single-unit case, where the positive bias of  $\PALL$ is estimated by $(\lambda_{10}-\lambda_{9})/40=$ `r sprintf('%.2f', (E10_lambda-E9_lambda)/40)` per unit.
This bias is large enough that we obtain $\lambda_{10}>\lambda_8$, with the difference being  $(\lambda_{10}-\lambda_{8})/40=$ `r sprintf('%.2f', (E10_lambda-E8_lambda)/40)` per unit.
We see that the $U=40$ results scale approximately linearly compared to $U=1$.

Experiments $E_{11}$--$E_{13}$ consider coupled models for the full, real dataset. 
We cannot accurately know the ground truth here. 
However, comparing estimated likelihood with the benchmarks, in light of the earlier experiment, the results are consistent with a conclusion that... \eic{TODO}

\section{Some theoretical considerations for lookahead PAL}
\label{sec:theory}

 Since vanilla particle filter algorithms are unbiased for the likelihood, it might be reasonable to expect the PAL-SMC algorithm to be unbiased for the PAL likelihood, but this is not true for the lookahead PAL filter used by WWR. 
 This is a property of the lookahead part of the algorithm, derived from [REF], rather than the PAL approximation. 
 Therefore, for the remainder of this section, we consider the simpler lookahead filter of \citet{rimella23}.
 
 Briefly, the vanilla particle filter is unbiased because the self-normalization constant happens to coincide with the conditional likelihood estimate. 
 Self-normalization does not always lead to unbiased likelihood estimates, as we can see from the following example.

Let $X$ take values $\{0,1\}$ with equal probability, and let $Y=X$ with probability 1. 
Suppose a single data point, $Y=1$. 
Suppose also an independent sample of $J$ particles, $x_{1:J}$, each with distribution matching $X$. 
Now, resample these particle with probability $p_j = (1-\epsilon) x_j/[\sum_j x_j] + \epsilon(1-x_j)/[\sum_j 1-x_j]$ so that, on average, a fraction $(1-\epsilon)$ of the resampled particles have value 1. 
Take $\epsilon \ll 1/J$, so that most resampled particle swarms contain no particles with value 0. 
Most particle swarms resulting from resampling will have $x_j = 1$ for all $j$, with the proper resampled weight $w_j = 1/(J\, p_j)$ being approximately $1/(2J)$ for all $j$. 
Rare swarms will have a particle with massive weight, approximately $1/(2J\epsilon)$. 
Under self-normalization, particle swarms with a massive weight will estimate the likelihood to be approximately zero, and particle swarms with $x_j=1$ for all $j$ will estimate the likelihood to be 1. 
By setting $\epsilon$ arbitrarily small, we can get an estimate whose expectation approaches 1 since with high probability we see only resampled swarms where every particle has value 1. 
If we take a different limit, with $J \to \infty$, the bias will go away asymptotically, but here we consider the case with fixed $J$ and $\epsilon \to 0$.

Importantly, the bias on the likelihood estimate in this example is positive. 
As mentioned earlier, a suboptimal forecast generally gives, on average, a negative bias on the conditional log-likelihood estimate, since log-likelihood is a proper scoring rule. 
This justifies assessing filters on their log-likelihood estimate in a similar way that one does for parameters in likelihood-based inference. 
A filter with a high log-likelihood estimate on simulated data from the target model is validated as a good likelihood approximation.
However, this does not necessarily apply to algorithms that look at future observations. When implementing lookahead algorithms, if you want the log-likelihood estimate to be conservative, you have to be extra careful to consider the bias. 
For unbiased likelihood estimates, the negative bias on the log-likelihood is a direct consequence of variance, and among such estimates it is reasonable to prefer a filter approximation with the highest log-likelihood estimate. 
For positively biased estimates, that is inappropriate.

\section{Conclusion}
\label{sec:conclusion}

The results in this article reinforce the investigation by \citet{hao24-arxiv} and lead to the conclusion that there is not currently a strong case for using PAL.
Simpler particle filter methods apply to arbitrary Markov process models, whereas PAL is limited to a specific class of discrete-state Markov process models.
Basic particle filters, and their block particle filter extensions, have the plug-and-play property \citep{breto09,he10} and likelihood optimization methods for particle filters such as iterated filtering \citep{ionides15} and certain automatic differentiation algorithms \citep{tan24} inherit this convenient property. 
PAL may potentially lead to dramatic computational improvements over particle filters some situations.
However, WWR's overdispersed generalization of PAL also requires a particle filter component, at which point it shares many of the limitations of particle filters.
The results of WWR, together with various other authors \citep{stocks18,he10}, show that overdispersion is frequently necessary for a dynamic model to provide an adequate statistical description of epidemiological data.